{
  "blocks": [
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun Segment Anything 2, a zero-shot instance segmentation model, on an image.\n\n** Dedicated inference server required (GPU recomended) **\n\nYou can use pass in boxes/predictions from other models to Segment Anything 2 to use as prompts for the model.\nIf you pass in box detections from another model, the class names of the boxes will be forwarded to the predicted masks.  If using the model unprompted, the model will assign integers as class names / ids.\n",
        "name": "Segment Anything 2 Model",
        "properties": {
          "type": {
            "const": "roboflow_core/segment_anything@v1",
            "enum": ["roboflow_core/segment_anything@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "boxes": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "`'predictions'` key from Object Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[object_detection_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Instance Segmentation Model outputs",
                    "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[instance_segmentation_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Keypoint Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[keypoint_detection_prediction]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Boxes (from other model predictions) to ground SAM2",
            "examples": ["$steps.object_detection_model.predictions"],
            "title": "Boxes"
          },
          "version": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": [
                  "hiera_large",
                  "hiera_small",
                  "hiera_tiny",
                  "hiera_b_plus"
                ],
                "type": "string"
              }
            ],
            "default": "hiera_tiny",
            "description": "Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus",
            "examples": ["hiera_large", "$inputs.openai_model"],
            "title": "Version"
          },
          "threshold": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Float value",
                    "docs": "\nExample:\n```\n1.3\n2.7\n```\n",
                    "name": "float"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "number"
              }
            ],
            "default": 0,
            "description": "Threshold for predicted masks scores",
            "examples": [0.3],
            "title": "Threshold"
          },
          "multimask_output": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": true,
            "description": "Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended.",
            "examples": [true, "$inputs.multimask_output"],
            "title": "Multimask Output"
          }
        },
        "required": ["type", "name", "images"],
        "search_keywords": ["SAM2", "META"],
        "short_description": "Convert bounding boxes to polygons, or run SAM2 on an entire image to generate a mask.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.segment_anything2.v1.SegmentAnything2BlockV1",
      "human_friendly_block_name": "Segment Anything 2 Model",
      "manifest_type_identifier": "roboflow_core/segment_anything@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "AggregationMode": {
            "enum": ["average", "max", "min"],
            "title": "AggregationMode",
            "type": "string"
          }
        },
        "additionalProperties": true,
        "block_type": "fusion",
        "license": "Apache-2.0",
        "long_description": "\nCombine detections from multiple detection-based models based on a majority vote \nstrategy.\n\nThis block is useful if you have multiple specialized models that you want to consult \nto determine whether a certain object is present in an image.\n\nSee the table below to explore the values you can use to configure the consensus block.\n",
        "name": "Detections Consensus",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/detections_consensus@v1",
              "DetectionsConsensus"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions_batches": {
            "description": "Reference to detection-like model predictions made against single image to agree on model consensus",
            "examples": [["$steps.a.predictions", "$steps.b.predictions"]],
            "items": {
              "kind": [
                {
                  "description": "`'predictions'` key from Object Detection Model output",
                  "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                  "name": "Batch[object_detection_prediction]"
                },
                {
                  "description": "`'predictions'` key from Instance Segmentation Model outputs",
                  "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                  "name": "Batch[instance_segmentation_prediction]"
                },
                {
                  "description": "`'predictions'` key from Keypoint Detection Model output",
                  "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                  "name": "Batch[keypoint_detection_prediction]"
                }
              ],
              "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
              "reference": true,
              "selected_element": "step_output",
              "type": "string"
            },
            "minItems": 1,
            "title": "Predictions Batches",
            "type": "array"
          },
          "required_votes": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Required number of votes for single detection from different models to accept detection as output detection",
            "examples": [2, "$inputs.required_votes"],
            "title": "Required Votes"
          },
          "class_aware": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Flag to decide if merging detections is class-aware or only bounding boxes aware",
            "examples": [true, "$inputs.class_aware"],
            "title": "Class Aware"
          },
          "iou_threshold": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.3,
            "description": "IoU threshold to consider detections from different models as matching (increasing votes for region)",
            "examples": [0.3, "$inputs.iou_threshold"],
            "title": "Iou Threshold"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Confidence threshold for merged detections",
            "examples": [0.1, "$inputs.confidence"],
            "title": "Confidence"
          },
          "classes_to_consider": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Optional list of classes to consider in consensus procedure.",
            "examples": [["a", "b"], "$inputs.classes_to_consider"],
            "title": "Classes To Consider"
          },
          "required_objects": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "additionalProperties": {
                  "exclusiveMinimum": 0,
                  "type": "integer"
                },
                "type": "object"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  },
                  {
                    "description": "Dictionary",
                    "docs": null,
                    "name": "dictionary"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to `InferenceParameter`, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.",
            "examples": [
              3,
              {
                "a": 7,
                "b": 2
              },
              "$inputs.required_objects"
            ],
            "title": "Required Objects"
          },
          "presence_confidence_aggregation": {
            "allOf": [
              {
                "$ref": "#/$defs/AggregationMode"
              }
            ],
            "default": "max",
            "description": "Mode dictating aggregation of confidence scores and classes both in case of object presence deduction procedure.",
            "examples": ["max", "min"]
          },
          "detections_merge_confidence_aggregation": {
            "allOf": [
              {
                "$ref": "#/$defs/AggregationMode"
              }
            ],
            "default": "average",
            "description": "Mode dictating aggregation of confidence scores and classes both in case of boxes consensus procedure. One of `average`, `max`, `min`. Default: `average`. While using for merging overlapping boxes, against classes - `average` equals to majority vote, `max` - for the class of detection with max confidence, `min` - for the class of detection with min confidence.",
            "examples": ["min", "max"]
          },
          "detections_merge_coordinates_aggregation": {
            "allOf": [
              {
                "$ref": "#/$defs/AggregationMode"
              }
            ],
            "default": "average",
            "description": "Mode dictating aggregation of bounding boxes. One of `average`, `max`, `min`. Default: `average`. `average` means taking mean from all boxes coordinates, `min` - taking smallest box, `max` - taking largest box.",
            "examples": ["min", "max"]
          }
        },
        "required": ["type", "name", "predictions_batches", "required_votes"],
        "short_description": "Combine predictions from multiple detections models to make a decision about object presence.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "object_present",
          "kind": [
            {
              "name": "boolean",
              "description": "Boolean flag",
              "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
            },
            {
              "name": "dictionary",
              "description": "Dictionary",
              "docs": null
            }
          ]
        },
        {
          "name": "presence_confidence",
          "kind": [
            {
              "name": "float_zero_to_one",
              "description": "`float` value in range `[0.0, 1.0]`",
              "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
            },
            {
              "name": "dictionary",
              "description": "Dictionary",
              "docs": null
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.fusion.detections_consensus.v1.DetectionsConsensusBlockV1",
      "human_friendly_block_name": "Detections Consensus",
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "manifest_type_identifier_aliases": ["DetectionsConsensus"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nUse the OpenAI CLIP zero-shot classification model to classify images.\n\nThis block accepts an image and a list of text prompts. The block then returns the \nsimilarity of each text label to the provided image.\n\nThis block is useful for classifying images without having to train a fine-tuned \nclassification model. For example, you could use CLIP to classify the type of vehicle \nin an image, or if an image contains NSFW material.\n",
        "name": "Clip Comparison",
        "properties": {
          "type": {
            "enum": ["roboflow_core/clip_comparison@v1", "ClipComparison"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "Unique name of step in workflows",
            "title": "Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "texts": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              }
            ],
            "description": "List of texts to calculate similarity against each input image",
            "examples": [["a", "b", "c"], "$inputs.texts"],
            "title": "Texts"
          }
        },
        "required": ["type", "name", "images", "texts"],
        "short_description": "Compare CLIP image and text embeddings.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "similarity",
          "kind": [
            {
              "name": "list_of_values",
              "description": "List of values of any types",
              "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
            }
          ]
        },
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "prediction_type",
          "kind": [
            {
              "name": "Batch[prediction_type]",
              "description": "String value with type of prediction",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the type of prediction.\n\nExamples:\n```\n[\"object-detection\", \"object-detection\"]\n[\"instance-segmentation\", \"instance-segmentation\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.clip_comparison.v1.ClipComparisonBlockV1",
      "human_friendly_block_name": "Clip Comparison",
      "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
      "manifest_type_identifier_aliases": ["ClipComparison"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "LMMConfig": {
            "properties": {
              "max_tokens": {
                "default": 450,
                "title": "Max Tokens",
                "type": "integer"
              },
              "gpt_image_detail": {
                "default": "auto",
                "description": "To be used for GPT-4V only.",
                "enum": ["low", "high", "auto"],
                "title": "Gpt Image Detail",
                "type": "string"
              },
              "gpt_model_version": {
                "default": "gpt-4o",
                "title": "Gpt Model Version",
                "type": "string"
              }
            },
            "title": "LMMConfig",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "model",
        "deprecated": true,
        "license": "Apache-2.0",
        "long_description": "\nAsk a question to a Large Multimodal Model (LMM) with an image and text.\n\nYou can specify arbitrary text prompts to an LMMBlock.\n\nThe LLMBlock supports two LMMs:\n\n- OpenAI's GPT-4 with Vision, and;\n- CogVLM.\n\nYou need to provide your OpenAI API key to use the GPT-4 with Vision model. You do not \nneed to provide an API key to use CogVLM.\n\n_If you want to classify an image into one or more categories, we recommend using the \ndedicated LMMForClassificationBlock._\n",
        "name": "LMM",
        "properties": {
          "type": {
            "enum": ["roboflow_core/lmm@v1", "LMM"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "prompt": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Holds unconstrained text prompt to LMM mode",
            "examples": ["my prompt", "$inputs.prompt"],
            "title": "Prompt"
          },
          "lmm_type": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": ["gpt_4v", "cog_vlm"],
                "type": "string"
              }
            ],
            "description": "Type of LMM to be used",
            "examples": ["gpt_4v", "$inputs.lmm_type"],
            "title": "Lmm Type"
          },
          "lmm_config": {
            "allOf": [
              {
                "$ref": "#/$defs/LMMConfig"
              }
            ],
            "description": "Configuration of LMM"
          },
          "remote_api_key": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
            "examples": ["xxx-xxx", "$inputs.api_key"],
            "private": true,
            "title": "Remote Api Key"
          },
          "json_output": {
            "anyOf": [
              {
                "additionalProperties": {
                  "type": "string"
                },
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Holds dictionary that maps name of requested output field into its description",
            "examples": [
              {
                "count": "number of cats in the picture"
              },
              "$inputs.json_output"
            ],
            "title": "Json Output"
          }
        },
        "required": ["type", "name", "images", "prompt", "lmm_type"],
        "short_description": "Run a large multimodal model such as ChatGPT-4v or CogVLM.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image_metadata]",
              "description": "Dictionary with image metadata required by supervision",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the image that prediction was made against.\n\nExamples:\n```\n[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "structured_output",
          "kind": [
            {
              "name": "Batch[dictionary]",
              "description": "Batch of dictionaries",
              "docs": "\nThis kind represent a batch of any Python dicts.\nExamples:\n```\n[{\"my_key\", \"my_value_1\"}, {\"my_key\", \"my_value_2\"}]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "raw_output",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "*",
          "kind": [
            {
              "name": "*",
              "description": "Equivalent of any element",
              "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.lmm.v1.LMMBlockV1",
      "human_friendly_block_name": "LMM",
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "manifest_type_identifier_aliases": ["LMM"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "LMMConfig": {
            "properties": {
              "max_tokens": {
                "default": 450,
                "title": "Max Tokens",
                "type": "integer"
              },
              "gpt_image_detail": {
                "default": "auto",
                "description": "To be used for GPT-4V only.",
                "enum": ["low", "high", "auto"],
                "title": "Gpt Image Detail",
                "type": "string"
              },
              "gpt_model_version": {
                "default": "gpt-4o",
                "title": "Gpt Model Version",
                "type": "string"
              }
            },
            "title": "LMMConfig",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nClassify an image into one or more categories using a Large Multimodal Model (LMM).\n\nYou can specify arbitrary classes to an LMMBlock.\n\nThe LLMBlock supports two LMMs:\n\n- OpenAI's GPT-4 with Vision, and;\n- CogVLM.\n\nYou need to provide your OpenAI API key to use the GPT-4 with Vision model. You do not \nneed to provide an API key to use CogVLM.\n",
        "name": "LMM For Classification",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/lmm_for_classification@v1",
              "LMMForClassification"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "lmm_type": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": ["gpt_4v", "cog_vlm"],
                "type": "string"
              }
            ],
            "description": "Type of LMM to be used",
            "examples": ["gpt_4v", "$inputs.lmm_type"],
            "title": "Lmm Type"
          },
          "classes": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "List of classes that LMM shall classify against",
            "examples": [["a", "b"], "$inputs.classes"],
            "title": "Classes"
          },
          "lmm_config": {
            "allOf": [
              {
                "$ref": "#/$defs/LMMConfig"
              }
            ],
            "description": "Configuration of LMM"
          },
          "remote_api_key": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
            "examples": ["xxx-xxx", "$inputs.api_key"],
            "private": true,
            "title": "Remote Api Key"
          }
        },
        "required": ["type", "name", "images", "lmm_type", "classes"],
        "short_description": "Run a large multimodal model such as ChatGPT-4v or CogVLM for classification.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "raw_output",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "top",
          "kind": [
            {
              "name": "Batch[top_class]",
              "description": "Batch of string values representing top class predicted by classification model",
              "docs": "\nThe kind represent top classes predicted by classification model - representing its predictions on batch of images.\n\nExample:\n```\n[\"car\", \"dog\", \"car\", \"cat\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image_metadata]",
              "description": "Dictionary with image metadata required by supervision",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the image that prediction was made against.\n\nExamples:\n```\n[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "prediction_type",
          "kind": [
            {
              "name": "Batch[prediction_type]",
              "description": "String value with type of prediction",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the type of prediction.\n\nExamples:\n```\n[\"object-detection\", \"object-detection\"]\n[\"instance-segmentation\", \"instance-segmentation\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.lmm_classifier.v1.LMMForClassificationBlockV1",
      "human_friendly_block_name": "LMM For Classification",
      "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
      "manifest_type_identifier_aliases": ["LMMForClassification"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nAsk a question to OpenAI's GPT-4 with Vision model.\n\nYou can specify arbitrary text prompts to the OpenAIBlock.\n\nYou need to provide your OpenAI API key to use the GPT-4 with Vision model. \n\n_This model was previously part of the LMM block._\n",
        "name": "OpenAI",
        "properties": {
          "type": {
            "enum": ["roboflow_core/open_ai@v1", "OpenAI"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "prompt": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Text prompt to the OpenAI model",
            "examples": ["my prompt", "$inputs.prompt"],
            "title": "Prompt"
          },
          "openai_api_key": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "Your OpenAI API key",
            "examples": ["xxx-xxx", "$inputs.openai_api_key"],
            "private": true,
            "title": "Openai Api Key"
          },
          "openai_model": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": ["gpt-4o", "gpt-4o-mini"],
                "type": "string"
              }
            ],
            "default": "gpt-4o",
            "description": "Model to be used",
            "examples": ["gpt-4o", "$inputs.openai_model"],
            "title": "Openai Model"
          },
          "json_output_format": {
            "anyOf": [
              {
                "additionalProperties": {
                  "type": "string"
                },
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Holds dictionary that maps name of requested output field into its description",
            "examples": [
              {
                "count": "number of cats in the picture"
              },
              "$inputs.json_output_format"
            ],
            "title": "Json Output Format"
          },
          "image_detail": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": ["auto", "high", "low"],
                "type": "string"
              }
            ],
            "default": "auto",
            "description": "Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.",
            "examples": ["auto", "high", "low"],
            "title": "Image Detail"
          },
          "max_tokens": {
            "default": 450,
            "description": "Maximum number of tokens the model can generate in it's response.",
            "title": "Max Tokens",
            "type": "integer"
          }
        },
        "required": ["type", "name", "images", "prompt", "openai_api_key"],
        "search_keywords": ["LMM", "ChatGPT"],
        "short_description": "Run OpenAI's GPT-4 with Vision",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image_metadata]",
              "description": "Dictionary with image metadata required by supervision",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the image that prediction was made against.\n\nExamples:\n```\n[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "structured_output",
          "kind": [
            {
              "name": "Batch[dictionary]",
              "description": "Batch of dictionaries",
              "docs": "\nThis kind represent a batch of any Python dicts.\nExamples:\n```\n[{\"my_key\", \"my_value_1\"}, {\"my_key\", \"my_value_2\"}]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "raw_output",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "*",
          "kind": [
            {
              "name": "*",
              "description": "Equivalent of any element",
              "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.openai.v1.OpenAIBlockV1",
      "human_friendly_block_name": "OpenAI",
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "manifest_type_identifier_aliases": ["OpenAI"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nAsk a question to CogVLM, an open source vision-language model.\n\nThis model requires a GPU and can only be run on self-hosted devices, and is not available on the Roboflow Hosted API.\n\n_This model was previously part of the LMM block._\n",
        "name": "CogVLM",
        "properties": {
          "type": {
            "enum": ["roboflow_core/cog_vlm@v1", "CogVLM"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "prompt": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Text prompt to the CogVLM model",
            "examples": ["my prompt", "$inputs.prompt"],
            "title": "Prompt"
          },
          "json_output_format": {
            "anyOf": [
              {
                "additionalProperties": {
                  "type": "string"
                },
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Holds dictionary that maps name of requested output field into its description",
            "examples": [
              {
                "count": "number of cats in the picture"
              },
              "$inputs.json_output_format"
            ],
            "title": "Json Output Format"
          }
        },
        "required": ["type", "name", "images", "prompt"],
        "search_keywords": ["LMM"],
        "short_description": "Run a self-hosted vision language model",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image_metadata]",
              "description": "Dictionary with image metadata required by supervision",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the image that prediction was made against.\n\nExamples:\n```\n[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "structured_output",
          "kind": [
            {
              "name": "Batch[dictionary]",
              "description": "Batch of dictionaries",
              "docs": "\nThis kind represent a batch of any Python dicts.\nExamples:\n```\n[{\"my_key\", \"my_value_1\"}, {\"my_key\", \"my_value_2\"}]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "raw_output",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "*",
          "kind": [
            {
              "name": "*",
              "description": "Equivalent of any element",
              "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.cog_vlm.v1.CogVLMBlockV1",
      "human_friendly_block_name": "CogVLM",
      "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
      "manifest_type_identifier_aliases": ["CogVLM"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\n Retrieve the characters in an image using Optical Character Recognition (OCR).\n\nThis block returns the text within an image.\n\nYou may want to use this block in combination with a detections-based block (i.e. \nObjectDetectionBlock). An object detection model could isolate specific regions from an \nimage (i.e. a shipping container ID in a logistics use case) for further processing. \nYou can then use a DynamicCropBlock to crop the region of interest before running OCR.\n\nUsing a detections model then cropping detections allows you to isolate your analysis \non particular regions of an image.\n",
        "name": "OCR Model",
        "properties": {
          "type": {
            "enum": ["roboflow_core/ocr_model@v1", "OCRModel"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "Unique name of step in workflows",
            "title": "Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          }
        },
        "required": ["type", "name", "images"],
        "short_description": "Extract text from an image using optical character recognition.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "result",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "prediction_type",
          "kind": [
            {
              "name": "Batch[prediction_type]",
              "description": "String value with type of prediction",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the type of prediction.\n\nExamples:\n```\n[\"object-detection\", \"object-detection\"]\n[\"instance-segmentation\", \"instance-segmentation\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.ocr.v1.OCRModelBlockV1",
      "human_friendly_block_name": "OCR Model",
      "manifest_type_identifier": "roboflow_core/ocr_model@v1",
      "manifest_type_identifier_aliases": ["OCRModel"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun YOLO-World, a zero-shot object detection model, on an image.\n\nYOLO-World accepts one or more text classes you want to identify in an image. The model \nreturns the location of objects that meet the specified class, if YOLO-World is able to \nidentify objects of that class.\n\nWe recommend experimenting with YOLO-World to evaluate the model on your use case \nbefore using this block in production. For example on how to effectively prompt \nYOLO-World, refer to the [Roboflow YOLO-World prompting \nguide](https://blog.roboflow.com/yolo-world-prompting-tips/).\n",
        "name": "YOLO-World Model",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/yolo_world_model@v1",
              "YoloWorldModel",
              "YoloWorld"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "class_names": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              }
            ],
            "description": "One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects.",
            "examples": [
              ["person", "car", "license plate"],
              "$inputs.class_names"
            ],
            "title": "Class Names"
          },
          "version": {
            "anyOf": [
              {
                "enum": ["v2-s", "v2-m", "v2-l", "v2-x", "s", "m", "l", "x"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "v2-s",
            "description": "Variant of YoloWorld model",
            "examples": ["v2-s", "$inputs.variant"],
            "title": "Version"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": 0.005,
            "description": "Confidence threshold for detections",
            "examples": [0.005, "$inputs.confidence"],
            "title": "Confidence"
          }
        },
        "required": ["type", "name", "images", "class_names"],
        "short_description": "Run a zero-shot object detection model.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.yolo_world.v1.YoloWorldModelBlockV1",
      "human_friendly_block_name": "YOLO-World Model",
      "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
      "manifest_type_identifier_aliases": ["YoloWorldModel", "YoloWorld"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun inference on an instance segmentation model hosted on or uploaded to Roboflow.\n\nYou can query any model that is private to your account, or any public model available \non [Roboflow Universe](https://universe.roboflow.com).\n\nYou will need to set your Roboflow API key in your Inference environment to use this \nblock. To learn more about setting your Roboflow API key, [refer to the Inference \ndocumentation](https://inference.roboflow.com/quickstart/configure_api_key/).\n",
        "name": "Instance Segmentation Model",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_instance_segmentation_model@v1",
              "RoboflowInstanceSegmentationModel",
              "InstanceSegmentationModel"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "model_id": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow model id",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, models are\nidentified with special strings in the format: `<project_name>/<version>`. You should\nexpect this value to be provided once this kind is used. In some special cases, Roboflow \nplatform accepts model alias as model id which will not conform provided schema. List\nof aliases can be found [here](https://inference.roboflow.com/quickstart/aliases/).  \n",
                    "name": "roboflow_model_id"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Roboflow model identifier",
            "examples": ["my_project/3", "$inputs.model"],
            "title": "Model"
          },
          "class_agnostic_nms": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": false,
            "description": "Value to decide if NMS is to be used in class-agnostic mode.",
            "examples": [true, "$inputs.class_agnostic_nms"],
            "title": "Class Agnostic Nms"
          },
          "class_filter": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
            "examples": [["a", "b", "c"], "$inputs.class_filter"],
            "title": "Class Filter"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.4,
            "description": "Confidence threshold for predictions",
            "examples": [0.3, "$inputs.confidence_threshold"],
            "title": "Confidence"
          },
          "iou_threshold": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.3,
            "description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
            "examples": [0.4, "$inputs.iou_threshold"],
            "title": "Iou Threshold"
          },
          "max_detections": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 300,
            "description": "Maximum number of detections to return",
            "examples": [300, "$inputs.max_detections"],
            "title": "Max Detections"
          },
          "max_candidates": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 3000,
            "description": "Maximum number of candidates as NMS input to be taken into account.",
            "examples": [3000, "$inputs.max_candidates"],
            "title": "Max Candidates"
          },
          "mask_decode_mode": {
            "anyOf": [
              {
                "enum": ["accurate", "tradeoff", "fast"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "accurate",
            "description": "Parameter of mask decoding in prediction post-processing.",
            "examples": ["accurate", "$inputs.mask_decode_mode"],
            "title": "Mask Decode Mode"
          },
          "tradeoff_factor": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Post-processing parameter to dictate tradeoff between fast and accurate",
            "examples": [0.3, "$inputs.tradeoff_factor"],
            "title": "Tradeoff Factor"
          },
          "disable_active_learning": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Parameter to decide if Active Learning data sampling is disabled for the model",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Active Learning"
          },
          "active_learning_target_dataset": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
            "examples": ["my_project", "$inputs.al_target_project"],
            "title": "Active Learning Target Dataset"
          }
        },
        "required": ["type", "name", "images", "model_id"],
        "short_description": "Predict the shape, size, and location of objects.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "inference_id",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.roboflow.instance_segmentation.v1.RoboflowInstanceSegmentationModelBlockV1",
      "human_friendly_block_name": "Instance Segmentation Model",
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "manifest_type_identifier_aliases": [
        "RoboflowInstanceSegmentationModel",
        "InstanceSegmentationModel"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun inference on a keypoint detection model hosted on or uploaded to Roboflow.\n\nYou can query any model that is private to your account, or any public model available \non [Roboflow Universe](https://universe.roboflow.com).\n\nYou will need to set your Roboflow API key in your Inference environment to use this \nblock. To learn more about setting your Roboflow API key, [refer to the Inference \ndocumentation](https://inference.roboflow.com/quickstart/configure_api_key/).\n",
        "name": "Keypoint Detection Model",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_keypoint_detection_model@v1",
              "RoboflowKeypointDetectionModel",
              "KeypointsDetectionModel"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "model_id": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow model id",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, models are\nidentified with special strings in the format: `<project_name>/<version>`. You should\nexpect this value to be provided once this kind is used. In some special cases, Roboflow \nplatform accepts model alias as model id which will not conform provided schema. List\nof aliases can be found [here](https://inference.roboflow.com/quickstart/aliases/).  \n",
                    "name": "roboflow_model_id"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Roboflow model identifier",
            "examples": ["my_project/3", "$inputs.model"],
            "title": "Model"
          },
          "class_agnostic_nms": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": false,
            "description": "Value to decide if NMS is to be used in class-agnostic mode.",
            "examples": [true, "$inputs.class_agnostic_nms"],
            "title": "Class Agnostic Nms"
          },
          "class_filter": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
            "examples": [["a", "b", "c"], "$inputs.class_filter"],
            "title": "Class Filter"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.4,
            "description": "Confidence threshold for predictions",
            "examples": [0.3, "$inputs.confidence_threshold"],
            "title": "Confidence"
          },
          "iou_threshold": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.3,
            "description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
            "examples": [0.4, "$inputs.iou_threshold"],
            "title": "Iou Threshold"
          },
          "max_detections": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 300,
            "description": "Maximum number of detections to return",
            "examples": [300, "$inputs.max_detections"],
            "title": "Max Detections"
          },
          "max_candidates": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 3000,
            "description": "Maximum number of candidates as NMS input to be taken into account.",
            "examples": [3000, "$inputs.max_candidates"],
            "title": "Max Candidates"
          },
          "keypoint_confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Confidence threshold to predict keypoint as visible.",
            "examples": [0.3, "$inputs.keypoint_confidence"],
            "title": "Keypoint Confidence"
          },
          "disable_active_learning": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Parameter to decide if Active Learning data sampling is disabled for the model",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Active Learning"
          },
          "active_learning_target_dataset": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
            "examples": ["my_project", "$inputs.al_target_project"],
            "title": "Active Learning Target Dataset"
          }
        },
        "required": ["type", "name", "images", "model_id"],
        "short_description": "Predict skeletons on objects.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "inference_id",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[keypoint_detection_prediction]",
              "description": "`'predictions'` key from Keypoint Detection Model output",
              "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.roboflow.keypoint_detection.v1.RoboflowKeypointDetectionModelBlockV1",
      "human_friendly_block_name": "Keypoint Detection Model",
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "manifest_type_identifier_aliases": [
        "RoboflowKeypointDetectionModel",
        "KeypointsDetectionModel"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun inference on a multi-class classification model hosted on or uploaded to Roboflow.\n\nYou can query any model that is private to your account, or any public model available \non [Roboflow Universe](https://universe.roboflow.com).\n\nYou will need to set your Roboflow API key in your Inference environment to use this \nblock. To learn more about setting your Roboflow API key, [refer to the Inference \ndocumentation](https://inference.roboflow.com/quickstart/configure_api_key/).\n",
        "name": "Single-Label Classification Model",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_classification_model@v1",
              "RoboflowClassificationModel",
              "ClassificationModel"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "model_id": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow model id",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, models are\nidentified with special strings in the format: `<project_name>/<version>`. You should\nexpect this value to be provided once this kind is used. In some special cases, Roboflow \nplatform accepts model alias as model id which will not conform provided schema. List\nof aliases can be found [here](https://inference.roboflow.com/quickstart/aliases/).  \n",
                    "name": "roboflow_model_id"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Roboflow model identifier",
            "examples": ["my_project/3", "$inputs.model"],
            "title": "Model"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.4,
            "description": "Confidence threshold for predictions",
            "examples": [0.3, "$inputs.confidence_threshold"],
            "title": "Confidence"
          },
          "disable_active_learning": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Parameter to decide if Active Learning data sampling is disabled for the model",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Active Learning"
          },
          "active_learning_target_dataset": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
            "examples": ["my_project", "$inputs.al_target_project"],
            "title": "Active Learning Target Dataset"
          }
        },
        "required": ["type", "name", "images", "model_id"],
        "short_description": "Apply a single tag to an image.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "inference_id",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[classification_prediction]",
              "description": "`'predictions'` key from Classification Model outputs",
              "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.roboflow.multi_class_classification.v1.RoboflowClassificationModelBlockV1",
      "human_friendly_block_name": "Single-Label Classification Model",
      "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
      "manifest_type_identifier_aliases": [
        "RoboflowClassificationModel",
        "ClassificationModel"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun inference on a multi-label classification model hosted on or uploaded to Roboflow.\n\nYou can query any model that is private to your account, or any public model available \non [Roboflow Universe](https://universe.roboflow.com).\n\nYou will need to set your Roboflow API key in your Inference environment to use this \nblock. To learn more about setting your Roboflow API key, [refer to the Inference \ndocumentation](https://inference.roboflow.com/quickstart/configure_api_key/).\n",
        "name": "Multi-Label Classification Model",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_multi_label_classification_model@v1",
              "RoboflowMultiLabelClassificationModel",
              "MultiLabelClassificationModel"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "model_id": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow model id",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, models are\nidentified with special strings in the format: `<project_name>/<version>`. You should\nexpect this value to be provided once this kind is used. In some special cases, Roboflow \nplatform accepts model alias as model id which will not conform provided schema. List\nof aliases can be found [here](https://inference.roboflow.com/quickstart/aliases/).  \n",
                    "name": "roboflow_model_id"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Roboflow model identifier",
            "examples": ["my_project/3", "$inputs.model"],
            "title": "Model"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.4,
            "description": "Confidence threshold for predictions",
            "examples": [0.3, "$inputs.confidence_threshold"],
            "title": "Confidence"
          },
          "disable_active_learning": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Parameter to decide if Active Learning data sampling is disabled for the model",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Active Learning"
          },
          "active_learning_target_dataset": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
            "examples": ["my_project", "$inputs.al_target_project"],
            "title": "Active Learning Target Dataset"
          }
        },
        "required": ["type", "name", "images", "model_id"],
        "short_description": "Apply multiple tags to an image.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "inference_id",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[classification_prediction]",
              "description": "`'predictions'` key from Classification Model outputs",
              "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.roboflow.multi_label_classification.v1.RoboflowMultiLabelClassificationModelBlockV1",
      "human_friendly_block_name": "Multi-Label Classification Model",
      "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
      "manifest_type_identifier_aliases": [
        "RoboflowMultiLabelClassificationModel",
        "MultiLabelClassificationModel"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nRun inference on a object-detection model hosted on or uploaded to Roboflow.\n\nYou can query any model that is private to your account, or any public model available \non [Roboflow Universe](https://universe.roboflow.com).\n\nYou will need to set your Roboflow API key in your Inference environment to use this \nblock. To learn more about setting your Roboflow API key, [refer to the Inference \ndocumentation](https://inference.roboflow.com/quickstart/configure_api_key/).\n",
        "name": "Object Detection Model",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_object_detection_model@v1",
              "RoboflowObjectDetectionModel",
              "ObjectDetectionModel"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "model_id": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow model id",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, models are\nidentified with special strings in the format: `<project_name>/<version>`. You should\nexpect this value to be provided once this kind is used. In some special cases, Roboflow \nplatform accepts model alias as model id which will not conform provided schema. List\nof aliases can be found [here](https://inference.roboflow.com/quickstart/aliases/).  \n",
                    "name": "roboflow_model_id"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "Roboflow model identifier",
            "examples": ["my_project/3", "$inputs.model"],
            "title": "Model"
          },
          "class_agnostic_nms": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": false,
            "description": "Value to decide if NMS is to be used in class-agnostic mode.",
            "examples": [true, "$inputs.class_agnostic_nms"],
            "title": "Class Agnostic Nms"
          },
          "class_filter": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
            "examples": [["a", "b", "c"], "$inputs.class_filter"],
            "title": "Class Filter"
          },
          "confidence": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.4,
            "description": "Confidence threshold for predictions",
            "examples": [0.3, "$inputs.confidence_threshold"],
            "title": "Confidence"
          },
          "iou_threshold": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.3,
            "description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
            "examples": [0.4, "$inputs.iou_threshold"],
            "title": "Iou Threshold"
          },
          "max_detections": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 300,
            "description": "Maximum number of detections to return",
            "examples": [300, "$inputs.max_detections"],
            "title": "Max Detections"
          },
          "max_candidates": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 3000,
            "description": "Maximum number of candidates as NMS input to be taken into account.",
            "examples": [3000, "$inputs.max_candidates"],
            "title": "Max Candidates"
          },
          "disable_active_learning": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Parameter to decide if Active Learning data sampling is disabled for the model",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Active Learning"
          },
          "active_learning_target_dataset": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
            "examples": ["my_project", "$inputs.al_target_project"],
            "title": "Active Learning Target Dataset"
          }
        },
        "required": ["type", "name", "images", "model_id"],
        "short_description": "Predict the location of objects with bounding boxes.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "inference_id",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.roboflow.object_detection.v1.RoboflowObjectDetectionModelBlockV1",
      "human_friendly_block_name": "Object Detection Model",
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "manifest_type_identifier_aliases": [
        "RoboflowObjectDetectionModel",
        "ObjectDetectionModel"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nDetect the location of barcodes in an image.\n\nThis block is useful for manufacturing and consumer packaged goods projects where you \nneed to detect a barcode region in an image. You can then apply Crop block to isolate \neach barcode then apply further processing (i.e. OCR of the characters on a barcode).\n",
        "name": "Barcode Detection",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/barcode_detector@v1",
              "BarcodeDetector",
              "BarcodeDetection"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          }
        },
        "required": ["type", "name", "images"],
        "short_description": "Detect and read barcodes in an image.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[bar_code_detection]",
              "description": "Prediction with barcode detection",
              "docs": "\nThis kind represents batch of predictions regarding barcodes location and data their provide.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected barcodes (detections) and their metadata\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.third_party.barcode_detection.v1.BarcodeDetectorBlockV1",
      "human_friendly_block_name": "Barcode Detection",
      "manifest_type_identifier": "roboflow_core/barcode_detector@v1",
      "manifest_type_identifier_aliases": [
        "BarcodeDetector",
        "BarcodeDetection"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nDetect the location of a QR code.\n\nThis block is useful for manufacturing and consumer packaged goods projects where you \nneed to detect a QR code region in an image. You can then apply Crop block to isolate \neach QR code then apply further processing (i.e. read a QR code with a custom block).\n",
        "name": "QR Code Detection",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/qr_code_detector@v1",
              "QRCodeDetector",
              "QRCodeDetection"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          }
        },
        "required": ["type", "name", "images"],
        "short_description": "Detect and read QR codes in an image.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[bar_code_detection]",
              "description": "Prediction with barcode detection",
              "docs": "\nThis kind represents batch of predictions regarding barcodes location and data their provide.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected barcodes (detections) and their metadata\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.third_party.qr_code_detection.v1.QRCodeDetectorBlockV1",
      "human_friendly_block_name": "QR Code Detection",
      "manifest_type_identifier": "roboflow_core/qr_code_detector@v1",
      "manifest_type_identifier_aliases": ["QRCodeDetector", "QRCodeDetection"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nCrop a Region of Interest (RoI) from an image, using absolute coordinates.\n\nThis is useful when placed after an ObjectDetection block as part of a multi-stage \nworkflow. For example, you could use an ObjectDetection block to detect objects, then \nthe AbsoluteStaticCrop block to crop objects, then an OCR block to run character \nrecognition on each of the individual cropped regions.\n",
        "name": "Absolute Static Crop",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/absolute_static_crop@v1",
              "AbsoluteStaticCrop"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "x_center": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Center X of static crop (absolute coordinate)",
            "examples": [40, "$inputs.center_x"],
            "title": "X Center"
          },
          "y_center": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Center Y of static crop (absolute coordinate)",
            "examples": [40, "$inputs.center_y"],
            "title": "Y Center"
          },
          "width": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Width of static crop (absolute value)",
            "examples": [40, "$inputs.width"],
            "title": "Width"
          },
          "height": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Height of static crop (absolute value)",
            "examples": [40, "$inputs.height"],
            "title": "Height"
          }
        },
        "required": [
          "type",
          "name",
          "images",
          "x_center",
          "y_center",
          "width",
          "height"
        ],
        "short_description": "Crop an image using fixed pixel coordinates.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "crops",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.absolute_static_crop.v1.AbsoluteStaticCropBlockV1",
      "human_friendly_block_name": "Absolute Static Crop",
      "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
      "manifest_type_identifier_aliases": ["AbsoluteStaticCrop"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nCreate dynamic crops from an image based on detections from detections-based model.\n\nThis is useful when placed after an ObjectDetection block as part of a multi-stage \nworkflow. For example, you could use an ObjectDetection block to detect objects, then \nthe DynamicCropBlock block to crop objects, then an OCR block to run character recognition on \neach of the individual cropped regions.\n",
        "name": "Dynamic Crop",
        "properties": {
          "type": {
            "enum": ["roboflow_core/dynamic_crop@v1", "DynamicCrop", "Crop"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image to Crop"
          },
          "predictions": {
            "description": "The output of a detection model describing the bounding boxes that will be used to crop the image.",
            "examples": ["$steps.my_object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Regions of Interest",
            "type": "string"
          }
        },
        "required": ["type", "name", "images", "predictions"],
        "short_description": "Crop an image using bounding boxes from a detection model.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "crops",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.dynamic_crop.v1.DynamicCropBlockV1",
      "human_friendly_block_name": "Dynamic Crop",
      "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
      "manifest_type_identifier_aliases": ["DynamicCrop", "Crop"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 1
    },
    {
      "block_schema": {
        "$defs": {
          "BinaryStatement": {
            "properties": {
              "type": {
                "const": "BinaryStatement",
                "enum": ["BinaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "left_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Left Operand"
              },
              "comparator": {
                "discriminator": {
                  "mapping": {
                    "!=": "#/$defs/NotEquals",
                    "(Detection) in zone": "#/$defs/DetectionInZone",
                    "(Number) !=": "#/$defs/NotEquals",
                    "(Number) <": "#/$defs/NumberLower",
                    "(Number) <=": "#/$defs/NumberLowerEqual",
                    "(Number) ==": "#/$defs/Equals",
                    "(Number) >": "#/$defs/NumberGreater",
                    "(Number) >=": "#/$defs/NumberGreaterEqual",
                    "(String) contains": "#/$defs/StringContains",
                    "(String) endsWith": "#/$defs/StringEndsWith",
                    "(String) startsWith": "#/$defs/StringStartsWith",
                    "==": "#/$defs/Equals",
                    "in (Sequence)": "#/$defs/In"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/In"
                  },
                  {
                    "$ref": "#/$defs/StringContains"
                  },
                  {
                    "$ref": "#/$defs/StringEndsWith"
                  },
                  {
                    "$ref": "#/$defs/StringStartsWith"
                  },
                  {
                    "$ref": "#/$defs/NumberLowerEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberLower"
                  },
                  {
                    "$ref": "#/$defs/NumberGreaterEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberGreater"
                  },
                  {
                    "$ref": "#/$defs/NotEquals"
                  },
                  {
                    "$ref": "#/$defs/Equals"
                  },
                  {
                    "$ref": "#/$defs/DetectionInZone"
                  }
                ],
                "title": "Comparator"
              },
              "right_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Right Operand"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "left_operand", "comparator", "right_operand"],
            "title": "BinaryStatement",
            "type": "object"
          },
          "ClassificationProperty": {
            "enum": [
              "top_class",
              "top_class_confidence",
              "all_classes",
              "all_confidences"
            ],
            "title": "ClassificationProperty",
            "type": "string"
          },
          "ClassificationPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "Batch[classification_prediction]",
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ClassificationPropertyExtract",
                "enum": ["ClassificationPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ClassificationProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ClassificationPropertyExtract",
            "type": "object"
          },
          "DetectionInZone": {
            "description": "Checks if detection is in zone",
            "operands_kinds": [
              [
                {
                  "name": "detection",
                  "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                  "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
                },
                {
                  "name": "zone",
                  "description": "Definition of polygon zone",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Detection) in zone",
                "enum": ["(Detection) in zone"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DetectionInZone",
            "type": "object"
          },
          "DetectionsFilter": {
            "compound": true,
            "description": "Filters out unwanted elements from detections-based prediction by applying filter operation in context of every single detection within prediction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsFilter",
                "enum": ["DetectionsFilter"],
                "title": "Type",
                "type": "string"
              },
              "filter_operation": {
                "$ref": "#/$defs/StatementGroup"
              }
            },
            "required": ["type", "filter_operation"],
            "title": "DetectionsFilter",
            "type": "object"
          },
          "DetectionsOffset": {
            "compound": false,
            "description": "Makes detected bounding boxes bigger by applying offset to its size",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsOffset",
                "enum": ["DetectionsOffset"],
                "title": "Type",
                "type": "string"
              },
              "offset_x": {
                "title": "Offset X",
                "type": "integer"
              },
              "offset_y": {
                "title": "Offset Y",
                "type": "integer"
              }
            },
            "required": ["type", "offset_x", "offset_y"],
            "title": "DetectionsOffset",
            "type": "object"
          },
          "DetectionsProperty": {
            "enum": [
              "confidence",
              "class_name",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "class_id",
              "center",
              "top_left",
              "top_right",
              "bottom_left",
              "bottom_right"
            ],
            "title": "DetectionsProperty",
            "type": "string"
          },
          "DetectionsPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsPropertyExtract",
                "enum": ["DetectionsPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "DetectionsPropertyExtract",
            "type": "object"
          },
          "DetectionsSelection": {
            "compound": false,
            "description": "Selects bounding boxes based on predefined criterias",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsSelection",
                "enum": ["DetectionsSelection"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSelectionMode"
              }
            },
            "required": ["type", "mode"],
            "title": "DetectionsSelection",
            "type": "object"
          },
          "DetectionsSelectionMode": {
            "enum": ["left_most", "right_most"],
            "title": "DetectionsSelectionMode",
            "type": "string"
          },
          "DetectionsShift": {
            "compound": false,
            "description": "Shifting detected bounding boxes in assigned direction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsShift",
                "enum": ["DetectionsShift"],
                "title": "Type",
                "type": "string"
              },
              "shift_x": {
                "title": "Shift X",
                "type": "integer"
              },
              "shift_y": {
                "title": "Shift Y",
                "type": "integer"
              }
            },
            "required": ["type", "shift_x", "shift_y"],
            "title": "DetectionsShift",
            "type": "object"
          },
          "DetectionsSortProperties": {
            "enum": [
              "confidence",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "center_x",
              "center_y"
            ],
            "title": "DetectionsSortProperties",
            "type": "string"
          },
          "Divide": {
            "compound": false,
            "description": "Dividing value against other",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Divide",
                "enum": ["Divide"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Divide",
            "type": "object"
          },
          "DoesNotExist": {
            "description": "Checks if value is not given (`None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DoesNotExist",
                "enum": ["DoesNotExist"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DoesNotExist",
            "type": "object"
          },
          "DynamicOperand": {
            "properties": {
              "type": {
                "const": "DynamicOperand",
                "enum": ["DynamicOperand"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              },
              "operand_name": {
                "default": "_",
                "title": "Operand Name",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DynamicOperand",
            "type": "object"
          },
          "Equals": {
            "description": "Checks if two values given are equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) ==", "=="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Equals",
            "type": "object"
          },
          "Exists": {
            "description": "Checks if value is given (not `None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Exists",
                "enum": ["Exists"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Exists",
            "type": "object"
          },
          "ExtractDetectionProperty": {
            "compound": false,
            "description": "Extracts property from single detection",
            "input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractDetectionProperty",
                "enum": ["ExtractDetectionProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractDetectionProperty",
            "type": "object"
          },
          "ExtractImageProperty": {
            "description": "Extracts specific property of image (like size)",
            "input_kind": [
              {
                "name": "image",
                "description": "Image in workflows",
                "docs": "TODO"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractImageProperty",
                "enum": ["ExtractImageProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ImageProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractImageProperty",
            "type": "object"
          },
          "ImageProperty": {
            "enum": ["size", "height", "width"],
            "title": "ImageProperty",
            "type": "string"
          },
          "In": {
            "description": "Checks if first value is element of second value (usually list or dictionary)",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "in (Sequence)",
                "enum": ["in (Sequence)"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "In",
            "type": "object"
          },
          "IsEmpty": {
            "description": "Checks if sequence is empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is empty",
                "enum": ["(Sequence) is empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsEmpty",
            "type": "object"
          },
          "IsFalse": {
            "description": "Checks if value is `False`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is False",
                "enum": ["(Boolean) is False"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsFalse",
            "type": "object"
          },
          "IsNotEmpty": {
            "description": "Checks if sequence is not empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is not empty",
                "enum": ["(Sequence) is not empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsNotEmpty",
            "type": "object"
          },
          "IsTrue": {
            "description": "Checks if value is `True`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is True",
                "enum": ["(Boolean) is True"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsTrue",
            "type": "object"
          },
          "LookupTable": {
            "compound": false,
            "description": "Changes value according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "LookupTable",
                "enum": ["LookupTable"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "LookupTable",
            "type": "object"
          },
          "Multiply": {
            "compound": false,
            "description": "Multiplication",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Multiply",
                "enum": ["Multiply"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Multiply",
            "type": "object"
          },
          "NotEquals": {
            "description": "Checks if two values given are not equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) !=", "!="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NotEquals",
            "type": "object"
          },
          "NumberCastingMode": {
            "enum": ["int", "float"],
            "title": "NumberCastingMode",
            "type": "string"
          },
          "NumberGreater": {
            "description": "Checks if first value (number) is greater than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >",
                "enum": ["(Number) >"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreater",
            "type": "object"
          },
          "NumberGreaterEqual": {
            "description": "Checks if first value (number) is greater or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >=",
                "enum": ["(Number) >="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreaterEqual",
            "type": "object"
          },
          "NumberLower": {
            "description": "Checks if first value (number) is lower than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <",
                "enum": ["(Number) <"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLower",
            "type": "object"
          },
          "NumberLowerEqual": {
            "description": "Checks if first value (number) is lower or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <=",
                "enum": ["(Number) <="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLowerEqual",
            "type": "object"
          },
          "NumberRound": {
            "compound": false,
            "description": "Rounds the number",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumberRound",
                "enum": ["NumberRound"],
                "title": "Type",
                "type": "string"
              },
              "decimal_digits": {
                "title": "Decimal Digits",
                "type": "integer"
              }
            },
            "required": ["type", "decimal_digits"],
            "title": "NumberRound",
            "type": "object"
          },
          "NumericSequenceAggregate": {
            "compound": false,
            "description": "Aggregates numeric sequence using aggregation function like min or max - adjusted to work on numbers",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumericSequenceAggregate",
                "enum": ["NumericSequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "function": {
                "$ref": "#/$defs/SequenceAggregationFunction"
              },
              "neutral_value": {
                "default": null,
                "title": "Neutral Value"
              }
            },
            "required": ["type", "function"],
            "title": "NumericSequenceAggregate",
            "type": "object"
          },
          "RandomNumber": {
            "description": "Special operation to let random sampling - ignoring input data and changing it into random floating point value. To be used mainly to sample predictions or images.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "RandomNumber",
                "enum": ["RandomNumber"],
                "title": "Type",
                "type": "string"
              },
              "min_value": {
                "default": 0,
                "title": "Min Value",
                "type": "number"
              },
              "max_value": {
                "default": 1,
                "title": "Max Value",
                "type": "number"
              }
            },
            "required": ["type"],
            "title": "RandomNumber",
            "type": "object"
          },
          "SequenceAggregate": {
            "compound": false,
            "description": "Aggregates sequence using generic aggregation methods - adjusted to majority data types",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceAggregate",
                "enum": ["SequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/SequenceAggregationMode"
              }
            },
            "required": ["type", "mode"],
            "title": "SequenceAggregate",
            "type": "object"
          },
          "SequenceAggregationFunction": {
            "enum": ["min", "max"],
            "title": "SequenceAggregationFunction",
            "type": "string"
          },
          "SequenceAggregationMode": {
            "enum": ["first", "last", "most_common", "least_common"],
            "title": "SequenceAggregationMode",
            "type": "string"
          },
          "SequenceApply": {
            "compound": true,
            "description": "Operation applies chain of operations at every element of sequence",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceApply",
                "enum": ["SequenceApply"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "operations"],
            "title": "SequenceApply",
            "type": "object"
          },
          "SequenceLength": {
            "compound": false,
            "description": "Operation determines the length of input sequence",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "dictionary",
                "description": "Dictionary",
                "docs": null
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceLength",
                "enum": ["SequenceLength"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "SequenceLength",
            "type": "object"
          },
          "SequenceMap": {
            "compound": true,
            "description": "Changes each value of sequence according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceMap",
                "enum": ["SequenceMap"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "SequenceMap",
            "type": "object"
          },
          "SortDetections": {
            "compound": false,
            "description": "Changes the order of detected bounding boxes.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SortDetections",
                "enum": ["SortDetections"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSortProperties"
              },
              "ascending": {
                "default": true,
                "title": "Ascending",
                "type": "boolean"
              }
            },
            "required": ["type", "mode"],
            "title": "SortDetections",
            "type": "object"
          },
          "StatementGroup": {
            "properties": {
              "type": {
                "const": "StatementGroup",
                "enum": ["StatementGroup"],
                "title": "Type",
                "type": "string"
              },
              "statements": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "BinaryStatement": "#/$defs/BinaryStatement",
                      "StatementGroup": "#/$defs/StatementGroup",
                      "UnaryStatement": "#/$defs/UnaryStatement"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/BinaryStatement"
                    },
                    {
                      "$ref": "#/$defs/UnaryStatement"
                    },
                    {
                      "$ref": "#/$defs/StatementGroup"
                    }
                  ]
                },
                "minItems": 1,
                "title": "Statements",
                "type": "array"
              },
              "operator": {
                "allOf": [
                  {
                    "$ref": "#/$defs/StatementsGroupsOperator"
                  }
                ],
                "default": "or"
              }
            },
            "required": ["type", "statements"],
            "title": "StatementGroup",
            "type": "object"
          },
          "StatementsGroupsOperator": {
            "enum": ["and", "or"],
            "title": "StatementsGroupsOperator",
            "type": "string"
          },
          "StaticOperand": {
            "properties": {
              "type": {
                "const": "StaticOperand",
                "enum": ["StaticOperand"],
                "title": "Type",
                "type": "string"
              },
              "value": {
                "title": "Value"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "value"],
            "title": "StaticOperand",
            "type": "object"
          },
          "StringContains": {
            "description": "Checks if string given as first value contains string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) contains",
                "enum": ["(String) contains"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringContains",
            "type": "object"
          },
          "StringEndsWith": {
            "description": "Checks if string given as first value ends with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) endsWith",
                "enum": ["(String) endsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringEndsWith",
            "type": "object"
          },
          "StringMatches": {
            "compound": false,
            "description": "Checks if string matches regex",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringMatches",
                "enum": ["StringMatches"],
                "title": "Type",
                "type": "string"
              },
              "regex": {
                "title": "Regex",
                "type": "string"
              }
            },
            "required": ["type", "regex"],
            "title": "StringMatches",
            "type": "object"
          },
          "StringStartsWith": {
            "description": "Checks if string given as first value starts with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) startsWith",
                "enum": ["(String) startsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringStartsWith",
            "type": "object"
          },
          "StringSubSequence": {
            "compound": false,
            "description": "Takes sub-string of the input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringSubSequence",
                "enum": ["StringSubSequence"],
                "title": "Type",
                "type": "string"
              },
              "start": {
                "default": 0,
                "title": "Start",
                "type": "integer"
              },
              "end": {
                "default": -1,
                "title": "End",
                "type": "integer"
              }
            },
            "required": ["type"],
            "title": "StringSubSequence",
            "type": "object"
          },
          "StringToLowerCase": {
            "compound": false,
            "description": "Executes lowercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToLowerCase",
                "enum": ["StringToLowerCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToLowerCase",
            "type": "object"
          },
          "StringToUpperCase": {
            "compound": false,
            "description": "Executes uppercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToUpperCase",
                "enum": ["StringToUpperCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToUpperCase",
            "type": "object"
          },
          "ToBoolean": {
            "compound": false,
            "description": "Changes input data into boolean",
            "input_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToBoolean",
                "enum": ["ToBoolean"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToBoolean",
            "type": "object"
          },
          "ToNumber": {
            "compound": false,
            "description": "Changes value into number - float or int depending on configuration",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToNumber",
                "enum": ["ToNumber"],
                "title": "Type",
                "type": "string"
              },
              "cast_to": {
                "$ref": "#/$defs/NumberCastingMode"
              }
            },
            "required": ["type", "cast_to"],
            "title": "ToNumber",
            "type": "object"
          },
          "ToString": {
            "compound": false,
            "description": "Stringifies data",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToString",
                "enum": ["ToString"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToString",
            "type": "object"
          },
          "UnaryStatement": {
            "properties": {
              "type": {
                "const": "UnaryStatement",
                "enum": ["UnaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Operand"
              },
              "operator": {
                "discriminator": {
                  "mapping": {
                    "(Boolean) is False": "#/$defs/IsFalse",
                    "(Boolean) is True": "#/$defs/IsTrue",
                    "(Sequence) is empty": "#/$defs/IsEmpty",
                    "(Sequence) is not empty": "#/$defs/IsNotEmpty",
                    "DoesNotExist": "#/$defs/DoesNotExist",
                    "Exists": "#/$defs/Exists"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/Exists"
                  },
                  {
                    "$ref": "#/$defs/DoesNotExist"
                  },
                  {
                    "$ref": "#/$defs/IsTrue"
                  },
                  {
                    "$ref": "#/$defs/IsFalse"
                  },
                  {
                    "$ref": "#/$defs/IsEmpty"
                  },
                  {
                    "$ref": "#/$defs/IsNotEmpty"
                  }
                ],
                "title": "Operator"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "operand", "operator"],
            "title": "UnaryStatement",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "Conditionally filter out model predictions.",
        "name": "Detections Filter",
        "properties": {
          "type": {
            "enum": ["roboflow_core/detections_filter@v1", "DetectionsFilter"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Reference to detection-like predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "operations": {
            "items": {
              "discriminator": {
                "mapping": {
                  "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                  "DetectionsFilter": "#/$defs/DetectionsFilter",
                  "DetectionsOffset": "#/$defs/DetectionsOffset",
                  "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                  "DetectionsSelection": "#/$defs/DetectionsSelection",
                  "DetectionsShift": "#/$defs/DetectionsShift",
                  "Divide": "#/$defs/Divide",
                  "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                  "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                  "LookupTable": "#/$defs/LookupTable",
                  "Multiply": "#/$defs/Multiply",
                  "NumberRound": "#/$defs/NumberRound",
                  "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                  "RandomNumber": "#/$defs/RandomNumber",
                  "SequenceAggregate": "#/$defs/SequenceAggregate",
                  "SequenceApply": "#/$defs/SequenceApply",
                  "SequenceLength": "#/$defs/SequenceLength",
                  "SequenceMap": "#/$defs/SequenceMap",
                  "SortDetections": "#/$defs/SortDetections",
                  "StringMatches": "#/$defs/StringMatches",
                  "StringSubSequence": "#/$defs/StringSubSequence",
                  "StringToLowerCase": "#/$defs/StringToLowerCase",
                  "StringToUpperCase": "#/$defs/StringToUpperCase",
                  "ToBoolean": "#/$defs/ToBoolean",
                  "ToNumber": "#/$defs/ToNumber",
                  "ToString": "#/$defs/ToString"
                },
                "propertyName": "type"
              },
              "oneOf": [
                {
                  "$ref": "#/$defs/StringToLowerCase"
                },
                {
                  "$ref": "#/$defs/StringToUpperCase"
                },
                {
                  "$ref": "#/$defs/LookupTable"
                },
                {
                  "$ref": "#/$defs/ToNumber"
                },
                {
                  "$ref": "#/$defs/NumberRound"
                },
                {
                  "$ref": "#/$defs/SequenceMap"
                },
                {
                  "$ref": "#/$defs/SequenceApply"
                },
                {
                  "$ref": "#/$defs/NumericSequenceAggregate"
                },
                {
                  "$ref": "#/$defs/ToString"
                },
                {
                  "$ref": "#/$defs/ToBoolean"
                },
                {
                  "$ref": "#/$defs/StringSubSequence"
                },
                {
                  "$ref": "#/$defs/DetectionsPropertyExtract"
                },
                {
                  "$ref": "#/$defs/SequenceAggregate"
                },
                {
                  "$ref": "#/$defs/ExtractDetectionProperty"
                },
                {
                  "$ref": "#/$defs/DetectionsFilter"
                },
                {
                  "$ref": "#/$defs/DetectionsOffset"
                },
                {
                  "$ref": "#/$defs/DetectionsShift"
                },
                {
                  "$ref": "#/$defs/RandomNumber"
                },
                {
                  "$ref": "#/$defs/StringMatches"
                },
                {
                  "$ref": "#/$defs/ExtractImageProperty"
                },
                {
                  "$ref": "#/$defs/SequenceLength"
                },
                {
                  "$ref": "#/$defs/Multiply"
                },
                {
                  "$ref": "#/$defs/Divide"
                },
                {
                  "$ref": "#/$defs/DetectionsSelection"
                },
                {
                  "$ref": "#/$defs/SortDetections"
                },
                {
                  "$ref": "#/$defs/ClassificationPropertyExtract"
                }
              ]
            },
            "title": "Operations",
            "type": "array"
          },
          "operations_parameters": {
            "additionalProperties": {
              "anyOf": [
                {
                  "kind": [
                    {
                      "description": "Image in workflows",
                      "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                      "name": "Batch[image]"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_image",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_parameter",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                  "reference": true,
                  "selected_element": "step_output",
                  "type": "string"
                }
              ]
            },
            "description": "References to additional parameters that may be provided in runtime to parametrise operations",
            "examples": ["$inputs.confidence", "$inputs.image"],
            "title": "Operations Parameters",
            "type": "object"
          }
        },
        "required": ["type", "name", "predictions", "operations"],
        "short_description": "Conditionally filter out model predictions.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[keypoint_detection_prediction]",
              "description": "`'predictions'` key from Keypoint Detection Model output",
              "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.detections_filter.v1.DetectionsFilterBlockV1",
      "human_friendly_block_name": "Detections Filter",
      "manifest_type_identifier": "roboflow_core/detections_filter@v1",
      "manifest_type_identifier_aliases": ["DetectionsFilter"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nApply a fixed offset to the width and height of a detection.\n\nYou can use this block to add padding around the result of a detection. This is useful \nto ensure that you can analyze bounding boxes that may be within the region of an \nobject instead of being around an object.\n",
        "name": "Detection Offset",
        "properties": {
          "type": {
            "enum": ["roboflow_core/detection_offset@v1", "DetectionOffset"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Reference to detection-like predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "offset_width": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Offset for boxes width",
            "examples": [10, "$inputs.offset_x"],
            "title": "Offset Width"
          },
          "offset_height": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Offset for boxes height",
            "examples": [10, "$inputs.offset_y"],
            "title": "Offset Height"
          }
        },
        "required": [
          "type",
          "name",
          "predictions",
          "offset_width",
          "offset_height"
        ],
        "short_description": "Apply a padding around the width and height of detections.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[keypoint_detection_prediction]",
              "description": "`'predictions'` key from Keypoint Detection Model output",
              "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.detection_offset.v1.DetectionOffsetBlockV1",
      "human_friendly_block_name": "Detection Offset",
      "manifest_type_identifier": "roboflow_core/detection_offset@v1",
      "manifest_type_identifier_aliases": ["DetectionOffset"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nCrop a Region of Interest (RoI) from an image, using relative coordinates.\n\nThis is useful when placed after an ObjectDetection block as part of a multi-stage \nworkflow. For example, you could use an ObjectDetection block to detect objects, then \nthe RelativeStaticCrop block to crop objects, then an OCR block to run character \nrecognition on each of the individual cropped regions.\n",
        "name": "Relative Static Crop",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/relative_statoic_crop@v1",
              "RelativeStaticCrop"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "x_center": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Center X of static crop (relative coordinate 0.0-1.0)",
            "examples": [0.3, "$inputs.center_x"],
            "title": "X Center"
          },
          "y_center": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Center Y of static crop (relative coordinate 0.0-1.0)",
            "examples": [0.3, "$inputs.center_y"],
            "title": "Y Center"
          },
          "width": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Width of static crop (relative value 0.0-1.0)",
            "examples": [0.3, "$inputs.width"],
            "title": "Width"
          },
          "height": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Height of static crop (relative value 0.0-1.0)",
            "examples": [0.3, "$inputs.height"],
            "title": "Height"
          }
        },
        "required": [
          "type",
          "name",
          "images",
          "x_center",
          "y_center",
          "width",
          "height"
        ],
        "short_description": "Crop an image proportional (%) to its dimensions.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "crops",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.relative_static_crop.v1.RelativeStaticCropBlockV1",
      "human_friendly_block_name": "Relative Static Crop",
      "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
      "manifest_type_identifier_aliases": ["RelativeStaticCrop"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "BinaryStatement": {
            "properties": {
              "type": {
                "const": "BinaryStatement",
                "enum": ["BinaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "left_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Left Operand"
              },
              "comparator": {
                "discriminator": {
                  "mapping": {
                    "!=": "#/$defs/NotEquals",
                    "(Detection) in zone": "#/$defs/DetectionInZone",
                    "(Number) !=": "#/$defs/NotEquals",
                    "(Number) <": "#/$defs/NumberLower",
                    "(Number) <=": "#/$defs/NumberLowerEqual",
                    "(Number) ==": "#/$defs/Equals",
                    "(Number) >": "#/$defs/NumberGreater",
                    "(Number) >=": "#/$defs/NumberGreaterEqual",
                    "(String) contains": "#/$defs/StringContains",
                    "(String) endsWith": "#/$defs/StringEndsWith",
                    "(String) startsWith": "#/$defs/StringStartsWith",
                    "==": "#/$defs/Equals",
                    "in (Sequence)": "#/$defs/In"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/In"
                  },
                  {
                    "$ref": "#/$defs/StringContains"
                  },
                  {
                    "$ref": "#/$defs/StringEndsWith"
                  },
                  {
                    "$ref": "#/$defs/StringStartsWith"
                  },
                  {
                    "$ref": "#/$defs/NumberLowerEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberLower"
                  },
                  {
                    "$ref": "#/$defs/NumberGreaterEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberGreater"
                  },
                  {
                    "$ref": "#/$defs/NotEquals"
                  },
                  {
                    "$ref": "#/$defs/Equals"
                  },
                  {
                    "$ref": "#/$defs/DetectionInZone"
                  }
                ],
                "title": "Comparator"
              },
              "right_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Right Operand"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "left_operand", "comparator", "right_operand"],
            "title": "BinaryStatement",
            "type": "object"
          },
          "ClassificationProperty": {
            "enum": [
              "top_class",
              "top_class_confidence",
              "all_classes",
              "all_confidences"
            ],
            "title": "ClassificationProperty",
            "type": "string"
          },
          "ClassificationPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "Batch[classification_prediction]",
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ClassificationPropertyExtract",
                "enum": ["ClassificationPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ClassificationProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ClassificationPropertyExtract",
            "type": "object"
          },
          "DetectionInZone": {
            "description": "Checks if detection is in zone",
            "operands_kinds": [
              [
                {
                  "name": "detection",
                  "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                  "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
                },
                {
                  "name": "zone",
                  "description": "Definition of polygon zone",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Detection) in zone",
                "enum": ["(Detection) in zone"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DetectionInZone",
            "type": "object"
          },
          "DetectionsFilter": {
            "compound": true,
            "description": "Filters out unwanted elements from detections-based prediction by applying filter operation in context of every single detection within prediction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsFilter",
                "enum": ["DetectionsFilter"],
                "title": "Type",
                "type": "string"
              },
              "filter_operation": {
                "$ref": "#/$defs/StatementGroup"
              }
            },
            "required": ["type", "filter_operation"],
            "title": "DetectionsFilter",
            "type": "object"
          },
          "DetectionsOffset": {
            "compound": false,
            "description": "Makes detected bounding boxes bigger by applying offset to its size",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsOffset",
                "enum": ["DetectionsOffset"],
                "title": "Type",
                "type": "string"
              },
              "offset_x": {
                "title": "Offset X",
                "type": "integer"
              },
              "offset_y": {
                "title": "Offset Y",
                "type": "integer"
              }
            },
            "required": ["type", "offset_x", "offset_y"],
            "title": "DetectionsOffset",
            "type": "object"
          },
          "DetectionsProperty": {
            "enum": [
              "confidence",
              "class_name",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "class_id",
              "center",
              "top_left",
              "top_right",
              "bottom_left",
              "bottom_right"
            ],
            "title": "DetectionsProperty",
            "type": "string"
          },
          "DetectionsPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsPropertyExtract",
                "enum": ["DetectionsPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "DetectionsPropertyExtract",
            "type": "object"
          },
          "DetectionsSelection": {
            "compound": false,
            "description": "Selects bounding boxes based on predefined criterias",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsSelection",
                "enum": ["DetectionsSelection"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSelectionMode"
              }
            },
            "required": ["type", "mode"],
            "title": "DetectionsSelection",
            "type": "object"
          },
          "DetectionsSelectionMode": {
            "enum": ["left_most", "right_most"],
            "title": "DetectionsSelectionMode",
            "type": "string"
          },
          "DetectionsShift": {
            "compound": false,
            "description": "Shifting detected bounding boxes in assigned direction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsShift",
                "enum": ["DetectionsShift"],
                "title": "Type",
                "type": "string"
              },
              "shift_x": {
                "title": "Shift X",
                "type": "integer"
              },
              "shift_y": {
                "title": "Shift Y",
                "type": "integer"
              }
            },
            "required": ["type", "shift_x", "shift_y"],
            "title": "DetectionsShift",
            "type": "object"
          },
          "DetectionsSortProperties": {
            "enum": [
              "confidence",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "center_x",
              "center_y"
            ],
            "title": "DetectionsSortProperties",
            "type": "string"
          },
          "Divide": {
            "compound": false,
            "description": "Dividing value against other",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Divide",
                "enum": ["Divide"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Divide",
            "type": "object"
          },
          "DoesNotExist": {
            "description": "Checks if value is not given (`None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DoesNotExist",
                "enum": ["DoesNotExist"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DoesNotExist",
            "type": "object"
          },
          "DynamicOperand": {
            "properties": {
              "type": {
                "const": "DynamicOperand",
                "enum": ["DynamicOperand"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              },
              "operand_name": {
                "default": "_",
                "title": "Operand Name",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DynamicOperand",
            "type": "object"
          },
          "Equals": {
            "description": "Checks if two values given are equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) ==", "=="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Equals",
            "type": "object"
          },
          "Exists": {
            "description": "Checks if value is given (not `None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Exists",
                "enum": ["Exists"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Exists",
            "type": "object"
          },
          "ExtractDetectionProperty": {
            "compound": false,
            "description": "Extracts property from single detection",
            "input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractDetectionProperty",
                "enum": ["ExtractDetectionProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractDetectionProperty",
            "type": "object"
          },
          "ExtractImageProperty": {
            "description": "Extracts specific property of image (like size)",
            "input_kind": [
              {
                "name": "image",
                "description": "Image in workflows",
                "docs": "TODO"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractImageProperty",
                "enum": ["ExtractImageProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ImageProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractImageProperty",
            "type": "object"
          },
          "ImageProperty": {
            "enum": ["size", "height", "width"],
            "title": "ImageProperty",
            "type": "string"
          },
          "In": {
            "description": "Checks if first value is element of second value (usually list or dictionary)",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "in (Sequence)",
                "enum": ["in (Sequence)"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "In",
            "type": "object"
          },
          "IsEmpty": {
            "description": "Checks if sequence is empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is empty",
                "enum": ["(Sequence) is empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsEmpty",
            "type": "object"
          },
          "IsFalse": {
            "description": "Checks if value is `False`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is False",
                "enum": ["(Boolean) is False"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsFalse",
            "type": "object"
          },
          "IsNotEmpty": {
            "description": "Checks if sequence is not empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is not empty",
                "enum": ["(Sequence) is not empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsNotEmpty",
            "type": "object"
          },
          "IsTrue": {
            "description": "Checks if value is `True`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is True",
                "enum": ["(Boolean) is True"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsTrue",
            "type": "object"
          },
          "LookupTable": {
            "compound": false,
            "description": "Changes value according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "LookupTable",
                "enum": ["LookupTable"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "LookupTable",
            "type": "object"
          },
          "Multiply": {
            "compound": false,
            "description": "Multiplication",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Multiply",
                "enum": ["Multiply"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Multiply",
            "type": "object"
          },
          "NotEquals": {
            "description": "Checks if two values given are not equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) !=", "!="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NotEquals",
            "type": "object"
          },
          "NumberCastingMode": {
            "enum": ["int", "float"],
            "title": "NumberCastingMode",
            "type": "string"
          },
          "NumberGreater": {
            "description": "Checks if first value (number) is greater than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >",
                "enum": ["(Number) >"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreater",
            "type": "object"
          },
          "NumberGreaterEqual": {
            "description": "Checks if first value (number) is greater or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >=",
                "enum": ["(Number) >="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreaterEqual",
            "type": "object"
          },
          "NumberLower": {
            "description": "Checks if first value (number) is lower than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <",
                "enum": ["(Number) <"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLower",
            "type": "object"
          },
          "NumberLowerEqual": {
            "description": "Checks if first value (number) is lower or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <=",
                "enum": ["(Number) <="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLowerEqual",
            "type": "object"
          },
          "NumberRound": {
            "compound": false,
            "description": "Rounds the number",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumberRound",
                "enum": ["NumberRound"],
                "title": "Type",
                "type": "string"
              },
              "decimal_digits": {
                "title": "Decimal Digits",
                "type": "integer"
              }
            },
            "required": ["type", "decimal_digits"],
            "title": "NumberRound",
            "type": "object"
          },
          "NumericSequenceAggregate": {
            "compound": false,
            "description": "Aggregates numeric sequence using aggregation function like min or max - adjusted to work on numbers",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumericSequenceAggregate",
                "enum": ["NumericSequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "function": {
                "$ref": "#/$defs/SequenceAggregationFunction"
              },
              "neutral_value": {
                "default": null,
                "title": "Neutral Value"
              }
            },
            "required": ["type", "function"],
            "title": "NumericSequenceAggregate",
            "type": "object"
          },
          "RandomNumber": {
            "description": "Special operation to let random sampling - ignoring input data and changing it into random floating point value. To be used mainly to sample predictions or images.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "RandomNumber",
                "enum": ["RandomNumber"],
                "title": "Type",
                "type": "string"
              },
              "min_value": {
                "default": 0,
                "title": "Min Value",
                "type": "number"
              },
              "max_value": {
                "default": 1,
                "title": "Max Value",
                "type": "number"
              }
            },
            "required": ["type"],
            "title": "RandomNumber",
            "type": "object"
          },
          "SequenceAggregate": {
            "compound": false,
            "description": "Aggregates sequence using generic aggregation methods - adjusted to majority data types",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceAggregate",
                "enum": ["SequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/SequenceAggregationMode"
              }
            },
            "required": ["type", "mode"],
            "title": "SequenceAggregate",
            "type": "object"
          },
          "SequenceAggregationFunction": {
            "enum": ["min", "max"],
            "title": "SequenceAggregationFunction",
            "type": "string"
          },
          "SequenceAggregationMode": {
            "enum": ["first", "last", "most_common", "least_common"],
            "title": "SequenceAggregationMode",
            "type": "string"
          },
          "SequenceApply": {
            "compound": true,
            "description": "Operation applies chain of operations at every element of sequence",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceApply",
                "enum": ["SequenceApply"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "operations"],
            "title": "SequenceApply",
            "type": "object"
          },
          "SequenceLength": {
            "compound": false,
            "description": "Operation determines the length of input sequence",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "dictionary",
                "description": "Dictionary",
                "docs": null
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceLength",
                "enum": ["SequenceLength"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "SequenceLength",
            "type": "object"
          },
          "SequenceMap": {
            "compound": true,
            "description": "Changes each value of sequence according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceMap",
                "enum": ["SequenceMap"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "SequenceMap",
            "type": "object"
          },
          "SortDetections": {
            "compound": false,
            "description": "Changes the order of detected bounding boxes.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SortDetections",
                "enum": ["SortDetections"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSortProperties"
              },
              "ascending": {
                "default": true,
                "title": "Ascending",
                "type": "boolean"
              }
            },
            "required": ["type", "mode"],
            "title": "SortDetections",
            "type": "object"
          },
          "StatementGroup": {
            "properties": {
              "type": {
                "const": "StatementGroup",
                "enum": ["StatementGroup"],
                "title": "Type",
                "type": "string"
              },
              "statements": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "BinaryStatement": "#/$defs/BinaryStatement",
                      "StatementGroup": "#/$defs/StatementGroup",
                      "UnaryStatement": "#/$defs/UnaryStatement"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/BinaryStatement"
                    },
                    {
                      "$ref": "#/$defs/UnaryStatement"
                    },
                    {
                      "$ref": "#/$defs/StatementGroup"
                    }
                  ]
                },
                "minItems": 1,
                "title": "Statements",
                "type": "array"
              },
              "operator": {
                "allOf": [
                  {
                    "$ref": "#/$defs/StatementsGroupsOperator"
                  }
                ],
                "default": "or"
              }
            },
            "required": ["type", "statements"],
            "title": "StatementGroup",
            "type": "object"
          },
          "StatementsGroupsOperator": {
            "enum": ["and", "or"],
            "title": "StatementsGroupsOperator",
            "type": "string"
          },
          "StaticOperand": {
            "properties": {
              "type": {
                "const": "StaticOperand",
                "enum": ["StaticOperand"],
                "title": "Type",
                "type": "string"
              },
              "value": {
                "title": "Value"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "value"],
            "title": "StaticOperand",
            "type": "object"
          },
          "StringContains": {
            "description": "Checks if string given as first value contains string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) contains",
                "enum": ["(String) contains"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringContains",
            "type": "object"
          },
          "StringEndsWith": {
            "description": "Checks if string given as first value ends with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) endsWith",
                "enum": ["(String) endsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringEndsWith",
            "type": "object"
          },
          "StringMatches": {
            "compound": false,
            "description": "Checks if string matches regex",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringMatches",
                "enum": ["StringMatches"],
                "title": "Type",
                "type": "string"
              },
              "regex": {
                "title": "Regex",
                "type": "string"
              }
            },
            "required": ["type", "regex"],
            "title": "StringMatches",
            "type": "object"
          },
          "StringStartsWith": {
            "description": "Checks if string given as first value starts with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) startsWith",
                "enum": ["(String) startsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringStartsWith",
            "type": "object"
          },
          "StringSubSequence": {
            "compound": false,
            "description": "Takes sub-string of the input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringSubSequence",
                "enum": ["StringSubSequence"],
                "title": "Type",
                "type": "string"
              },
              "start": {
                "default": 0,
                "title": "Start",
                "type": "integer"
              },
              "end": {
                "default": -1,
                "title": "End",
                "type": "integer"
              }
            },
            "required": ["type"],
            "title": "StringSubSequence",
            "type": "object"
          },
          "StringToLowerCase": {
            "compound": false,
            "description": "Executes lowercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToLowerCase",
                "enum": ["StringToLowerCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToLowerCase",
            "type": "object"
          },
          "StringToUpperCase": {
            "compound": false,
            "description": "Executes uppercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToUpperCase",
                "enum": ["StringToUpperCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToUpperCase",
            "type": "object"
          },
          "ToBoolean": {
            "compound": false,
            "description": "Changes input data into boolean",
            "input_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToBoolean",
                "enum": ["ToBoolean"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToBoolean",
            "type": "object"
          },
          "ToNumber": {
            "compound": false,
            "description": "Changes value into number - float or int depending on configuration",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToNumber",
                "enum": ["ToNumber"],
                "title": "Type",
                "type": "string"
              },
              "cast_to": {
                "$ref": "#/$defs/NumberCastingMode"
              }
            },
            "required": ["type", "cast_to"],
            "title": "ToNumber",
            "type": "object"
          },
          "ToString": {
            "compound": false,
            "description": "Stringifies data",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToString",
                "enum": ["ToString"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToString",
            "type": "object"
          },
          "UnaryStatement": {
            "properties": {
              "type": {
                "const": "UnaryStatement",
                "enum": ["UnaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Operand"
              },
              "operator": {
                "discriminator": {
                  "mapping": {
                    "(Boolean) is False": "#/$defs/IsFalse",
                    "(Boolean) is True": "#/$defs/IsTrue",
                    "(Sequence) is empty": "#/$defs/IsEmpty",
                    "(Sequence) is not empty": "#/$defs/IsNotEmpty",
                    "DoesNotExist": "#/$defs/DoesNotExist",
                    "Exists": "#/$defs/Exists"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/Exists"
                  },
                  {
                    "$ref": "#/$defs/DoesNotExist"
                  },
                  {
                    "$ref": "#/$defs/IsTrue"
                  },
                  {
                    "$ref": "#/$defs/IsFalse"
                  },
                  {
                    "$ref": "#/$defs/IsEmpty"
                  },
                  {
                    "$ref": "#/$defs/IsNotEmpty"
                  }
                ],
                "title": "Operator"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "operand", "operator"],
            "title": "UnaryStatement",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nBlock changes detected Bounding Boxes in a way specified in configuration.\n\nIt supports such operations as changing the size of Bounding Boxes. \n",
        "name": "Detections Transformation",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/detections_transformation@v1",
              "DetectionsTransformation"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Reference to detection-like predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "operations": {
            "items": {
              "discriminator": {
                "mapping": {
                  "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                  "DetectionsFilter": "#/$defs/DetectionsFilter",
                  "DetectionsOffset": "#/$defs/DetectionsOffset",
                  "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                  "DetectionsSelection": "#/$defs/DetectionsSelection",
                  "DetectionsShift": "#/$defs/DetectionsShift",
                  "Divide": "#/$defs/Divide",
                  "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                  "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                  "LookupTable": "#/$defs/LookupTable",
                  "Multiply": "#/$defs/Multiply",
                  "NumberRound": "#/$defs/NumberRound",
                  "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                  "RandomNumber": "#/$defs/RandomNumber",
                  "SequenceAggregate": "#/$defs/SequenceAggregate",
                  "SequenceApply": "#/$defs/SequenceApply",
                  "SequenceLength": "#/$defs/SequenceLength",
                  "SequenceMap": "#/$defs/SequenceMap",
                  "SortDetections": "#/$defs/SortDetections",
                  "StringMatches": "#/$defs/StringMatches",
                  "StringSubSequence": "#/$defs/StringSubSequence",
                  "StringToLowerCase": "#/$defs/StringToLowerCase",
                  "StringToUpperCase": "#/$defs/StringToUpperCase",
                  "ToBoolean": "#/$defs/ToBoolean",
                  "ToNumber": "#/$defs/ToNumber",
                  "ToString": "#/$defs/ToString"
                },
                "propertyName": "type"
              },
              "oneOf": [
                {
                  "$ref": "#/$defs/StringToLowerCase"
                },
                {
                  "$ref": "#/$defs/StringToUpperCase"
                },
                {
                  "$ref": "#/$defs/LookupTable"
                },
                {
                  "$ref": "#/$defs/ToNumber"
                },
                {
                  "$ref": "#/$defs/NumberRound"
                },
                {
                  "$ref": "#/$defs/SequenceMap"
                },
                {
                  "$ref": "#/$defs/SequenceApply"
                },
                {
                  "$ref": "#/$defs/NumericSequenceAggregate"
                },
                {
                  "$ref": "#/$defs/ToString"
                },
                {
                  "$ref": "#/$defs/ToBoolean"
                },
                {
                  "$ref": "#/$defs/StringSubSequence"
                },
                {
                  "$ref": "#/$defs/DetectionsPropertyExtract"
                },
                {
                  "$ref": "#/$defs/SequenceAggregate"
                },
                {
                  "$ref": "#/$defs/ExtractDetectionProperty"
                },
                {
                  "$ref": "#/$defs/DetectionsFilter"
                },
                {
                  "$ref": "#/$defs/DetectionsOffset"
                },
                {
                  "$ref": "#/$defs/DetectionsShift"
                },
                {
                  "$ref": "#/$defs/RandomNumber"
                },
                {
                  "$ref": "#/$defs/StringMatches"
                },
                {
                  "$ref": "#/$defs/ExtractImageProperty"
                },
                {
                  "$ref": "#/$defs/SequenceLength"
                },
                {
                  "$ref": "#/$defs/Multiply"
                },
                {
                  "$ref": "#/$defs/Divide"
                },
                {
                  "$ref": "#/$defs/DetectionsSelection"
                },
                {
                  "$ref": "#/$defs/SortDetections"
                },
                {
                  "$ref": "#/$defs/ClassificationPropertyExtract"
                }
              ]
            },
            "title": "Operations",
            "type": "array"
          },
          "operations_parameters": {
            "additionalProperties": {
              "anyOf": [
                {
                  "kind": [
                    {
                      "description": "Image in workflows",
                      "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                      "name": "Batch[image]"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_image",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_parameter",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                  "reference": true,
                  "selected_element": "step_output",
                  "type": "string"
                }
              ]
            },
            "description": "References to additional parameters that may be provided in runtime to parameterize operations",
            "examples": ["$inputs.confidence", "$inputs.image"],
            "title": "Operations Parameters",
            "type": "object"
          }
        },
        "required": ["type", "name", "predictions", "operations"],
        "short_description": "Apply transformations on detected bounding boxes.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[keypoint_detection_prediction]",
              "description": "`'predictions'` key from Keypoint Detection Model output",
              "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.detections_transformation.v1.DetectionsTransformationBlockV1",
      "human_friendly_block_name": "Detections Transformation",
      "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
      "manifest_type_identifier_aliases": ["DetectionsTransformation"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "sink",
        "license": "Apache-2.0",
        "long_description": "\nBlock let users save their images and predictions into Roboflow Dataset. Persisting data from\nproduction environments helps iteratively building more robust models. \n\nBlock provides configuration options to decide how data should be stored and what are the limits \nto be applied. We advice using this block in combination with rate limiter blocks to effectively \ncollect data that the model struggle with.\n",
        "name": "Roboflow Dataset Upload",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_dataset_upload@v1",
              "RoboflowDatasetUpload"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "predictions": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "`'predictions'` key from Object Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[object_detection_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Instance Segmentation Model outputs",
                    "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[instance_segmentation_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Keypoint Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[keypoint_detection_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Classification Model outputs",
                    "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[classification_prediction]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Reference q detection-like predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "title": "Predictions"
          },
          "target_project": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "name of Roboflow dataset / project to be used as target for collected data",
            "examples": ["my_dataset", "$inputs.target_al_dataset"],
            "title": "Target Project"
          },
          "usage_quota_name": {
            "description": "Unique name for Roboflow project pointed by `target_project` parameter, that identifies usage quota applied for this block.",
            "examples": ["quota-for-data-sampling-1"],
            "title": "Usage Quota Name",
            "type": "string"
          },
          "persist_predictions": {
            "default": true,
            "description": "Boolean flag to decide if predictions should be registered along with images",
            "examples": [true, false],
            "title": "Persist Predictions",
            "type": "boolean"
          },
          "minutely_usage_limit": {
            "default": 10,
            "description": "Maximum number of data registration requests per minute accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
            "examples": [10, 60],
            "title": "Minutely Usage Limit",
            "type": "integer"
          },
          "hourly_usage_limit": {
            "default": 100,
            "description": "Maximum number of data registration requests per hour accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
            "examples": [10, 60],
            "title": "Hourly Usage Limit",
            "type": "integer"
          },
          "daily_usage_limit": {
            "default": 1000,
            "description": "Maximum number of data registration requests per day accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
            "examples": [10, 60],
            "title": "Daily Usage Limit",
            "type": "integer"
          },
          "max_image_size": {
            "default": [512, 512],
            "description": "Maximum size of the image to be registered - bigger images will be downsized preserving aspect ratio. Format of data: `(width, height)`",
            "examples": [
              [512, 512],
              [1920, 1080]
            ],
            "maxItems": 2,
            "minItems": 2,
            "prefixItems": [
              {
                "type": "integer"
              },
              {
                "type": "integer"
              }
            ],
            "title": "Max Image Size",
            "type": "array"
          },
          "compression_level": {
            "default": 75,
            "description": "Compression level for images registered",
            "examples": [75],
            "exclusiveMinimum": 0,
            "maximum": 100,
            "title": "Compression Level",
            "type": "integer"
          },
          "registration_tags": {
            "description": "Tags to be attached to registered datapoints",
            "examples": [
              ["location-florida", "factory-name", "$inputs.dynamic_tag"]
            ],
            "items": {
              "anyOf": [
                {
                  "kind": [
                    {
                      "description": "String value",
                      "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                      "name": "string"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_parameter",
                  "type": "string"
                },
                {
                  "type": "string"
                }
              ]
            },
            "title": "Registration Tags",
            "type": "array"
          },
          "disable_sink": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": false,
            "description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Sink"
          },
          "fire_and_forget": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
            "title": "Fire And Forget"
          },
          "labeling_batch_prefix": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "workflows_data_collector",
            "description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
            "examples": ["my_labeling_batch_name"],
            "title": "Labeling Batch Prefix"
          },
          "labeling_batches_recreation_frequency": {
            "default": "never",
            "description": "Frequency in which new labeling batches are created in Roboflow app. New batches are created with name prefix provided in `labeling_batch_prefix` in given time intervals.Useful in organising labeling flow.",
            "enum": ["never", "daily", "weekly", "monthly"],
            "examples": ["never", "daily"],
            "title": "Labeling Batches Recreation Frequency",
            "type": "string"
          }
        },
        "required": [
          "type",
          "name",
          "images",
          "target_project",
          "usage_quota_name"
        ],
        "short_description": "Save images and predictions in your Roboflow Dataset",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "error_status",
          "kind": [
            {
              "name": "Batch[boolean]",
              "description": "Boolean values batch",
              "docs": "\nThis kind represents batch of boolean values. \n\nExamples:\n```\n[True, False, False, True]\n[True, True]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "message",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.sinks.roboflow.dataset_upload.v1.RoboflowDatasetUploadBlockV1",
      "human_friendly_block_name": "Roboflow Dataset Upload",
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "manifest_type_identifier_aliases": ["RoboflowDatasetUpload"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "BinaryStatement": {
            "properties": {
              "type": {
                "const": "BinaryStatement",
                "enum": ["BinaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "left_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Left Operand"
              },
              "comparator": {
                "discriminator": {
                  "mapping": {
                    "!=": "#/$defs/NotEquals",
                    "(Detection) in zone": "#/$defs/DetectionInZone",
                    "(Number) !=": "#/$defs/NotEquals",
                    "(Number) <": "#/$defs/NumberLower",
                    "(Number) <=": "#/$defs/NumberLowerEqual",
                    "(Number) ==": "#/$defs/Equals",
                    "(Number) >": "#/$defs/NumberGreater",
                    "(Number) >=": "#/$defs/NumberGreaterEqual",
                    "(String) contains": "#/$defs/StringContains",
                    "(String) endsWith": "#/$defs/StringEndsWith",
                    "(String) startsWith": "#/$defs/StringStartsWith",
                    "==": "#/$defs/Equals",
                    "in (Sequence)": "#/$defs/In"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/In"
                  },
                  {
                    "$ref": "#/$defs/StringContains"
                  },
                  {
                    "$ref": "#/$defs/StringEndsWith"
                  },
                  {
                    "$ref": "#/$defs/StringStartsWith"
                  },
                  {
                    "$ref": "#/$defs/NumberLowerEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberLower"
                  },
                  {
                    "$ref": "#/$defs/NumberGreaterEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberGreater"
                  },
                  {
                    "$ref": "#/$defs/NotEquals"
                  },
                  {
                    "$ref": "#/$defs/Equals"
                  },
                  {
                    "$ref": "#/$defs/DetectionInZone"
                  }
                ],
                "title": "Comparator"
              },
              "right_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Right Operand"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "left_operand", "comparator", "right_operand"],
            "title": "BinaryStatement",
            "type": "object"
          },
          "ClassificationProperty": {
            "enum": [
              "top_class",
              "top_class_confidence",
              "all_classes",
              "all_confidences"
            ],
            "title": "ClassificationProperty",
            "type": "string"
          },
          "ClassificationPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "Batch[classification_prediction]",
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ClassificationPropertyExtract",
                "enum": ["ClassificationPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ClassificationProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ClassificationPropertyExtract",
            "type": "object"
          },
          "DetectionInZone": {
            "description": "Checks if detection is in zone",
            "operands_kinds": [
              [
                {
                  "name": "detection",
                  "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                  "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
                },
                {
                  "name": "zone",
                  "description": "Definition of polygon zone",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Detection) in zone",
                "enum": ["(Detection) in zone"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DetectionInZone",
            "type": "object"
          },
          "DetectionsFilter": {
            "compound": true,
            "description": "Filters out unwanted elements from detections-based prediction by applying filter operation in context of every single detection within prediction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsFilter",
                "enum": ["DetectionsFilter"],
                "title": "Type",
                "type": "string"
              },
              "filter_operation": {
                "$ref": "#/$defs/StatementGroup"
              }
            },
            "required": ["type", "filter_operation"],
            "title": "DetectionsFilter",
            "type": "object"
          },
          "DetectionsOffset": {
            "compound": false,
            "description": "Makes detected bounding boxes bigger by applying offset to its size",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsOffset",
                "enum": ["DetectionsOffset"],
                "title": "Type",
                "type": "string"
              },
              "offset_x": {
                "title": "Offset X",
                "type": "integer"
              },
              "offset_y": {
                "title": "Offset Y",
                "type": "integer"
              }
            },
            "required": ["type", "offset_x", "offset_y"],
            "title": "DetectionsOffset",
            "type": "object"
          },
          "DetectionsProperty": {
            "enum": [
              "confidence",
              "class_name",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "class_id",
              "center",
              "top_left",
              "top_right",
              "bottom_left",
              "bottom_right"
            ],
            "title": "DetectionsProperty",
            "type": "string"
          },
          "DetectionsPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsPropertyExtract",
                "enum": ["DetectionsPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "DetectionsPropertyExtract",
            "type": "object"
          },
          "DetectionsSelection": {
            "compound": false,
            "description": "Selects bounding boxes based on predefined criterias",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsSelection",
                "enum": ["DetectionsSelection"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSelectionMode"
              }
            },
            "required": ["type", "mode"],
            "title": "DetectionsSelection",
            "type": "object"
          },
          "DetectionsSelectionMode": {
            "enum": ["left_most", "right_most"],
            "title": "DetectionsSelectionMode",
            "type": "string"
          },
          "DetectionsShift": {
            "compound": false,
            "description": "Shifting detected bounding boxes in assigned direction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsShift",
                "enum": ["DetectionsShift"],
                "title": "Type",
                "type": "string"
              },
              "shift_x": {
                "title": "Shift X",
                "type": "integer"
              },
              "shift_y": {
                "title": "Shift Y",
                "type": "integer"
              }
            },
            "required": ["type", "shift_x", "shift_y"],
            "title": "DetectionsShift",
            "type": "object"
          },
          "DetectionsSortProperties": {
            "enum": [
              "confidence",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "center_x",
              "center_y"
            ],
            "title": "DetectionsSortProperties",
            "type": "string"
          },
          "Divide": {
            "compound": false,
            "description": "Dividing value against other",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Divide",
                "enum": ["Divide"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Divide",
            "type": "object"
          },
          "DoesNotExist": {
            "description": "Checks if value is not given (`None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DoesNotExist",
                "enum": ["DoesNotExist"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DoesNotExist",
            "type": "object"
          },
          "DynamicOperand": {
            "properties": {
              "type": {
                "const": "DynamicOperand",
                "enum": ["DynamicOperand"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              },
              "operand_name": {
                "default": "_",
                "title": "Operand Name",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DynamicOperand",
            "type": "object"
          },
          "Equals": {
            "description": "Checks if two values given are equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) ==", "=="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Equals",
            "type": "object"
          },
          "Exists": {
            "description": "Checks if value is given (not `None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Exists",
                "enum": ["Exists"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Exists",
            "type": "object"
          },
          "ExtractDetectionProperty": {
            "compound": false,
            "description": "Extracts property from single detection",
            "input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractDetectionProperty",
                "enum": ["ExtractDetectionProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractDetectionProperty",
            "type": "object"
          },
          "ExtractImageProperty": {
            "description": "Extracts specific property of image (like size)",
            "input_kind": [
              {
                "name": "image",
                "description": "Image in workflows",
                "docs": "TODO"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractImageProperty",
                "enum": ["ExtractImageProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ImageProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractImageProperty",
            "type": "object"
          },
          "ImageProperty": {
            "enum": ["size", "height", "width"],
            "title": "ImageProperty",
            "type": "string"
          },
          "In": {
            "description": "Checks if first value is element of second value (usually list or dictionary)",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "in (Sequence)",
                "enum": ["in (Sequence)"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "In",
            "type": "object"
          },
          "IsEmpty": {
            "description": "Checks if sequence is empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is empty",
                "enum": ["(Sequence) is empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsEmpty",
            "type": "object"
          },
          "IsFalse": {
            "description": "Checks if value is `False`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is False",
                "enum": ["(Boolean) is False"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsFalse",
            "type": "object"
          },
          "IsNotEmpty": {
            "description": "Checks if sequence is not empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is not empty",
                "enum": ["(Sequence) is not empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsNotEmpty",
            "type": "object"
          },
          "IsTrue": {
            "description": "Checks if value is `True`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is True",
                "enum": ["(Boolean) is True"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsTrue",
            "type": "object"
          },
          "LookupTable": {
            "compound": false,
            "description": "Changes value according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "LookupTable",
                "enum": ["LookupTable"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "LookupTable",
            "type": "object"
          },
          "Multiply": {
            "compound": false,
            "description": "Multiplication",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Multiply",
                "enum": ["Multiply"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Multiply",
            "type": "object"
          },
          "NotEquals": {
            "description": "Checks if two values given are not equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) !=", "!="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NotEquals",
            "type": "object"
          },
          "NumberCastingMode": {
            "enum": ["int", "float"],
            "title": "NumberCastingMode",
            "type": "string"
          },
          "NumberGreater": {
            "description": "Checks if first value (number) is greater than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >",
                "enum": ["(Number) >"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreater",
            "type": "object"
          },
          "NumberGreaterEqual": {
            "description": "Checks if first value (number) is greater or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >=",
                "enum": ["(Number) >="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreaterEqual",
            "type": "object"
          },
          "NumberLower": {
            "description": "Checks if first value (number) is lower than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <",
                "enum": ["(Number) <"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLower",
            "type": "object"
          },
          "NumberLowerEqual": {
            "description": "Checks if first value (number) is lower or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <=",
                "enum": ["(Number) <="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLowerEqual",
            "type": "object"
          },
          "NumberRound": {
            "compound": false,
            "description": "Rounds the number",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumberRound",
                "enum": ["NumberRound"],
                "title": "Type",
                "type": "string"
              },
              "decimal_digits": {
                "title": "Decimal Digits",
                "type": "integer"
              }
            },
            "required": ["type", "decimal_digits"],
            "title": "NumberRound",
            "type": "object"
          },
          "NumericSequenceAggregate": {
            "compound": false,
            "description": "Aggregates numeric sequence using aggregation function like min or max - adjusted to work on numbers",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumericSequenceAggregate",
                "enum": ["NumericSequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "function": {
                "$ref": "#/$defs/SequenceAggregationFunction"
              },
              "neutral_value": {
                "default": null,
                "title": "Neutral Value"
              }
            },
            "required": ["type", "function"],
            "title": "NumericSequenceAggregate",
            "type": "object"
          },
          "RandomNumber": {
            "description": "Special operation to let random sampling - ignoring input data and changing it into random floating point value. To be used mainly to sample predictions or images.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "RandomNumber",
                "enum": ["RandomNumber"],
                "title": "Type",
                "type": "string"
              },
              "min_value": {
                "default": 0,
                "title": "Min Value",
                "type": "number"
              },
              "max_value": {
                "default": 1,
                "title": "Max Value",
                "type": "number"
              }
            },
            "required": ["type"],
            "title": "RandomNumber",
            "type": "object"
          },
          "SequenceAggregate": {
            "compound": false,
            "description": "Aggregates sequence using generic aggregation methods - adjusted to majority data types",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceAggregate",
                "enum": ["SequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/SequenceAggregationMode"
              }
            },
            "required": ["type", "mode"],
            "title": "SequenceAggregate",
            "type": "object"
          },
          "SequenceAggregationFunction": {
            "enum": ["min", "max"],
            "title": "SequenceAggregationFunction",
            "type": "string"
          },
          "SequenceAggregationMode": {
            "enum": ["first", "last", "most_common", "least_common"],
            "title": "SequenceAggregationMode",
            "type": "string"
          },
          "SequenceApply": {
            "compound": true,
            "description": "Operation applies chain of operations at every element of sequence",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceApply",
                "enum": ["SequenceApply"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "operations"],
            "title": "SequenceApply",
            "type": "object"
          },
          "SequenceLength": {
            "compound": false,
            "description": "Operation determines the length of input sequence",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "dictionary",
                "description": "Dictionary",
                "docs": null
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceLength",
                "enum": ["SequenceLength"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "SequenceLength",
            "type": "object"
          },
          "SequenceMap": {
            "compound": true,
            "description": "Changes each value of sequence according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceMap",
                "enum": ["SequenceMap"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "SequenceMap",
            "type": "object"
          },
          "SortDetections": {
            "compound": false,
            "description": "Changes the order of detected bounding boxes.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SortDetections",
                "enum": ["SortDetections"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSortProperties"
              },
              "ascending": {
                "default": true,
                "title": "Ascending",
                "type": "boolean"
              }
            },
            "required": ["type", "mode"],
            "title": "SortDetections",
            "type": "object"
          },
          "StatementGroup": {
            "properties": {
              "type": {
                "const": "StatementGroup",
                "enum": ["StatementGroup"],
                "title": "Type",
                "type": "string"
              },
              "statements": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "BinaryStatement": "#/$defs/BinaryStatement",
                      "StatementGroup": "#/$defs/StatementGroup",
                      "UnaryStatement": "#/$defs/UnaryStatement"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/BinaryStatement"
                    },
                    {
                      "$ref": "#/$defs/UnaryStatement"
                    },
                    {
                      "$ref": "#/$defs/StatementGroup"
                    }
                  ]
                },
                "minItems": 1,
                "title": "Statements",
                "type": "array"
              },
              "operator": {
                "allOf": [
                  {
                    "$ref": "#/$defs/StatementsGroupsOperator"
                  }
                ],
                "default": "or"
              }
            },
            "required": ["type", "statements"],
            "title": "StatementGroup",
            "type": "object"
          },
          "StatementsGroupsOperator": {
            "enum": ["and", "or"],
            "title": "StatementsGroupsOperator",
            "type": "string"
          },
          "StaticOperand": {
            "properties": {
              "type": {
                "const": "StaticOperand",
                "enum": ["StaticOperand"],
                "title": "Type",
                "type": "string"
              },
              "value": {
                "title": "Value"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "value"],
            "title": "StaticOperand",
            "type": "object"
          },
          "StringContains": {
            "description": "Checks if string given as first value contains string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) contains",
                "enum": ["(String) contains"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringContains",
            "type": "object"
          },
          "StringEndsWith": {
            "description": "Checks if string given as first value ends with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) endsWith",
                "enum": ["(String) endsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringEndsWith",
            "type": "object"
          },
          "StringMatches": {
            "compound": false,
            "description": "Checks if string matches regex",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringMatches",
                "enum": ["StringMatches"],
                "title": "Type",
                "type": "string"
              },
              "regex": {
                "title": "Regex",
                "type": "string"
              }
            },
            "required": ["type", "regex"],
            "title": "StringMatches",
            "type": "object"
          },
          "StringStartsWith": {
            "description": "Checks if string given as first value starts with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) startsWith",
                "enum": ["(String) startsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringStartsWith",
            "type": "object"
          },
          "StringSubSequence": {
            "compound": false,
            "description": "Takes sub-string of the input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringSubSequence",
                "enum": ["StringSubSequence"],
                "title": "Type",
                "type": "string"
              },
              "start": {
                "default": 0,
                "title": "Start",
                "type": "integer"
              },
              "end": {
                "default": -1,
                "title": "End",
                "type": "integer"
              }
            },
            "required": ["type"],
            "title": "StringSubSequence",
            "type": "object"
          },
          "StringToLowerCase": {
            "compound": false,
            "description": "Executes lowercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToLowerCase",
                "enum": ["StringToLowerCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToLowerCase",
            "type": "object"
          },
          "StringToUpperCase": {
            "compound": false,
            "description": "Executes uppercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToUpperCase",
                "enum": ["StringToUpperCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToUpperCase",
            "type": "object"
          },
          "ToBoolean": {
            "compound": false,
            "description": "Changes input data into boolean",
            "input_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToBoolean",
                "enum": ["ToBoolean"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToBoolean",
            "type": "object"
          },
          "ToNumber": {
            "compound": false,
            "description": "Changes value into number - float or int depending on configuration",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToNumber",
                "enum": ["ToNumber"],
                "title": "Type",
                "type": "string"
              },
              "cast_to": {
                "$ref": "#/$defs/NumberCastingMode"
              }
            },
            "required": ["type", "cast_to"],
            "title": "ToNumber",
            "type": "object"
          },
          "ToString": {
            "compound": false,
            "description": "Stringifies data",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToString",
                "enum": ["ToString"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToString",
            "type": "object"
          },
          "UnaryStatement": {
            "properties": {
              "type": {
                "const": "UnaryStatement",
                "enum": ["UnaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Operand"
              },
              "operator": {
                "discriminator": {
                  "mapping": {
                    "(Boolean) is False": "#/$defs/IsFalse",
                    "(Boolean) is True": "#/$defs/IsTrue",
                    "(Sequence) is empty": "#/$defs/IsEmpty",
                    "(Sequence) is not empty": "#/$defs/IsNotEmpty",
                    "DoesNotExist": "#/$defs/DoesNotExist",
                    "Exists": "#/$defs/Exists"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/Exists"
                  },
                  {
                    "$ref": "#/$defs/DoesNotExist"
                  },
                  {
                    "$ref": "#/$defs/IsTrue"
                  },
                  {
                    "$ref": "#/$defs/IsFalse"
                  },
                  {
                    "$ref": "#/$defs/IsEmpty"
                  },
                  {
                    "$ref": "#/$defs/IsNotEmpty"
                  }
                ],
                "title": "Operator"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "operand", "operator"],
            "title": "UnaryStatement",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "flow_control",
        "license": "Apache-2.0",
        "long_description": "\nBased on provided configuration, block decides if it should follow to pointed\nexecution path\n",
        "name": "Continue If",
        "properties": {
          "type": {
            "enum": ["roboflow_core/continue_if@v1", "ContinueIf"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "condition_statement": {
            "allOf": [
              {
                "$ref": "#/$defs/StatementGroup"
              }
            ],
            "description": "Workflows UQL definition of conditional logic."
          },
          "evaluation_parameters": {
            "additionalProperties": {
              "anyOf": [
                {
                  "kind": [
                    {
                      "description": "Image in workflows",
                      "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                      "name": "Batch[image]"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_image",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_parameter",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                  "reference": true,
                  "selected_element": "step_output",
                  "type": "string"
                }
              ]
            },
            "description": "References to additional parameters that may be provided in runtime to parametrise operations",
            "examples": [
              "$inputs.confidence",
              "$inputs.image",
              "$steps.my_step.top"
            ],
            "title": "Evaluation Parameters",
            "type": "object"
          },
          "next_steps": {
            "description": "Reference to step which shall be executed if expression evaluates to true",
            "examples": [["$steps.on_true"]],
            "items": {
              "pattern": "^\\$steps\\.[A-Za-z_0-9\\-]+",
              "reference": true,
              "selected_element": "step",
              "type": "string"
            },
            "title": "Next Steps",
            "type": "array"
          }
        },
        "required": ["type", "name", "condition_statement", "next_steps"],
        "short_description": "Conditionally stop execution of a branch.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.flow_control.continue_if.v1.ContinueIfBlockV1",
      "human_friendly_block_name": "Continue If",
      "manifest_type_identifier": "roboflow_core/continue_if@v1",
      "manifest_type_identifier_aliases": ["ContinueIf"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nThe `PerspectiveCorrectionBlock` is a transformer block designed to correct\ncoordinates of detections based on transformation defined by two polygons.\nThis block is best suited when produced coordinates should be considered as if camera\nwas placed directly above the scene and was not introducing distortions.\n",
        "name": "Perspective Correction",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/perspective_correction@v1",
              "PerspectiveCorrection"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "`'predictions'` key from Object Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[object_detection_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Instance Segmentation Model outputs",
                    "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[instance_segmentation_prediction]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "title": "Predictions"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image to Crop"
          },
          "perspective_polygons": {
            "anyOf": [
              {
                "items": {},
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Perspective polygons (for each batch at least one must be consisting of 4 vertices)",
            "title": "Perspective Polygons"
          },
          "transformed_rect_width": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 1000,
            "description": "Transformed rect width",
            "title": "Transformed Rect Width"
          },
          "transformed_rect_height": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 1000,
            "description": "Transformed rect height",
            "title": "Transformed Rect Height"
          },
          "extend_perspective_polygon_by_detections_anchor": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "",
            "description": "If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS",
            "title": "Extend Perspective Polygon By Detections Anchor"
          },
          "warp_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": false,
            "description": "If set to True, image will be warped into transformed rect",
            "title": "Warp Image"
          }
        },
        "required": ["type", "name", "images", "perspective_polygons"],
        "short_description": "Correct coordinates of detections from plane defined by given polygon to straight rectangular plane of given width and height",
        "title": "PerspectiveCorrectionManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "corrected_coordinates",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "warped_image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.perspective_correction.v1.PerspectiveCorrectionBlockV1",
      "human_friendly_block_name": "Perspective Correction",
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "manifest_type_identifier_aliases": ["PerspectiveCorrection"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nThe `DynamicZoneBlock` is a transformer block designed to simplify polygon\nso it's geometrically convex and then reduce number of vertices to requested amount.\nThis block is best suited when Zone needs to be created based on shape of detected object\n(i.e. basketball field, road segment, zebra crossing etc.)\nInput detections should be filtered and contain only desired classes of interest.\n",
        "name": "Dynamic Zone",
        "properties": {
          "type": {
            "enum": ["roboflow_core/dynamic_zone@v1", "DynamicZone"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "",
            "examples": ["$segmentation.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "required_number_of_vertices": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Keep simplifying polygon until number of vertices matches this number",
            "title": "Required Number Of Vertices"
          }
        },
        "required": [
          "type",
          "name",
          "predictions",
          "required_number_of_vertices"
        ],
        "short_description": "Simplify polygons so they are geometrically convex and simplify them to contain only requested amount of vertices",
        "title": "DynamicZonesManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "zones",
          "kind": [
            {
              "name": "list_of_values",
              "description": "List of values of any types",
              "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.dynamic_zones.v1.DynamicZonesBlockV1",
      "human_friendly_block_name": "Dynamic Zone",
      "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
      "manifest_type_identifier_aliases": ["DynamicZone"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "fusion",
        "license": "Apache-2.0",
        "long_description": "\nCombine results of detection model with classification results performed separately for \neach and every bounding box. \n\nBounding boxes without top class predicted by classification model are discarded, \nfor multi-label classification results, most confident label is taken as bounding box\nclass.  \n",
        "name": "Detections Classes Replacement",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/detections_classes_replacement@v1",
              "DetectionsClassesReplacement"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "object_detection_predictions": {
            "description": "The output of a detection model describing the bounding boxes that will have classes replaced.",
            "examples": ["$steps.my_object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Regions of Interest",
            "type": "string"
          },
          "classification_predictions": {
            "description": "The output of classification model for crops taken based on RoIs pointed as the other parameter",
            "examples": ["$steps.my_classification_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[classification_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Classification results for crops",
            "type": "string"
          }
        },
        "required": [
          "type",
          "name",
          "object_detection_predictions",
          "classification_predictions"
        ],
        "short_description": "Replaces classes of detections with classes predicted by a chained classification model",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[keypoint_detection_prediction]",
              "description": "`'predictions'` key from Keypoint Detection Model output",
              "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.fusion.detections_classes_replacement.v1.DetectionsClassesReplacementBlockV1",
      "human_friendly_block_name": "Detections Classes Replacement",
      "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
      "manifest_type_identifier_aliases": ["DetectionsClassesReplacement"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {
        "object_detection_predictions": 0,
        "classification_predictions": 1
      },
      "dimensionality_reference_property": "object_detection_predictions",
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "BinaryStatement": {
            "properties": {
              "type": {
                "const": "BinaryStatement",
                "enum": ["BinaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "left_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Left Operand"
              },
              "comparator": {
                "discriminator": {
                  "mapping": {
                    "!=": "#/$defs/NotEquals",
                    "(Detection) in zone": "#/$defs/DetectionInZone",
                    "(Number) !=": "#/$defs/NotEquals",
                    "(Number) <": "#/$defs/NumberLower",
                    "(Number) <=": "#/$defs/NumberLowerEqual",
                    "(Number) ==": "#/$defs/Equals",
                    "(Number) >": "#/$defs/NumberGreater",
                    "(Number) >=": "#/$defs/NumberGreaterEqual",
                    "(String) contains": "#/$defs/StringContains",
                    "(String) endsWith": "#/$defs/StringEndsWith",
                    "(String) startsWith": "#/$defs/StringStartsWith",
                    "==": "#/$defs/Equals",
                    "in (Sequence)": "#/$defs/In"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/In"
                  },
                  {
                    "$ref": "#/$defs/StringContains"
                  },
                  {
                    "$ref": "#/$defs/StringEndsWith"
                  },
                  {
                    "$ref": "#/$defs/StringStartsWith"
                  },
                  {
                    "$ref": "#/$defs/NumberLowerEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberLower"
                  },
                  {
                    "$ref": "#/$defs/NumberGreaterEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberGreater"
                  },
                  {
                    "$ref": "#/$defs/NotEquals"
                  },
                  {
                    "$ref": "#/$defs/Equals"
                  },
                  {
                    "$ref": "#/$defs/DetectionInZone"
                  }
                ],
                "title": "Comparator"
              },
              "right_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Right Operand"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "left_operand", "comparator", "right_operand"],
            "title": "BinaryStatement",
            "type": "object"
          },
          "CaseDefinition": {
            "properties": {
              "type": {
                "const": "CaseDefinition",
                "enum": ["CaseDefinition"],
                "title": "Type",
                "type": "string"
              },
              "condition": {
                "$ref": "#/$defs/StatementGroup"
              },
              "result": {
                "discriminator": {
                  "mapping": {
                    "DynamicCaseResult": "#/$defs/DynamicCaseResult",
                    "StaticCaseResult": "#/$defs/StaticCaseResult"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticCaseResult"
                  },
                  {
                    "$ref": "#/$defs/DynamicCaseResult"
                  }
                ],
                "title": "Result"
              }
            },
            "required": ["type", "condition", "result"],
            "title": "CaseDefinition",
            "type": "object"
          },
          "CasesDefinition": {
            "properties": {
              "type": {
                "const": "CasesDefinition",
                "enum": ["CasesDefinition"],
                "title": "Type",
                "type": "string"
              },
              "cases": {
                "items": {
                  "$ref": "#/$defs/CaseDefinition"
                },
                "title": "Cases",
                "type": "array"
              },
              "default": {
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticCaseResult"
                  },
                  {
                    "$ref": "#/$defs/DynamicCaseResult"
                  }
                ],
                "discriminator": {
                  "mapping": {
                    "DynamicCaseResult": "#/$defs/DynamicCaseResult",
                    "StaticCaseResult": "#/$defs/StaticCaseResult"
                  },
                  "propertyName": "type"
                },
                "title": "Default"
              }
            },
            "required": ["type", "cases", "default"],
            "title": "CasesDefinition",
            "type": "object"
          },
          "ClassificationProperty": {
            "enum": [
              "top_class",
              "top_class_confidence",
              "all_classes",
              "all_confidences"
            ],
            "title": "ClassificationProperty",
            "type": "string"
          },
          "ClassificationPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "Batch[classification_prediction]",
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ClassificationPropertyExtract",
                "enum": ["ClassificationPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ClassificationProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ClassificationPropertyExtract",
            "type": "object"
          },
          "DetectionInZone": {
            "description": "Checks if detection is in zone",
            "operands_kinds": [
              [
                {
                  "name": "detection",
                  "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                  "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
                },
                {
                  "name": "zone",
                  "description": "Definition of polygon zone",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Detection) in zone",
                "enum": ["(Detection) in zone"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DetectionInZone",
            "type": "object"
          },
          "DetectionsFilter": {
            "compound": true,
            "description": "Filters out unwanted elements from detections-based prediction by applying filter operation in context of every single detection within prediction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsFilter",
                "enum": ["DetectionsFilter"],
                "title": "Type",
                "type": "string"
              },
              "filter_operation": {
                "$ref": "#/$defs/StatementGroup"
              }
            },
            "required": ["type", "filter_operation"],
            "title": "DetectionsFilter",
            "type": "object"
          },
          "DetectionsOffset": {
            "compound": false,
            "description": "Makes detected bounding boxes bigger by applying offset to its size",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsOffset",
                "enum": ["DetectionsOffset"],
                "title": "Type",
                "type": "string"
              },
              "offset_x": {
                "title": "Offset X",
                "type": "integer"
              },
              "offset_y": {
                "title": "Offset Y",
                "type": "integer"
              }
            },
            "required": ["type", "offset_x", "offset_y"],
            "title": "DetectionsOffset",
            "type": "object"
          },
          "DetectionsProperty": {
            "enum": [
              "confidence",
              "class_name",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "class_id",
              "center",
              "top_left",
              "top_right",
              "bottom_left",
              "bottom_right"
            ],
            "title": "DetectionsProperty",
            "type": "string"
          },
          "DetectionsPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsPropertyExtract",
                "enum": ["DetectionsPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "DetectionsPropertyExtract",
            "type": "object"
          },
          "DetectionsSelection": {
            "compound": false,
            "description": "Selects bounding boxes based on predefined criterias",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsSelection",
                "enum": ["DetectionsSelection"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSelectionMode"
              }
            },
            "required": ["type", "mode"],
            "title": "DetectionsSelection",
            "type": "object"
          },
          "DetectionsSelectionMode": {
            "enum": ["left_most", "right_most"],
            "title": "DetectionsSelectionMode",
            "type": "string"
          },
          "DetectionsShift": {
            "compound": false,
            "description": "Shifting detected bounding boxes in assigned direction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsShift",
                "enum": ["DetectionsShift"],
                "title": "Type",
                "type": "string"
              },
              "shift_x": {
                "title": "Shift X",
                "type": "integer"
              },
              "shift_y": {
                "title": "Shift Y",
                "type": "integer"
              }
            },
            "required": ["type", "shift_x", "shift_y"],
            "title": "DetectionsShift",
            "type": "object"
          },
          "DetectionsSortProperties": {
            "enum": [
              "confidence",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "center_x",
              "center_y"
            ],
            "title": "DetectionsSortProperties",
            "type": "string"
          },
          "Divide": {
            "compound": false,
            "description": "Dividing value against other",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Divide",
                "enum": ["Divide"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Divide",
            "type": "object"
          },
          "DoesNotExist": {
            "description": "Checks if value is not given (`None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DoesNotExist",
                "enum": ["DoesNotExist"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DoesNotExist",
            "type": "object"
          },
          "DynamicCaseResult": {
            "properties": {
              "type": {
                "const": "DynamicCaseResult",
                "enum": ["DynamicCaseResult"],
                "title": "Type",
                "type": "string"
              },
              "parameter_name": {
                "title": "Parameter Name",
                "type": "string"
              },
              "operations": {
                "anyOf": [
                  {
                    "items": {
                      "discriminator": {
                        "mapping": {
                          "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                          "DetectionsFilter": "#/$defs/DetectionsFilter",
                          "DetectionsOffset": "#/$defs/DetectionsOffset",
                          "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                          "DetectionsSelection": "#/$defs/DetectionsSelection",
                          "DetectionsShift": "#/$defs/DetectionsShift",
                          "Divide": "#/$defs/Divide",
                          "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                          "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                          "LookupTable": "#/$defs/LookupTable",
                          "Multiply": "#/$defs/Multiply",
                          "NumberRound": "#/$defs/NumberRound",
                          "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                          "RandomNumber": "#/$defs/RandomNumber",
                          "SequenceAggregate": "#/$defs/SequenceAggregate",
                          "SequenceApply": "#/$defs/SequenceApply",
                          "SequenceLength": "#/$defs/SequenceLength",
                          "SequenceMap": "#/$defs/SequenceMap",
                          "SortDetections": "#/$defs/SortDetections",
                          "StringMatches": "#/$defs/StringMatches",
                          "StringSubSequence": "#/$defs/StringSubSequence",
                          "StringToLowerCase": "#/$defs/StringToLowerCase",
                          "StringToUpperCase": "#/$defs/StringToUpperCase",
                          "ToBoolean": "#/$defs/ToBoolean",
                          "ToNumber": "#/$defs/ToNumber",
                          "ToString": "#/$defs/ToString"
                        },
                        "propertyName": "type"
                      },
                      "oneOf": [
                        {
                          "$ref": "#/$defs/StringToLowerCase"
                        },
                        {
                          "$ref": "#/$defs/StringToUpperCase"
                        },
                        {
                          "$ref": "#/$defs/LookupTable"
                        },
                        {
                          "$ref": "#/$defs/ToNumber"
                        },
                        {
                          "$ref": "#/$defs/NumberRound"
                        },
                        {
                          "$ref": "#/$defs/SequenceMap"
                        },
                        {
                          "$ref": "#/$defs/SequenceApply"
                        },
                        {
                          "$ref": "#/$defs/NumericSequenceAggregate"
                        },
                        {
                          "$ref": "#/$defs/ToString"
                        },
                        {
                          "$ref": "#/$defs/ToBoolean"
                        },
                        {
                          "$ref": "#/$defs/StringSubSequence"
                        },
                        {
                          "$ref": "#/$defs/DetectionsPropertyExtract"
                        },
                        {
                          "$ref": "#/$defs/SequenceAggregate"
                        },
                        {
                          "$ref": "#/$defs/ExtractDetectionProperty"
                        },
                        {
                          "$ref": "#/$defs/DetectionsFilter"
                        },
                        {
                          "$ref": "#/$defs/DetectionsOffset"
                        },
                        {
                          "$ref": "#/$defs/DetectionsShift"
                        },
                        {
                          "$ref": "#/$defs/RandomNumber"
                        },
                        {
                          "$ref": "#/$defs/StringMatches"
                        },
                        {
                          "$ref": "#/$defs/ExtractImageProperty"
                        },
                        {
                          "$ref": "#/$defs/SequenceLength"
                        },
                        {
                          "$ref": "#/$defs/Multiply"
                        },
                        {
                          "$ref": "#/$defs/Divide"
                        },
                        {
                          "$ref": "#/$defs/DetectionsSelection"
                        },
                        {
                          "$ref": "#/$defs/SortDetections"
                        },
                        {
                          "$ref": "#/$defs/ClassificationPropertyExtract"
                        }
                      ]
                    },
                    "type": "array"
                  },
                  {
                    "type": "null"
                  }
                ],
                "default": null,
                "title": "Operations"
              }
            },
            "required": ["type", "parameter_name"],
            "title": "DynamicCaseResult",
            "type": "object"
          },
          "DynamicOperand": {
            "properties": {
              "type": {
                "const": "DynamicOperand",
                "enum": ["DynamicOperand"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              },
              "operand_name": {
                "default": "_",
                "title": "Operand Name",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DynamicOperand",
            "type": "object"
          },
          "Equals": {
            "description": "Checks if two values given are equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) ==", "=="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Equals",
            "type": "object"
          },
          "Exists": {
            "description": "Checks if value is given (not `None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Exists",
                "enum": ["Exists"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Exists",
            "type": "object"
          },
          "ExtractDetectionProperty": {
            "compound": false,
            "description": "Extracts property from single detection",
            "input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractDetectionProperty",
                "enum": ["ExtractDetectionProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractDetectionProperty",
            "type": "object"
          },
          "ExtractImageProperty": {
            "description": "Extracts specific property of image (like size)",
            "input_kind": [
              {
                "name": "image",
                "description": "Image in workflows",
                "docs": "TODO"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractImageProperty",
                "enum": ["ExtractImageProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ImageProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractImageProperty",
            "type": "object"
          },
          "ImageProperty": {
            "enum": ["size", "height", "width"],
            "title": "ImageProperty",
            "type": "string"
          },
          "In": {
            "description": "Checks if first value is element of second value (usually list or dictionary)",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "in (Sequence)",
                "enum": ["in (Sequence)"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "In",
            "type": "object"
          },
          "IsEmpty": {
            "description": "Checks if sequence is empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is empty",
                "enum": ["(Sequence) is empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsEmpty",
            "type": "object"
          },
          "IsFalse": {
            "description": "Checks if value is `False`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is False",
                "enum": ["(Boolean) is False"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsFalse",
            "type": "object"
          },
          "IsNotEmpty": {
            "description": "Checks if sequence is not empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is not empty",
                "enum": ["(Sequence) is not empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsNotEmpty",
            "type": "object"
          },
          "IsTrue": {
            "description": "Checks if value is `True`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is True",
                "enum": ["(Boolean) is True"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsTrue",
            "type": "object"
          },
          "LookupTable": {
            "compound": false,
            "description": "Changes value according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "LookupTable",
                "enum": ["LookupTable"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "LookupTable",
            "type": "object"
          },
          "Multiply": {
            "compound": false,
            "description": "Multiplication",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Multiply",
                "enum": ["Multiply"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Multiply",
            "type": "object"
          },
          "NotEquals": {
            "description": "Checks if two values given are not equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) !=", "!="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NotEquals",
            "type": "object"
          },
          "NumberCastingMode": {
            "enum": ["int", "float"],
            "title": "NumberCastingMode",
            "type": "string"
          },
          "NumberGreater": {
            "description": "Checks if first value (number) is greater than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >",
                "enum": ["(Number) >"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreater",
            "type": "object"
          },
          "NumberGreaterEqual": {
            "description": "Checks if first value (number) is greater or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >=",
                "enum": ["(Number) >="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreaterEqual",
            "type": "object"
          },
          "NumberLower": {
            "description": "Checks if first value (number) is lower than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <",
                "enum": ["(Number) <"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLower",
            "type": "object"
          },
          "NumberLowerEqual": {
            "description": "Checks if first value (number) is lower or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <=",
                "enum": ["(Number) <="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLowerEqual",
            "type": "object"
          },
          "NumberRound": {
            "compound": false,
            "description": "Rounds the number",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumberRound",
                "enum": ["NumberRound"],
                "title": "Type",
                "type": "string"
              },
              "decimal_digits": {
                "title": "Decimal Digits",
                "type": "integer"
              }
            },
            "required": ["type", "decimal_digits"],
            "title": "NumberRound",
            "type": "object"
          },
          "NumericSequenceAggregate": {
            "compound": false,
            "description": "Aggregates numeric sequence using aggregation function like min or max - adjusted to work on numbers",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumericSequenceAggregate",
                "enum": ["NumericSequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "function": {
                "$ref": "#/$defs/SequenceAggregationFunction"
              },
              "neutral_value": {
                "default": null,
                "title": "Neutral Value"
              }
            },
            "required": ["type", "function"],
            "title": "NumericSequenceAggregate",
            "type": "object"
          },
          "RandomNumber": {
            "description": "Special operation to let random sampling - ignoring input data and changing it into random floating point value. To be used mainly to sample predictions or images.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "RandomNumber",
                "enum": ["RandomNumber"],
                "title": "Type",
                "type": "string"
              },
              "min_value": {
                "default": 0,
                "title": "Min Value",
                "type": "number"
              },
              "max_value": {
                "default": 1,
                "title": "Max Value",
                "type": "number"
              }
            },
            "required": ["type"],
            "title": "RandomNumber",
            "type": "object"
          },
          "SequenceAggregate": {
            "compound": false,
            "description": "Aggregates sequence using generic aggregation methods - adjusted to majority data types",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceAggregate",
                "enum": ["SequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/SequenceAggregationMode"
              }
            },
            "required": ["type", "mode"],
            "title": "SequenceAggregate",
            "type": "object"
          },
          "SequenceAggregationFunction": {
            "enum": ["min", "max"],
            "title": "SequenceAggregationFunction",
            "type": "string"
          },
          "SequenceAggregationMode": {
            "enum": ["first", "last", "most_common", "least_common"],
            "title": "SequenceAggregationMode",
            "type": "string"
          },
          "SequenceApply": {
            "compound": true,
            "description": "Operation applies chain of operations at every element of sequence",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceApply",
                "enum": ["SequenceApply"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "operations"],
            "title": "SequenceApply",
            "type": "object"
          },
          "SequenceLength": {
            "compound": false,
            "description": "Operation determines the length of input sequence",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "dictionary",
                "description": "Dictionary",
                "docs": null
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceLength",
                "enum": ["SequenceLength"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "SequenceLength",
            "type": "object"
          },
          "SequenceMap": {
            "compound": true,
            "description": "Changes each value of sequence according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceMap",
                "enum": ["SequenceMap"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "SequenceMap",
            "type": "object"
          },
          "SortDetections": {
            "compound": false,
            "description": "Changes the order of detected bounding boxes.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SortDetections",
                "enum": ["SortDetections"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSortProperties"
              },
              "ascending": {
                "default": true,
                "title": "Ascending",
                "type": "boolean"
              }
            },
            "required": ["type", "mode"],
            "title": "SortDetections",
            "type": "object"
          },
          "StatementGroup": {
            "properties": {
              "type": {
                "const": "StatementGroup",
                "enum": ["StatementGroup"],
                "title": "Type",
                "type": "string"
              },
              "statements": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "BinaryStatement": "#/$defs/BinaryStatement",
                      "StatementGroup": "#/$defs/StatementGroup",
                      "UnaryStatement": "#/$defs/UnaryStatement"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/BinaryStatement"
                    },
                    {
                      "$ref": "#/$defs/UnaryStatement"
                    },
                    {
                      "$ref": "#/$defs/StatementGroup"
                    }
                  ]
                },
                "minItems": 1,
                "title": "Statements",
                "type": "array"
              },
              "operator": {
                "allOf": [
                  {
                    "$ref": "#/$defs/StatementsGroupsOperator"
                  }
                ],
                "default": "or"
              }
            },
            "required": ["type", "statements"],
            "title": "StatementGroup",
            "type": "object"
          },
          "StatementsGroupsOperator": {
            "enum": ["and", "or"],
            "title": "StatementsGroupsOperator",
            "type": "string"
          },
          "StaticCaseResult": {
            "properties": {
              "type": {
                "const": "StaticCaseResult",
                "enum": ["StaticCaseResult"],
                "title": "Type",
                "type": "string"
              },
              "value": {
                "title": "Value"
              }
            },
            "required": ["type", "value"],
            "title": "StaticCaseResult",
            "type": "object"
          },
          "StaticOperand": {
            "properties": {
              "type": {
                "const": "StaticOperand",
                "enum": ["StaticOperand"],
                "title": "Type",
                "type": "string"
              },
              "value": {
                "title": "Value"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "value"],
            "title": "StaticOperand",
            "type": "object"
          },
          "StringContains": {
            "description": "Checks if string given as first value contains string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) contains",
                "enum": ["(String) contains"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringContains",
            "type": "object"
          },
          "StringEndsWith": {
            "description": "Checks if string given as first value ends with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) endsWith",
                "enum": ["(String) endsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringEndsWith",
            "type": "object"
          },
          "StringMatches": {
            "compound": false,
            "description": "Checks if string matches regex",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringMatches",
                "enum": ["StringMatches"],
                "title": "Type",
                "type": "string"
              },
              "regex": {
                "title": "Regex",
                "type": "string"
              }
            },
            "required": ["type", "regex"],
            "title": "StringMatches",
            "type": "object"
          },
          "StringStartsWith": {
            "description": "Checks if string given as first value starts with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) startsWith",
                "enum": ["(String) startsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringStartsWith",
            "type": "object"
          },
          "StringSubSequence": {
            "compound": false,
            "description": "Takes sub-string of the input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringSubSequence",
                "enum": ["StringSubSequence"],
                "title": "Type",
                "type": "string"
              },
              "start": {
                "default": 0,
                "title": "Start",
                "type": "integer"
              },
              "end": {
                "default": -1,
                "title": "End",
                "type": "integer"
              }
            },
            "required": ["type"],
            "title": "StringSubSequence",
            "type": "object"
          },
          "StringToLowerCase": {
            "compound": false,
            "description": "Executes lowercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToLowerCase",
                "enum": ["StringToLowerCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToLowerCase",
            "type": "object"
          },
          "StringToUpperCase": {
            "compound": false,
            "description": "Executes uppercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToUpperCase",
                "enum": ["StringToUpperCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToUpperCase",
            "type": "object"
          },
          "ToBoolean": {
            "compound": false,
            "description": "Changes input data into boolean",
            "input_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToBoolean",
                "enum": ["ToBoolean"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToBoolean",
            "type": "object"
          },
          "ToNumber": {
            "compound": false,
            "description": "Changes value into number - float or int depending on configuration",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToNumber",
                "enum": ["ToNumber"],
                "title": "Type",
                "type": "string"
              },
              "cast_to": {
                "$ref": "#/$defs/NumberCastingMode"
              }
            },
            "required": ["type", "cast_to"],
            "title": "ToNumber",
            "type": "object"
          },
          "ToString": {
            "compound": false,
            "description": "Stringifies data",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToString",
                "enum": ["ToString"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToString",
            "type": "object"
          },
          "UnaryStatement": {
            "properties": {
              "type": {
                "const": "UnaryStatement",
                "enum": ["UnaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Operand"
              },
              "operator": {
                "discriminator": {
                  "mapping": {
                    "(Boolean) is False": "#/$defs/IsFalse",
                    "(Boolean) is True": "#/$defs/IsTrue",
                    "(Sequence) is empty": "#/$defs/IsEmpty",
                    "(Sequence) is not empty": "#/$defs/IsNotEmpty",
                    "DoesNotExist": "#/$defs/DoesNotExist",
                    "Exists": "#/$defs/Exists"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/Exists"
                  },
                  {
                    "$ref": "#/$defs/DoesNotExist"
                  },
                  {
                    "$ref": "#/$defs/IsTrue"
                  },
                  {
                    "$ref": "#/$defs/IsFalse"
                  },
                  {
                    "$ref": "#/$defs/IsEmpty"
                  },
                  {
                    "$ref": "#/$defs/IsNotEmpty"
                  }
                ],
                "title": "Operator"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "operand", "operator"],
            "title": "UnaryStatement",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "formatter",
        "license": "Apache-2.0",
        "long_description": "\nCreates specific output based on defined input variables and configured rules - which is\nuseful while creating business logic in workflows.\n\nBased on configuration, block takes input data, optionally performs operation on data, \nsave it as variables and evaluate switch-case like statements to get the final result.\n",
        "name": "Expression",
        "properties": {
          "type": {
            "enum": ["roboflow_core/expression@v1", "Expression"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "data": {
            "additionalProperties": {
              "anyOf": [
                {
                  "kind": [
                    {
                      "description": "Image in workflows",
                      "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                      "name": "Batch[image]"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_image",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_parameter",
                  "type": "string"
                },
                {
                  "kind": [
                    {
                      "description": "Equivalent of any element",
                      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                      "name": "*"
                    }
                  ],
                  "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                  "reference": true,
                  "selected_element": "step_output",
                  "type": "string"
                }
              ]
            },
            "description": "References data to be used to construct results",
            "examples": [
              "$inputs.confidence",
              "$inputs.image",
              "$steps.my_step.top"
            ],
            "title": "Data",
            "type": "object"
          },
          "data_operations": {
            "additionalProperties": {
              "items": {
                "discriminator": {
                  "mapping": {
                    "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                    "DetectionsFilter": "#/$defs/DetectionsFilter",
                    "DetectionsOffset": "#/$defs/DetectionsOffset",
                    "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                    "DetectionsSelection": "#/$defs/DetectionsSelection",
                    "DetectionsShift": "#/$defs/DetectionsShift",
                    "Divide": "#/$defs/Divide",
                    "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                    "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                    "LookupTable": "#/$defs/LookupTable",
                    "Multiply": "#/$defs/Multiply",
                    "NumberRound": "#/$defs/NumberRound",
                    "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                    "RandomNumber": "#/$defs/RandomNumber",
                    "SequenceAggregate": "#/$defs/SequenceAggregate",
                    "SequenceApply": "#/$defs/SequenceApply",
                    "SequenceLength": "#/$defs/SequenceLength",
                    "SequenceMap": "#/$defs/SequenceMap",
                    "SortDetections": "#/$defs/SortDetections",
                    "StringMatches": "#/$defs/StringMatches",
                    "StringSubSequence": "#/$defs/StringSubSequence",
                    "StringToLowerCase": "#/$defs/StringToLowerCase",
                    "StringToUpperCase": "#/$defs/StringToUpperCase",
                    "ToBoolean": "#/$defs/ToBoolean",
                    "ToNumber": "#/$defs/ToNumber",
                    "ToString": "#/$defs/ToString"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StringToLowerCase"
                  },
                  {
                    "$ref": "#/$defs/StringToUpperCase"
                  },
                  {
                    "$ref": "#/$defs/LookupTable"
                  },
                  {
                    "$ref": "#/$defs/ToNumber"
                  },
                  {
                    "$ref": "#/$defs/NumberRound"
                  },
                  {
                    "$ref": "#/$defs/SequenceMap"
                  },
                  {
                    "$ref": "#/$defs/SequenceApply"
                  },
                  {
                    "$ref": "#/$defs/NumericSequenceAggregate"
                  },
                  {
                    "$ref": "#/$defs/ToString"
                  },
                  {
                    "$ref": "#/$defs/ToBoolean"
                  },
                  {
                    "$ref": "#/$defs/StringSubSequence"
                  },
                  {
                    "$ref": "#/$defs/DetectionsPropertyExtract"
                  },
                  {
                    "$ref": "#/$defs/SequenceAggregate"
                  },
                  {
                    "$ref": "#/$defs/ExtractDetectionProperty"
                  },
                  {
                    "$ref": "#/$defs/DetectionsFilter"
                  },
                  {
                    "$ref": "#/$defs/DetectionsOffset"
                  },
                  {
                    "$ref": "#/$defs/DetectionsShift"
                  },
                  {
                    "$ref": "#/$defs/RandomNumber"
                  },
                  {
                    "$ref": "#/$defs/StringMatches"
                  },
                  {
                    "$ref": "#/$defs/ExtractImageProperty"
                  },
                  {
                    "$ref": "#/$defs/SequenceLength"
                  },
                  {
                    "$ref": "#/$defs/Multiply"
                  },
                  {
                    "$ref": "#/$defs/Divide"
                  },
                  {
                    "$ref": "#/$defs/DetectionsSelection"
                  },
                  {
                    "$ref": "#/$defs/SortDetections"
                  },
                  {
                    "$ref": "#/$defs/ClassificationPropertyExtract"
                  }
                ]
              },
              "type": "array"
            },
            "description": "UQL definitions of operations to be performed on defined data before switch-case instruction",
            "examples": [
              {
                "predictions": [
                  {
                    "property_name": "class_name",
                    "type": "DetectionsPropertyExtract"
                  }
                ]
              }
            ],
            "title": "Data Operations",
            "type": "object"
          },
          "switch": {
            "$ref": "#/$defs/CasesDefinition"
          }
        },
        "required": ["type", "name", "data", "switch"],
        "short_description": "Creates specific output based on defined input variables and configured rules.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "output",
          "kind": [
            {
              "name": "*",
              "description": "Equivalent of any element",
              "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.formatters.expression.v1.ExpressionBlockV1",
      "human_friendly_block_name": "Expression",
      "manifest_type_identifier": "roboflow_core/expression@v1",
      "manifest_type_identifier_aliases": ["Expression"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "$defs": {
          "BinaryStatement": {
            "properties": {
              "type": {
                "const": "BinaryStatement",
                "enum": ["BinaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "left_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Left Operand"
              },
              "comparator": {
                "discriminator": {
                  "mapping": {
                    "!=": "#/$defs/NotEquals",
                    "(Detection) in zone": "#/$defs/DetectionInZone",
                    "(Number) !=": "#/$defs/NotEquals",
                    "(Number) <": "#/$defs/NumberLower",
                    "(Number) <=": "#/$defs/NumberLowerEqual",
                    "(Number) ==": "#/$defs/Equals",
                    "(Number) >": "#/$defs/NumberGreater",
                    "(Number) >=": "#/$defs/NumberGreaterEqual",
                    "(String) contains": "#/$defs/StringContains",
                    "(String) endsWith": "#/$defs/StringEndsWith",
                    "(String) startsWith": "#/$defs/StringStartsWith",
                    "==": "#/$defs/Equals",
                    "in (Sequence)": "#/$defs/In"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/In"
                  },
                  {
                    "$ref": "#/$defs/StringContains"
                  },
                  {
                    "$ref": "#/$defs/StringEndsWith"
                  },
                  {
                    "$ref": "#/$defs/StringStartsWith"
                  },
                  {
                    "$ref": "#/$defs/NumberLowerEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberLower"
                  },
                  {
                    "$ref": "#/$defs/NumberGreaterEqual"
                  },
                  {
                    "$ref": "#/$defs/NumberGreater"
                  },
                  {
                    "$ref": "#/$defs/NotEquals"
                  },
                  {
                    "$ref": "#/$defs/Equals"
                  },
                  {
                    "$ref": "#/$defs/DetectionInZone"
                  }
                ],
                "title": "Comparator"
              },
              "right_operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Right Operand"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "left_operand", "comparator", "right_operand"],
            "title": "BinaryStatement",
            "type": "object"
          },
          "ClassificationProperty": {
            "enum": [
              "top_class",
              "top_class_confidence",
              "all_classes",
              "all_confidences"
            ],
            "title": "ClassificationProperty",
            "type": "string"
          },
          "ClassificationPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "Batch[classification_prediction]",
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ClassificationPropertyExtract",
                "enum": ["ClassificationPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ClassificationProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ClassificationPropertyExtract",
            "type": "object"
          },
          "DetectionInZone": {
            "description": "Checks if detection is in zone",
            "operands_kinds": [
              [
                {
                  "name": "detection",
                  "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                  "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
                },
                {
                  "name": "zone",
                  "description": "Definition of polygon zone",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Detection) in zone",
                "enum": ["(Detection) in zone"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DetectionInZone",
            "type": "object"
          },
          "DetectionsFilter": {
            "compound": true,
            "description": "Filters out unwanted elements from detections-based prediction by applying filter operation in context of every single detection within prediction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsFilter",
                "enum": ["DetectionsFilter"],
                "title": "Type",
                "type": "string"
              },
              "filter_operation": {
                "$ref": "#/$defs/StatementGroup"
              }
            },
            "required": ["type", "filter_operation"],
            "title": "DetectionsFilter",
            "type": "object"
          },
          "DetectionsOffset": {
            "compound": false,
            "description": "Makes detected bounding boxes bigger by applying offset to its size",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsOffset",
                "enum": ["DetectionsOffset"],
                "title": "Type",
                "type": "string"
              },
              "offset_x": {
                "title": "Offset X",
                "type": "integer"
              },
              "offset_y": {
                "title": "Offset Y",
                "type": "integer"
              }
            },
            "required": ["type", "offset_x", "offset_y"],
            "title": "DetectionsOffset",
            "type": "object"
          },
          "DetectionsProperty": {
            "enum": [
              "confidence",
              "class_name",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "class_id",
              "center",
              "top_left",
              "top_right",
              "bottom_left",
              "bottom_right"
            ],
            "title": "DetectionsProperty",
            "type": "string"
          },
          "DetectionsPropertyExtract": {
            "compound": false,
            "description": "Extracts property from detections-based prediction(as a list of elements - one element represents single detection)",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsPropertyExtract",
                "enum": ["DetectionsPropertyExtract"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "DetectionsPropertyExtract",
            "type": "object"
          },
          "DetectionsSelection": {
            "compound": false,
            "description": "Selects bounding boxes based on predefined criterias",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsSelection",
                "enum": ["DetectionsSelection"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSelectionMode"
              }
            },
            "required": ["type", "mode"],
            "title": "DetectionsSelection",
            "type": "object"
          },
          "DetectionsSelectionMode": {
            "enum": ["left_most", "right_most"],
            "title": "DetectionsSelectionMode",
            "type": "string"
          },
          "DetectionsShift": {
            "compound": false,
            "description": "Shifting detected bounding boxes in assigned direction",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DetectionsShift",
                "enum": ["DetectionsShift"],
                "title": "Type",
                "type": "string"
              },
              "shift_x": {
                "title": "Shift X",
                "type": "integer"
              },
              "shift_y": {
                "title": "Shift Y",
                "type": "integer"
              }
            },
            "required": ["type", "shift_x", "shift_y"],
            "title": "DetectionsShift",
            "type": "object"
          },
          "DetectionsSortProperties": {
            "enum": [
              "confidence",
              "x_min",
              "x_max",
              "y_min",
              "y_max",
              "size",
              "center_x",
              "center_y"
            ],
            "title": "DetectionsSortProperties",
            "type": "string"
          },
          "Divide": {
            "compound": false,
            "description": "Dividing value against other",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Divide",
                "enum": ["Divide"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Divide",
            "type": "object"
          },
          "DoesNotExist": {
            "description": "Checks if value is not given (`None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "DoesNotExist",
                "enum": ["DoesNotExist"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DoesNotExist",
            "type": "object"
          },
          "DynamicOperand": {
            "properties": {
              "type": {
                "const": "DynamicOperand",
                "enum": ["DynamicOperand"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              },
              "operand_name": {
                "default": "_",
                "title": "Operand Name",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "DynamicOperand",
            "type": "object"
          },
          "Equals": {
            "description": "Checks if two values given are equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) ==", "=="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Equals",
            "type": "object"
          },
          "Exists": {
            "description": "Checks if value is given (not `None`)",
            "operands_kinds": [
              [
                {
                  "name": "*",
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Exists",
                "enum": ["Exists"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "Exists",
            "type": "object"
          },
          "ExtractDetectionProperty": {
            "compound": false,
            "description": "Extracts property from single detection",
            "input_kind": [
              {
                "name": "detection",
                "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
                "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractDetectionProperty",
                "enum": ["ExtractDetectionProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/DetectionsProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractDetectionProperty",
            "type": "object"
          },
          "ExtractImageProperty": {
            "description": "Extracts specific property of image (like size)",
            "input_kind": [
              {
                "name": "image",
                "description": "Image in workflows",
                "docs": "TODO"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ExtractImageProperty",
                "enum": ["ExtractImageProperty"],
                "title": "Type",
                "type": "string"
              },
              "property_name": {
                "$ref": "#/$defs/ImageProperty"
              }
            },
            "required": ["type", "property_name"],
            "title": "ExtractImageProperty",
            "type": "object"
          },
          "ImageProperty": {
            "enum": ["size", "height", "width"],
            "title": "ImageProperty",
            "type": "string"
          },
          "In": {
            "description": "Checks if first value is element of second value (usually list or dictionary)",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "in (Sequence)",
                "enum": ["in (Sequence)"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "In",
            "type": "object"
          },
          "IsEmpty": {
            "description": "Checks if sequence is empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is empty",
                "enum": ["(Sequence) is empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsEmpty",
            "type": "object"
          },
          "IsFalse": {
            "description": "Checks if value is `False`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is False",
                "enum": ["(Boolean) is False"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsFalse",
            "type": "object"
          },
          "IsNotEmpty": {
            "description": "Checks if sequence is not empty",
            "operands_kinds": [
              [
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                },
                {
                  "name": "dictionary",
                  "description": "Dictionary",
                  "docs": null
                },
                {
                  "name": "object_detection_prediction",
                  "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
                },
                {
                  "name": "instance_segmentation_prediction",
                  "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
                },
                {
                  "name": "keypoint_detection_prediction",
                  "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                  "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Sequence) is not empty",
                "enum": ["(Sequence) is not empty"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsNotEmpty",
            "type": "object"
          },
          "IsTrue": {
            "description": "Checks if value is `True`",
            "operands_kinds": [
              [
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                }
              ]
            ],
            "operands_number": 1,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Boolean) is True",
                "enum": ["(Boolean) is True"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "IsTrue",
            "type": "object"
          },
          "LookupTable": {
            "compound": false,
            "description": "Changes value according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "LookupTable",
                "enum": ["LookupTable"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "LookupTable",
            "type": "object"
          },
          "Multiply": {
            "compound": false,
            "description": "Multiplication",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "Multiply",
                "enum": ["Multiply"],
                "title": "Type",
                "type": "string"
              },
              "other": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "number"
                  }
                ],
                "title": "Other"
              }
            },
            "required": ["type", "other"],
            "title": "Multiply",
            "type": "object"
          },
          "NotEquals": {
            "description": "Checks if two values given are not equal",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                },
                {
                  "name": "boolean",
                  "description": "Boolean flag",
                  "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
                },
                {
                  "name": "list_of_values",
                  "description": "List of values of any types",
                  "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "enum": ["(Number) !=", "!="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NotEquals",
            "type": "object"
          },
          "NumberCastingMode": {
            "enum": ["int", "float"],
            "title": "NumberCastingMode",
            "type": "string"
          },
          "NumberGreater": {
            "description": "Checks if first value (number) is greater than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >",
                "enum": ["(Number) >"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreater",
            "type": "object"
          },
          "NumberGreaterEqual": {
            "description": "Checks if first value (number) is greater or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) >=",
                "enum": ["(Number) >="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberGreaterEqual",
            "type": "object"
          },
          "NumberLower": {
            "description": "Checks if first value (number) is lower than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <",
                "enum": ["(Number) <"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLower",
            "type": "object"
          },
          "NumberLowerEqual": {
            "description": "Checks if first value (number) is lower or equal than the second value (number)",
            "operands_kinds": [
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ],
              [
                {
                  "name": "integer",
                  "description": "Integer value",
                  "docs": "\nExamples:\n```\n1\n2\n```\n"
                },
                {
                  "name": "float",
                  "description": "Float value",
                  "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
                },
                {
                  "name": "float_zero_to_one",
                  "description": "`float` value in range `[0.0, 1.0]`",
                  "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(Number) <=",
                "enum": ["(Number) <="],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "NumberLowerEqual",
            "type": "object"
          },
          "NumberRound": {
            "compound": false,
            "description": "Rounds the number",
            "input_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumberRound",
                "enum": ["NumberRound"],
                "title": "Type",
                "type": "string"
              },
              "decimal_digits": {
                "title": "Decimal Digits",
                "type": "integer"
              }
            },
            "required": ["type", "decimal_digits"],
            "title": "NumberRound",
            "type": "object"
          },
          "NumericSequenceAggregate": {
            "compound": false,
            "description": "Aggregates numeric sequence using aggregation function like min or max - adjusted to work on numbers",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "NumericSequenceAggregate",
                "enum": ["NumericSequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "function": {
                "$ref": "#/$defs/SequenceAggregationFunction"
              },
              "neutral_value": {
                "default": null,
                "title": "Neutral Value"
              }
            },
            "required": ["type", "function"],
            "title": "NumericSequenceAggregate",
            "type": "object"
          },
          "RandomNumber": {
            "description": "Special operation to let random sampling - ignoring input data and changing it into random floating point value. To be used mainly to sample predictions or images.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "RandomNumber",
                "enum": ["RandomNumber"],
                "title": "Type",
                "type": "string"
              },
              "min_value": {
                "default": 0,
                "title": "Min Value",
                "type": "number"
              },
              "max_value": {
                "default": 1,
                "title": "Max Value",
                "type": "number"
              }
            },
            "required": ["type"],
            "title": "RandomNumber",
            "type": "object"
          },
          "SequenceAggregate": {
            "compound": false,
            "description": "Aggregates sequence using generic aggregation methods - adjusted to majority data types",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceAggregate",
                "enum": ["SequenceAggregate"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/SequenceAggregationMode"
              }
            },
            "required": ["type", "mode"],
            "title": "SequenceAggregate",
            "type": "object"
          },
          "SequenceAggregationFunction": {
            "enum": ["min", "max"],
            "title": "SequenceAggregationFunction",
            "type": "string"
          },
          "SequenceAggregationMode": {
            "enum": ["first", "last", "most_common", "least_common"],
            "title": "SequenceAggregationMode",
            "type": "string"
          },
          "SequenceApply": {
            "compound": true,
            "description": "Operation applies chain of operations at every element of sequence",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceApply",
                "enum": ["SequenceApply"],
                "title": "Type",
                "type": "string"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "operations"],
            "title": "SequenceApply",
            "type": "object"
          },
          "SequenceLength": {
            "compound": false,
            "description": "Operation determines the length of input sequence",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              },
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              },
              {
                "name": "dictionary",
                "description": "Dictionary",
                "docs": null
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceLength",
                "enum": ["SequenceLength"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "SequenceLength",
            "type": "object"
          },
          "SequenceMap": {
            "compound": true,
            "description": "Changes each value of sequence according to mapping stated in lookup table",
            "input_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "nested_operation_input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "nested_operation_output_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "list_of_values",
                "description": "List of values of any types",
                "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SequenceMap",
                "enum": ["SequenceMap"],
                "title": "Type",
                "type": "string"
              },
              "lookup_table": {
                "title": "Lookup Table",
                "type": "object"
              }
            },
            "required": ["type", "lookup_table"],
            "title": "SequenceMap",
            "type": "object"
          },
          "SortDetections": {
            "compound": false,
            "description": "Changes the order of detected bounding boxes.",
            "input_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "object_detection_prediction",
                "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
              },
              {
                "name": "instance_segmentation_prediction",
                "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
              },
              {
                "name": "keypoint_detection_prediction",
                "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
                "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "SortDetections",
                "enum": ["SortDetections"],
                "title": "Type",
                "type": "string"
              },
              "mode": {
                "$ref": "#/$defs/DetectionsSortProperties"
              },
              "ascending": {
                "default": true,
                "title": "Ascending",
                "type": "boolean"
              }
            },
            "required": ["type", "mode"],
            "title": "SortDetections",
            "type": "object"
          },
          "StatementGroup": {
            "properties": {
              "type": {
                "const": "StatementGroup",
                "enum": ["StatementGroup"],
                "title": "Type",
                "type": "string"
              },
              "statements": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "BinaryStatement": "#/$defs/BinaryStatement",
                      "StatementGroup": "#/$defs/StatementGroup",
                      "UnaryStatement": "#/$defs/UnaryStatement"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/BinaryStatement"
                    },
                    {
                      "$ref": "#/$defs/UnaryStatement"
                    },
                    {
                      "$ref": "#/$defs/StatementGroup"
                    }
                  ]
                },
                "minItems": 1,
                "title": "Statements",
                "type": "array"
              },
              "operator": {
                "allOf": [
                  {
                    "$ref": "#/$defs/StatementsGroupsOperator"
                  }
                ],
                "default": "or"
              }
            },
            "required": ["type", "statements"],
            "title": "StatementGroup",
            "type": "object"
          },
          "StatementsGroupsOperator": {
            "enum": ["and", "or"],
            "title": "StatementsGroupsOperator",
            "type": "string"
          },
          "StaticOperand": {
            "properties": {
              "type": {
                "const": "StaticOperand",
                "enum": ["StaticOperand"],
                "title": "Type",
                "type": "string"
              },
              "value": {
                "title": "Value"
              },
              "operations": {
                "items": {
                  "discriminator": {
                    "mapping": {
                      "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                      "DetectionsFilter": "#/$defs/DetectionsFilter",
                      "DetectionsOffset": "#/$defs/DetectionsOffset",
                      "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                      "DetectionsSelection": "#/$defs/DetectionsSelection",
                      "DetectionsShift": "#/$defs/DetectionsShift",
                      "Divide": "#/$defs/Divide",
                      "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                      "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                      "LookupTable": "#/$defs/LookupTable",
                      "Multiply": "#/$defs/Multiply",
                      "NumberRound": "#/$defs/NumberRound",
                      "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                      "RandomNumber": "#/$defs/RandomNumber",
                      "SequenceAggregate": "#/$defs/SequenceAggregate",
                      "SequenceApply": "#/$defs/SequenceApply",
                      "SequenceLength": "#/$defs/SequenceLength",
                      "SequenceMap": "#/$defs/SequenceMap",
                      "SortDetections": "#/$defs/SortDetections",
                      "StringMatches": "#/$defs/StringMatches",
                      "StringSubSequence": "#/$defs/StringSubSequence",
                      "StringToLowerCase": "#/$defs/StringToLowerCase",
                      "StringToUpperCase": "#/$defs/StringToUpperCase",
                      "ToBoolean": "#/$defs/ToBoolean",
                      "ToNumber": "#/$defs/ToNumber",
                      "ToString": "#/$defs/ToString"
                    },
                    "propertyName": "type"
                  },
                  "oneOf": [
                    {
                      "$ref": "#/$defs/StringToLowerCase"
                    },
                    {
                      "$ref": "#/$defs/StringToUpperCase"
                    },
                    {
                      "$ref": "#/$defs/LookupTable"
                    },
                    {
                      "$ref": "#/$defs/ToNumber"
                    },
                    {
                      "$ref": "#/$defs/NumberRound"
                    },
                    {
                      "$ref": "#/$defs/SequenceMap"
                    },
                    {
                      "$ref": "#/$defs/SequenceApply"
                    },
                    {
                      "$ref": "#/$defs/NumericSequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ToString"
                    },
                    {
                      "$ref": "#/$defs/ToBoolean"
                    },
                    {
                      "$ref": "#/$defs/StringSubSequence"
                    },
                    {
                      "$ref": "#/$defs/DetectionsPropertyExtract"
                    },
                    {
                      "$ref": "#/$defs/SequenceAggregate"
                    },
                    {
                      "$ref": "#/$defs/ExtractDetectionProperty"
                    },
                    {
                      "$ref": "#/$defs/DetectionsFilter"
                    },
                    {
                      "$ref": "#/$defs/DetectionsOffset"
                    },
                    {
                      "$ref": "#/$defs/DetectionsShift"
                    },
                    {
                      "$ref": "#/$defs/RandomNumber"
                    },
                    {
                      "$ref": "#/$defs/StringMatches"
                    },
                    {
                      "$ref": "#/$defs/ExtractImageProperty"
                    },
                    {
                      "$ref": "#/$defs/SequenceLength"
                    },
                    {
                      "$ref": "#/$defs/Multiply"
                    },
                    {
                      "$ref": "#/$defs/Divide"
                    },
                    {
                      "$ref": "#/$defs/DetectionsSelection"
                    },
                    {
                      "$ref": "#/$defs/SortDetections"
                    },
                    {
                      "$ref": "#/$defs/ClassificationPropertyExtract"
                    }
                  ]
                },
                "title": "Operations",
                "type": "array"
              }
            },
            "required": ["type", "value"],
            "title": "StaticOperand",
            "type": "object"
          },
          "StringContains": {
            "description": "Checks if string given as first value contains string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) contains",
                "enum": ["(String) contains"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringContains",
            "type": "object"
          },
          "StringEndsWith": {
            "description": "Checks if string given as first value ends with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) endsWith",
                "enum": ["(String) endsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringEndsWith",
            "type": "object"
          },
          "StringMatches": {
            "compound": false,
            "description": "Checks if string matches regex",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringMatches",
                "enum": ["StringMatches"],
                "title": "Type",
                "type": "string"
              },
              "regex": {
                "title": "Regex",
                "type": "string"
              }
            },
            "required": ["type", "regex"],
            "title": "StringMatches",
            "type": "object"
          },
          "StringStartsWith": {
            "description": "Checks if string given as first value starts with string provided as second value",
            "operands_kinds": [
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ],
              [
                {
                  "name": "string",
                  "description": "String value",
                  "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
                }
              ]
            ],
            "operands_number": 2,
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "(String) startsWith",
                "enum": ["(String) startsWith"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringStartsWith",
            "type": "object"
          },
          "StringSubSequence": {
            "compound": false,
            "description": "Takes sub-string of the input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringSubSequence",
                "enum": ["StringSubSequence"],
                "title": "Type",
                "type": "string"
              },
              "start": {
                "default": 0,
                "title": "Start",
                "type": "integer"
              },
              "end": {
                "default": -1,
                "title": "End",
                "type": "integer"
              }
            },
            "required": ["type"],
            "title": "StringSubSequence",
            "type": "object"
          },
          "StringToLowerCase": {
            "compound": false,
            "description": "Executes lowercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToLowerCase",
                "enum": ["StringToLowerCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToLowerCase",
            "type": "object"
          },
          "StringToUpperCase": {
            "compound": false,
            "description": "Executes uppercase operation on input string",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "StringToUpperCase",
                "enum": ["StringToUpperCase"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "StringToUpperCase",
            "type": "object"
          },
          "ToBoolean": {
            "compound": false,
            "description": "Changes input data into boolean",
            "input_kind": [
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToBoolean",
                "enum": ["ToBoolean"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToBoolean",
            "type": "object"
          },
          "ToNumber": {
            "compound": false,
            "description": "Changes value into number - float or int depending on configuration",
            "input_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              },
              {
                "name": "boolean",
                "description": "Boolean flag",
                "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
              },
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "output_kind": [
              {
                "name": "integer",
                "description": "Integer value",
                "docs": "\nExamples:\n```\n1\n2\n```\n"
              },
              {
                "name": "float",
                "description": "Float value",
                "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
              },
              {
                "name": "float_zero_to_one",
                "description": "`float` value in range `[0.0, 1.0]`",
                "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToNumber",
                "enum": ["ToNumber"],
                "title": "Type",
                "type": "string"
              },
              "cast_to": {
                "$ref": "#/$defs/NumberCastingMode"
              }
            },
            "required": ["type", "cast_to"],
            "title": "ToNumber",
            "type": "object"
          },
          "ToString": {
            "compound": false,
            "description": "Stringifies data",
            "input_kind": [
              {
                "name": "*",
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
              }
            ],
            "output_kind": [
              {
                "name": "string",
                "description": "String value",
                "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
              }
            ],
            "properties": {
              "type": {
                "const": "ToString",
                "enum": ["ToString"],
                "title": "Type",
                "type": "string"
              }
            },
            "required": ["type"],
            "title": "ToString",
            "type": "object"
          },
          "UnaryStatement": {
            "properties": {
              "type": {
                "const": "UnaryStatement",
                "enum": ["UnaryStatement"],
                "title": "Type",
                "type": "string"
              },
              "operand": {
                "discriminator": {
                  "mapping": {
                    "DynamicOperand": "#/$defs/DynamicOperand",
                    "StaticOperand": "#/$defs/StaticOperand"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/StaticOperand"
                  },
                  {
                    "$ref": "#/$defs/DynamicOperand"
                  }
                ],
                "title": "Operand"
              },
              "operator": {
                "discriminator": {
                  "mapping": {
                    "(Boolean) is False": "#/$defs/IsFalse",
                    "(Boolean) is True": "#/$defs/IsTrue",
                    "(Sequence) is empty": "#/$defs/IsEmpty",
                    "(Sequence) is not empty": "#/$defs/IsNotEmpty",
                    "DoesNotExist": "#/$defs/DoesNotExist",
                    "Exists": "#/$defs/Exists"
                  },
                  "propertyName": "type"
                },
                "oneOf": [
                  {
                    "$ref": "#/$defs/Exists"
                  },
                  {
                    "$ref": "#/$defs/DoesNotExist"
                  },
                  {
                    "$ref": "#/$defs/IsTrue"
                  },
                  {
                    "$ref": "#/$defs/IsFalse"
                  },
                  {
                    "$ref": "#/$defs/IsEmpty"
                  },
                  {
                    "$ref": "#/$defs/IsNotEmpty"
                  }
                ],
                "title": "Operator"
              },
              "negate": {
                "default": false,
                "title": "Negate",
                "type": "boolean"
              }
            },
            "required": ["type", "operand", "operator"],
            "title": "UnaryStatement",
            "type": "object"
          }
        },
        "additionalProperties": true,
        "block_type": "formatter",
        "license": "Apache-2.0",
        "long_description": "\nDefine a field using properties from previous workflow steps.\n\nExample use-cases:\n* extraction of all class names for object-detection predictions\n* extraction of class / confidence from classification result\n* extraction ocr text from OCR result\n",
        "name": "Property Definition",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/property_definition@v1",
              "PropertyDefinition",
              "PropertyExtraction"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "data": {
            "description": "Reference data to extract property from",
            "examples": ["$steps.my_step.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[classification_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Data",
            "type": "string"
          },
          "operations": {
            "items": {
              "discriminator": {
                "mapping": {
                  "ClassificationPropertyExtract": "#/$defs/ClassificationPropertyExtract",
                  "DetectionsFilter": "#/$defs/DetectionsFilter",
                  "DetectionsOffset": "#/$defs/DetectionsOffset",
                  "DetectionsPropertyExtract": "#/$defs/DetectionsPropertyExtract",
                  "DetectionsSelection": "#/$defs/DetectionsSelection",
                  "DetectionsShift": "#/$defs/DetectionsShift",
                  "Divide": "#/$defs/Divide",
                  "ExtractDetectionProperty": "#/$defs/ExtractDetectionProperty",
                  "ExtractImageProperty": "#/$defs/ExtractImageProperty",
                  "LookupTable": "#/$defs/LookupTable",
                  "Multiply": "#/$defs/Multiply",
                  "NumberRound": "#/$defs/NumberRound",
                  "NumericSequenceAggregate": "#/$defs/NumericSequenceAggregate",
                  "RandomNumber": "#/$defs/RandomNumber",
                  "SequenceAggregate": "#/$defs/SequenceAggregate",
                  "SequenceApply": "#/$defs/SequenceApply",
                  "SequenceLength": "#/$defs/SequenceLength",
                  "SequenceMap": "#/$defs/SequenceMap",
                  "SortDetections": "#/$defs/SortDetections",
                  "StringMatches": "#/$defs/StringMatches",
                  "StringSubSequence": "#/$defs/StringSubSequence",
                  "StringToLowerCase": "#/$defs/StringToLowerCase",
                  "StringToUpperCase": "#/$defs/StringToUpperCase",
                  "ToBoolean": "#/$defs/ToBoolean",
                  "ToNumber": "#/$defs/ToNumber",
                  "ToString": "#/$defs/ToString"
                },
                "propertyName": "type"
              },
              "oneOf": [
                {
                  "$ref": "#/$defs/StringToLowerCase"
                },
                {
                  "$ref": "#/$defs/StringToUpperCase"
                },
                {
                  "$ref": "#/$defs/LookupTable"
                },
                {
                  "$ref": "#/$defs/ToNumber"
                },
                {
                  "$ref": "#/$defs/NumberRound"
                },
                {
                  "$ref": "#/$defs/SequenceMap"
                },
                {
                  "$ref": "#/$defs/SequenceApply"
                },
                {
                  "$ref": "#/$defs/NumericSequenceAggregate"
                },
                {
                  "$ref": "#/$defs/ToString"
                },
                {
                  "$ref": "#/$defs/ToBoolean"
                },
                {
                  "$ref": "#/$defs/StringSubSequence"
                },
                {
                  "$ref": "#/$defs/DetectionsPropertyExtract"
                },
                {
                  "$ref": "#/$defs/SequenceAggregate"
                },
                {
                  "$ref": "#/$defs/ExtractDetectionProperty"
                },
                {
                  "$ref": "#/$defs/DetectionsFilter"
                },
                {
                  "$ref": "#/$defs/DetectionsOffset"
                },
                {
                  "$ref": "#/$defs/DetectionsShift"
                },
                {
                  "$ref": "#/$defs/RandomNumber"
                },
                {
                  "$ref": "#/$defs/StringMatches"
                },
                {
                  "$ref": "#/$defs/ExtractImageProperty"
                },
                {
                  "$ref": "#/$defs/SequenceLength"
                },
                {
                  "$ref": "#/$defs/Multiply"
                },
                {
                  "$ref": "#/$defs/Divide"
                },
                {
                  "$ref": "#/$defs/DetectionsSelection"
                },
                {
                  "$ref": "#/$defs/SortDetections"
                },
                {
                  "$ref": "#/$defs/ClassificationPropertyExtract"
                }
              ]
            },
            "title": "Operations",
            "type": "array"
          }
        },
        "required": ["type", "name", "data", "operations"],
        "search_keywords": [
          "property",
          "field",
          "number",
          "count",
          "classes",
          "confidences",
          "labels",
          "coordinates"
        ],
        "short_description": "Define a variable from model predictions, such as the class names, confidences, or number of detections.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "output",
          "kind": [
            {
              "name": "*",
              "description": "Equivalent of any element",
              "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.formatters.property_definition.v1.PropertyDefinitionBlockV1",
      "human_friendly_block_name": "Property Definition",
      "manifest_type_identifier": "roboflow_core/property_definition@v1",
      "manifest_type_identifier_aliases": [
        "PropertyDefinition",
        "PropertyExtraction"
      ],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "fusion",
        "license": "Apache-2.0",
        "long_description": "\nTakes multiple step outputs at data depth level n, concatenate them into list and reduce dimensionality\nto level n-1.\n\nUseful in scenarios like:\n* aggregation of classification results for dynamically cropped images\n* aggregation of OCR results for dynamically cropped images\n",
        "name": "Dimension Collapse",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/dimension_collapse@v1",
              "DimensionCollapse"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "data": {
            "description": "Reference to step outputs at depth level n to be concatenated and moved into level n-1.",
            "examples": ["$steps.ocr_step.results"],
            "kind": [
              {
                "description": "Equivalent of any element",
                "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                "name": "*"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Data",
            "type": "string"
          }
        },
        "required": ["type", "name", "data"],
        "short_description": "Collapses dimensionality level by aggregation of nested data into list",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "output",
          "kind": [
            {
              "name": "list_of_values",
              "description": "List of values of any types",
              "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.fusion.dimension_collapse.v1.DimensionCollapseBlockV1",
      "human_friendly_block_name": "Dimension Collapse",
      "manifest_type_identifier": "roboflow_core/dimension_collapse@v1",
      "manifest_type_identifier_aliases": ["DimensionCollapse"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": -1
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "formatter",
        "license": "Apache-2.0",
        "long_description": "\nTakes input data which may not be present due to filtering or conditional execution and\nfills with default value to make it compliant with further processing.\n",
        "name": "First Non Empty Or Default",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/first_non_empty_or_default@v1",
              "FirstNonEmptyOrDefault"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "data": {
            "description": "Reference data to replace empty values",
            "examples": ["$steps.my_step.predictions"],
            "items": {
              "kind": [
                {
                  "description": "Equivalent of any element",
                  "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n",
                  "name": "*"
                }
              ],
              "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
              "reference": true,
              "selected_element": "step_output",
              "type": "string"
            },
            "minItems": 1,
            "title": "Data",
            "type": "array"
          },
          "default": {
            "default": null,
            "description": "Default value that will be placed whenever there is no data found",
            "examples": ["empty"],
            "title": "Default"
          }
        },
        "required": ["type", "name", "data"],
        "short_description": "Takes first non-empty data element or default",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "output",
          "kind": [
            {
              "name": "*",
              "description": "Equivalent of any element",
              "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.formatters.first_non_empty_or_default.v1.FirstNonEmptyOrDefaultBlockV1",
      "human_friendly_block_name": "First Non Empty Or Default",
      "manifest_type_identifier": "roboflow_core/first_non_empty_or_default@v1",
      "manifest_type_identifier_aliases": ["FirstNonEmptyOrDefault"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `BackgroundColorVisualization` block draws all areas\noutside of detected regions in an image with a specified\ncolor.\n",
        "name": "Background Color Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/background_color_visualization@v1",
              "BackgroundColorVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "BLACK",
            "description": "Color of the background.",
            "examples": [
              "WHITE",
              "#FFFFFF",
              "rgb(255, 255, 255)$inputs.background_color"
            ],
            "title": "Color"
          },
          "opacity": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.5,
            "description": "Transparency of the Mask overlay.",
            "examples": [0.5, "$inputs.opacity"],
            "title": "Opacity"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Paints a mask over all areas outside of detected regions in an image.",
        "title": "BackgroundColorManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.background_color.v1.BackgroundColorVisualizationBlockV1",
      "human_friendly_block_name": "Background Color Visualization",
      "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
      "manifest_type_identifier_aliases": ["BackgroundColorVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `BlurVisualization` block blurs detected\nobjects in an image using Supervision's `sv.BlurAnnotator`.\n",
        "name": "Blur Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/blur_visualization@v1",
              "BlurVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "kernel_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 15,
            "description": "Size of the average pooling kernel used for blurring.",
            "examples": [15, "$inputs.kernel_size"],
            "title": "Kernel Size"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Blurs detected objects in an image.",
        "title": "BlurManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.blur.v1.BlurVisualizationBlockV1",
      "human_friendly_block_name": "Blur Visualization",
      "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
      "manifest_type_identifier_aliases": ["BlurVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `BoundingBoxVisualization` block draws a box around detected\nobjects in an image using Supervision's `sv.RoundBoxAnnotator`.\n",
        "name": "Bounding Box Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/bounding_box_visualization@v1",
              "BoundingBoxVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 2,
            "description": "Thickness of the bounding box in pixels.",
            "examples": [2, "$inputs.thickness"],
            "title": "Thickness"
          },
          "roundness": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Roundness of the corners of the bounding box.",
            "examples": [0, "$inputs.roundness"],
            "title": "Roundness"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws a box around detected objects in an image.",
        "title": "BoundingBoxManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.bounding_box.v1.BoundingBoxVisualizationBlockV1",
      "human_friendly_block_name": "Bounding Box Visualization",
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "manifest_type_identifier_aliases": ["BoundingBoxVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `CircleVisualization` block draws a circle around detected\nobjects in an image using Supervision's `sv.CircleAnnotator`.\n",
        "name": "Circle Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/circle_visualization@v1",
              "CircleVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 2,
            "description": "Thickness of the lines in pixels.",
            "examples": [2, "$inputs.thickness"],
            "title": "Thickness"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws a circle around detected objects in an image.",
        "title": "CircleManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.circle.v1.CircleVisualizationBlockV1",
      "human_friendly_block_name": "Circle Visualization",
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "manifest_type_identifier_aliases": ["CircleVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `ColorVisualization` block paints a solid color on detected\nobjects in an image using Supervision's `sv.ColorAnnotator`.\n",
        "name": "Color Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/color_visualization@v1",
              "ColorVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "opacity": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.5,
            "description": "Transparency of the color overlay.",
            "examples": [0.5, "$inputs.opacity"],
            "title": "Opacity"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Paints a solid color on detected objects in an image.",
        "title": "ColorManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.color.v1.ColorVisualizationBlockV1",
      "human_friendly_block_name": "Color Visualization",
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "manifest_type_identifier_aliases": ["ColorVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `CornerVisualization` block draws the corners of detected\nobjects in an image using Supervision's `sv.BoxCornerAnnotator`.\n",
        "name": "Corner Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/corner_visualization@v1",
              "CornerVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 4,
            "description": "Thickness of the lines in pixels.",
            "examples": [4, "$inputs.thickness"],
            "title": "Thickness"
          },
          "corner_length": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 15,
            "description": "Length of the corner lines in pixels.",
            "examples": [15, "$inputs.corner_length"],
            "title": "Corner Length"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws the corners of detected objects in an image.",
        "title": "CornerManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.corner.v1.CornerVisualizationBlockV1",
      "human_friendly_block_name": "Corner Visualization",
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "manifest_type_identifier_aliases": ["CornerVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `CropVisualization` block draws scaled up crops of detections\non the scene using Supervision's `sv.CropAnnotator`.\n",
        "name": "Crop Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/crop_visualization@v1",
              "CropVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "position": {
            "anyOf": [
              {
                "enum": [
                  "CENTER",
                  "CENTER_LEFT",
                  "CENTER_RIGHT",
                  "TOP_CENTER",
                  "TOP_LEFT",
                  "TOP_RIGHT",
                  "BOTTOM_LEFT",
                  "BOTTOM_CENTER",
                  "BOTTOM_RIGHT",
                  "CENTER_OF_MASS"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "TOP_CENTER",
            "description": "The anchor position for placing the crop.",
            "examples": ["CENTER", "$inputs.position"],
            "title": "Position"
          },
          "scale_factor": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "Float value",
                    "docs": "\nExample:\n```\n1.3\n2.7\n```\n",
                    "name": "float"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 2,
            "description": "The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.",
            "examples": [2, "$inputs.scale_factor"],
            "title": "Scale Factor"
          },
          "border_thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 2,
            "description": "Thickness of the outline in pixels.",
            "examples": [2, "$inputs.border_thickness"],
            "title": "Border Thickness"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws scaled up crops of detections on the scene.",
        "title": "CropManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.crop.v1.CropVisualizationBlockV1",
      "human_friendly_block_name": "Crop Visualization",
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "manifest_type_identifier_aliases": ["CropVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `DotVisualization` block draws dots on an image at specific coordinates\nbased on provided detections using Supervision's `sv.DotAnnotator`.\n",
        "name": "Dot Visualization",
        "properties": {
          "type": {
            "enum": ["roboflow_core/dot_visualization@v1", "DotVisualization"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "position": {
            "anyOf": [
              {
                "enum": [
                  "CENTER",
                  "CENTER_LEFT",
                  "CENTER_RIGHT",
                  "TOP_CENTER",
                  "TOP_LEFT",
                  "TOP_RIGHT",
                  "BOTTOM_LEFT",
                  "BOTTOM_CENTER",
                  "BOTTOM_RIGHT",
                  "CENTER_OF_MASS"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CENTER",
            "description": "The anchor position for placing the dot.",
            "examples": ["CENTER", "$inputs.position"],
            "title": "Position"
          },
          "radius": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 4,
            "description": "Radius of the dot in pixels.",
            "examples": [4, "$inputs.radius"],
            "title": "Radius"
          },
          "outline_thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Thickness of the outline of the dot in pixels.",
            "examples": [2, "$inputs.outline_thickness"],
            "title": "Outline Thickness"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws dots on an image at specific coordinates based on provided detections.",
        "title": "DotManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.dot.v1.DotVisualizationBlockV1",
      "human_friendly_block_name": "Dot Visualization",
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "manifest_type_identifier_aliases": ["DotVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `EllipseVisualization` block draws ellipses that highlight detected\nobjects in an image using Supervision's `sv.EllipseAnnotator`.\n",
        "name": "Ellipse Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/ellipse_visualization@v1",
              "EllipseVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 2,
            "description": "Thickness of the lines in pixels.",
            "examples": [2, "$inputs.thickness"],
            "title": "Thickness"
          },
          "start_angle": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": -45,
            "description": "Starting angle of the ellipse in degrees.",
            "examples": [-45, "$inputs.start_angle"],
            "title": "Start Angle"
          },
          "end_angle": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 235,
            "description": "Ending angle of the ellipse in degrees.",
            "examples": [235, "$inputs.end_angle"],
            "title": "End Angle"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws ellipses that highlight detected objects in an image.",
        "title": "EllipseManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.ellipse.v1.EllipseVisualizationBlockV1",
      "human_friendly_block_name": "Ellipse Visualization",
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "manifest_type_identifier_aliases": ["EllipseVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `HaloVisualization` block uses a detected polygon\nfrom an instance segmentation to draw a halo using\n`sv.HaloAnnotator`.\n",
        "name": "Halo Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/halo_visualization@v1",
              "HaloVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.instance_segmentation_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "opacity": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.8,
            "description": "Transparency of the halo overlay.",
            "examples": [0.8, "$inputs.opacity"],
            "title": "Opacity"
          },
          "kernel_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 40,
            "description": "Size of the average pooling kernel used for creating the halo.",
            "examples": [40, "$inputs.kernel_size"],
            "title": "Kernel Size"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Paints a halo around detected objects in an image.",
        "title": "HaloManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.halo.v1.HaloVisualizationBlockV1",
      "human_friendly_block_name": "Halo Visualization",
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "manifest_type_identifier_aliases": ["HaloVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `LabelVisualization` block draws labels on an image at specific coordinates\nbased on provided detections using Supervision's `sv.LabelAnnotator`.\n",
        "name": "Label Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/label_visualization@v1",
              "LabelVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "text": {
            "anyOf": [
              {
                "enum": [
                  "Class",
                  "Confidence",
                  "Class and Confidence",
                  "Index",
                  "Dimensions",
                  "Area"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "Class",
            "description": "The type of text to display.",
            "examples": ["LABEL", "$inputs.text"],
            "title": "Text"
          },
          "text_position": {
            "anyOf": [
              {
                "enum": [
                  "CENTER",
                  "CENTER_LEFT",
                  "CENTER_RIGHT",
                  "TOP_CENTER",
                  "TOP_LEFT",
                  "TOP_RIGHT",
                  "BOTTOM_LEFT",
                  "BOTTOM_CENTER",
                  "BOTTOM_RIGHT",
                  "CENTER_OF_MASS"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "TOP_LEFT",
            "description": "The anchor position for placing the label.",
            "examples": ["CENTER", "$inputs.text_position"],
            "title": "Text Position"
          },
          "text_color": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "WHITE",
            "description": "Color of the text.",
            "examples": [
              "WHITE",
              "#FFFFFF",
              "rgb(255, 255, 255)$inputs.text_color"
            ],
            "title": "Text Color"
          },
          "text_scale": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "Float value",
                    "docs": "\nExample:\n```\n1.3\n2.7\n```\n",
                    "name": "float"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 1,
            "description": "Scale of the text.",
            "examples": [1, "$inputs.text_scale"],
            "title": "Text Scale"
          },
          "text_thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 1,
            "description": "Thickness of the text characters.",
            "examples": [1, "$inputs.text_thickness"],
            "title": "Text Thickness"
          },
          "text_padding": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Padding around the text in pixels.",
            "examples": [10, "$inputs.text_padding"],
            "title": "Text Padding"
          },
          "border_radius": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Radius of the label in pixels.",
            "examples": [0, "$inputs.border_radius"],
            "title": "Border Radius"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws labels on an image at specific coordinates based on provided detections.",
        "title": "LabelManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.label.v1.LabelVisualizationBlockV1",
      "human_friendly_block_name": "Label Visualization",
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "manifest_type_identifier_aliases": ["LabelVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `MaskVisualization` block uses a detected polygon\nfrom an instance segmentation to draw a mask using\n`sv.MaskAnnotator`.\n",
        "name": "Mask Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/mask_visualization@v1",
              "MaskVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.instance_segmentation_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "opacity": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.5,
            "description": "Transparency of the Mask overlay.",
            "examples": [0.5, "$inputs.opacity"],
            "title": "Opacity"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Paints a mask over detected objects in an image.",
        "title": "MaskManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.mask.v1.MaskVisualizationBlockV1",
      "human_friendly_block_name": "Mask Visualization",
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "manifest_type_identifier_aliases": ["MaskVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `PixelateVisualization` block pixelates detected\nobjects in an image using Supervision's `sv.PixelateAnnotator`.\n",
        "name": "Pixelate Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/pixelate_visualization@v1",
              "PixelateVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "pixel_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 20,
            "description": "Size of the pixelation.",
            "examples": [20, "$inputs.pixel_size"],
            "title": "Pixel Size"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Pixelates detected objects in an image.",
        "title": "PixelateManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.pixelate.v1.PixelateVisualizationBlockV1",
      "human_friendly_block_name": "Pixelate Visualization",
      "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
      "manifest_type_identifier_aliases": ["PixelateVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `PolygonVisualization` block uses a detections from an\ninstance segmentation to draw polygons around objects using\n`sv.PolygonAnnotator`.\n",
        "name": "Polygon Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/polygon_visualization@v1",
              "PolygonVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.instance_segmentation_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 2,
            "description": "Thickness of the outline in pixels.",
            "examples": [2, "$inputs.thickness"],
            "title": "Thickness"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws a polygon around detected objects in an image.",
        "title": "PolygonManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.polygon.v1.PolygonVisualizationBlockV1",
      "human_friendly_block_name": "Polygon Visualization",
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "manifest_type_identifier_aliases": ["PolygonVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "visualization",
        "license": "Apache-2.0",
        "long_description": "\nThe `TriangleVisualization` block draws triangle markers on an image at specific coordinates\nbased on provided detections using Supervision's `sv.TriangleAnnotator`.\n",
        "name": "Triangle Visualization",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/triangle_visualization@v1",
              "TriangleVisualization"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "copy_image": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
            "title": "Copy Image"
          },
          "color_palette": {
            "anyOf": [
              {
                "enum": [
                  "DEFAULT",
                  "CUSTOM",
                  "ROBOFLOW",
                  "Matplotlib Viridis",
                  "Matplotlib Plasma",
                  "Matplotlib Inferno",
                  "Matplotlib Magma",
                  "Matplotlib Cividis",
                  "Matplotlib Pastel1",
                  "Matplotlib Pastel2",
                  "Matplotlib Paired",
                  "Matplotlib Accent",
                  "Matplotlib Dark2",
                  "Matplotlib Set1",
                  "Matplotlib Set2",
                  "Matplotlib Set3",
                  "Matplotlib Tab10",
                  "Matplotlib Tab20",
                  "Matplotlib Tab20b",
                  "Matplotlib Tab20c"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "DEFAULT",
            "description": "Color palette to use for annotations.",
            "examples": ["DEFAULT", "$inputs.color_palette"],
            "title": "Color Palette"
          },
          "palette_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
            "examples": [10, "$inputs.palette_size"],
            "title": "Palette Size"
          },
          "custom_colors": {
            "anyOf": [
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": [],
            "description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
            "examples": [
              ["#FF0000", "#00FF00", "#0000FF"],
              "$inputs.custom_colors"
            ],
            "title": "Custom Colors"
          },
          "color_axis": {
            "anyOf": [
              {
                "enum": ["INDEX", "CLASS", "TRACK"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "CLASS",
            "description": "Strategy to use for mapping colors to annotations.",
            "examples": ["CLASS", "$inputs.color_axis"],
            "title": "Color Axis"
          },
          "position": {
            "anyOf": [
              {
                "enum": [
                  "CENTER",
                  "CENTER_LEFT",
                  "CENTER_RIGHT",
                  "TOP_CENTER",
                  "TOP_LEFT",
                  "TOP_RIGHT",
                  "BOTTOM_LEFT",
                  "BOTTOM_CENTER",
                  "BOTTOM_RIGHT",
                  "CENTER_OF_MASS"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "TOP_CENTER",
            "description": "The anchor position for placing the triangle.",
            "examples": ["CENTER", "$inputs.position"],
            "title": "Position"
          },
          "base": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Base width of the triangle in pixels.",
            "examples": [10, "$inputs.base"],
            "title": "Base"
          },
          "height": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 10,
            "description": "Height of the triangle in pixels.",
            "examples": [10, "$inputs.height"],
            "title": "Height"
          },
          "outline_thickness": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0,
            "description": "Thickness of the outline of the triangle in pixels.",
            "examples": [2, "$inputs.outline_thickness"],
            "title": "Outline Thickness"
          }
        },
        "required": ["type", "name", "predictions", "image"],
        "short_description": "Draws triangle markers on an image at specific coordinates based on provided detections.",
        "title": "TriangleManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.visualizations.triangle.v1.TriangleVisualizationBlockV1",
      "human_friendly_block_name": "Triangle Visualization",
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "manifest_type_identifier_aliases": ["TriangleVisualization"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "sink",
        "license": "Apache-2.0",
        "long_description": "\nBlock allows users to add custom metadata to each inference result in Roboflow Model Monitoring dashboard.\n\nThis is useful for adding information specific to your use case. For example, if you want to be able to\nfilter inferences by a specific label such as location, you can attach those labels to each inference with this block.\n\nFor more information on Model Monitoring at Roboflow, see https://docs.roboflow.com/deploy/model-monitoring.\n",
        "name": "Roboflow Custom Metadata",
        "properties": {
          "type": {
            "enum": [
              "roboflow_core/roboflow_custom_metadata@v1",
              "RoboflowCustomMetadata"
            ],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "predictions": {
            "description": "Reference data to extract property from",
            "examples": ["$steps.my_step.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              },
              {
                "description": "`'predictions'` key from Keypoint Detection Model output",
                "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[keypoint_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Classification Model outputs",
                "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[classification_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "field_value": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Batch of string values",
                    "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[string]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "This is the name of the metadata field you are creating",
            "examples": ["toronto", "pass", "fail"],
            "title": "Field Value"
          },
          "field_name": {
            "description": "Name of the field to be updated in Roboflow Customer Metadata",
            "examples": ["The name of the value of the field"],
            "title": "Field Name",
            "type": "string"
          },
          "fire_and_forget": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
            "title": "Fire And Forget"
          }
        },
        "required": [
          "type",
          "name",
          "predictions",
          "field_value",
          "field_name"
        ],
        "short_description": "Add custom metadata to Roboflow Model Monitoring dashboard",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "error_status",
          "kind": [
            {
              "name": "boolean",
              "description": "Boolean flag",
              "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
            }
          ]
        },
        {
          "name": "message",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.sinks.roboflow.custom_metadata.v1.RoboflowCustomMetadataBlockV1",
      "human_friendly_block_name": "Roboflow Custom Metadata",
      "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
      "manifest_type_identifier_aliases": ["RoboflowCustomMetadata"],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "fusion",
        "license": "Apache-2.0",
        "long_description": "\nThis block merges detections that were inferred for multiple sub-parts of the same input image\ninto single detection. \n\nBlock may be helpful in the following scenarios:\n* to apply [Slicing Adaptive Inference (SAHI)](https://ieeexplore.ieee.org/document/9897990) technique, \nas a final step of procedure, which involves Image Slicer block and model block at previous stages.\n* to merge together detections performed by precise, high-resolution model applied as secondary\nmodel after coarse detection is performed in the first stage and Dynamic Crop is applied later. \n",
        "name": "Detections Stitch",
        "properties": {
          "type": {
            "const": "roboflow_core/detections_stitch@v1",
            "enum": ["roboflow_core/detections_stitch@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "reference_image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "Image that was origin to take crops that yielded predictions.",
            "examples": ["$inputs.image"],
            "title": "Reference Image"
          },
          "predictions": {
            "description": "The output of a detection model describing the bounding boxes to be merged.",
            "examples": ["$steps.my_object_detection_model.predictions"],
            "kind": [
              {
                "description": "`'predictions'` key from Object Detection Model output",
                "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[object_detection_prediction]"
              },
              {
                "description": "`'predictions'` key from Instance Segmentation Model outputs",
                "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                "name": "Batch[instance_segmentation_prediction]"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Predictions",
            "type": "string"
          },
          "overlap_filtering_strategy": {
            "anyOf": [
              {
                "enum": ["none", "nms", "nmm"],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "nms",
            "description": "Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards surplus detections, NMM merges them.",
            "examples": ["nms", "$inputs.overlap_filtering_strategy"],
            "title": "Overlap Filtering Strategy"
          },
          "iou_threshold": {
            "anyOf": [
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.3,
            "description": "Parameter of overlap filtering strategy. If box intersection over union is above this  ratio, discard or merge the lower confidence box.",
            "examples": [0.4, "$inputs.iou_threshold"],
            "title": "Iou Threshold"
          }
        },
        "required": ["type", "name", "reference_image", "predictions"],
        "short_description": "Merges detections made against multiple pieces of input image into single detection.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            },
            {
              "name": "Batch[instance_segmentation_prediction]",
              "description": "`'predictions'` key from Instance Segmentation Model outputs",
              "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.fusion.detections_stitch.v1.DetectionsStitchBlockV1",
      "human_friendly_block_name": "Detections Stitch",
      "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {
        "predictions": 1
      },
      "dimensionality_reference_property": "reference_image",
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "transformation",
        "license": "Apache-2.0",
        "long_description": "\nThis block enables [Slicing Adaptive Inference (SAHI)](https://ieeexplore.ieee.org/document/9897990) technique in \nWorkflows providing implementation for first step of procedure - making slices out of input image.\n\nTo use the block effectively, it must be paired with detection model (object-detection or \ninstance segmentation) running against output images from this block. At the end - \nDetections Stitch block must be applied on top of predictions to merge them as if \nthe prediction was made against input image, not its slices.\n\nWe recommend adjusting the size of slices to match the model's input size and the scale of objects in the dataset \nthe model was trained on. Models generally perform best on data that is similar to what they encountered during \ntraining. The default size of slices is 640, but this might not be optimal if the model's input size is 320, as each \nslice would be downsized by a factor of two during inference. Similarly, if the model's input size is 1280, each slice \nwill be artificially up-scaled. The best setup should be determined experimentally based on the specific data and model \nyou are using.\n\nTo learn more about SAHI please visit [Roboflow blog](https://blog.roboflow.com/how-to-use-sahi-to-detect-small-objects/)\nwhich describes the technique in details, yet not in context of Roboflow workflows.\n",
        "name": "Image Slicer",
        "properties": {
          "type": {
            "const": "roboflow_core/image_slicer@v1",
            "enum": ["roboflow_core/image_slicer@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image to slice"
          },
          "slice_width": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 640,
            "description": "Width of each slice, in pixels",
            "examples": [320, "$inputs.slice_width"],
            "title": "Slice Width"
          },
          "slice_height": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 640,
            "description": "Height of each slice, in pixels",
            "examples": [320, "$inputs.slice_height"],
            "title": "Slice Height"
          },
          "overlap_ratio_width": {
            "anyOf": [
              {
                "exclusiveMaximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.2,
            "description": "Overlap ratio between consecutive slices in the width dimension",
            "examples": [0.2, "$inputs.overlap_ratio_width"],
            "title": "Overlap Ratio Width"
          },
          "overlap_ratio_height": {
            "anyOf": [
              {
                "exclusiveMaximum": 1,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.2,
            "description": "Overlap ratio between consecutive slices in the height dimension",
            "examples": [0.2, "$inputs.overlap_ratio_height"],
            "title": "Overlap Ratio Height"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Splits input image into series of smaller images to perform accurate prediction.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "slices",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.transformations.image_slicer.v1.ImageSlicerBlockV1",
      "human_friendly_block_name": "Image Slicer",
      "manifest_type_identifier": "roboflow_core/image_slicer@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 1
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nExtract the dominant color from an input image using K-means clustering.\n\nThis block identifies the most prevalent color in an image.\nProcessing time is dependant on color complexity and image size.\nMost images should complete in under half a second.\n\nThe output is a list of RGB values representing the dominant color, making it easy \nto use in further processing or visualization tasks.\n\nNote: The block operates on the assumption that the input image is in RGB format. \n",
        "name": "Dominant Color",
        "properties": {
          "type": {
            "const": "roboflow_core/dominant_color@v1",
            "enum": ["roboflow_core/dominant_color@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "color_clusters": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 4,
            "description": "Number of dominant colors to identify. Higher values increase precision but may slow processing.",
            "examples": [4, "$inputs.color_clusters"],
            "gt": 0,
            "le": 10,
            "title": "Color Clusters"
          },
          "max_iterations": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 100,
            "description": "Max number of iterations to perform. Higher values increase precision but may slow processing.",
            "examples": [100, "$inputs.max_iterations"],
            "gt": 0,
            "le": 500,
            "title": "Max Iterations"
          },
          "target_size": {
            "anyOf": [
              {
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 100,
            "description": "Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision.",
            "examples": [100, "$inputs.target_size"],
            "gt": 0,
            "le": 250,
            "title": "Target Size"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Get the dominant color of an image in RGB format.",
        "title": "DominantColorManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "rgb_color",
          "kind": [
            {
              "name": "rgb_color",
              "description": "RGB color",
              "docs": "\nThis kind represents RGB color as a tuple (R, G, B).\n\nExamples:\n```\n(128, 32, 64)\n(255, 255, 255)\n```\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.dominant_color.v1.DominantColorBlockV1",
      "human_friendly_block_name": "Dominant Color",
      "manifest_type_identifier": "roboflow_core/dominant_color@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "Count the number of pixels that match a specific color within a given tolerance.",
        "name": "Pixel Color Count",
        "properties": {
          "type": {
            "const": "roboflow_core/pixel_color_count@v1",
            "enum": ["roboflow_core/pixel_color_count@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "target_color": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "RGB color",
                    "docs": "\nThis kind represents RGB color as a tuple (R, G, B).\n\nExamples:\n```\n(128, 32, 64)\n(255, 255, 255)\n```\n",
                    "name": "rgb_color"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              },
              {
                "type": "string"
              },
              {
                "maxItems": 3,
                "minItems": 3,
                "prefixItems": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "integer"
                  },
                  {
                    "type": "integer"
                  }
                ],
                "type": "array"
              }
            ],
            "description": "Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).",
            "examples": ["#431112", "$inputs.target_color", [18, 17, 67]],
            "title": "Target Color"
          },
          "tolerance": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "integer"
              }
            ],
            "default": 10,
            "description": "Tolerance for color matching.",
            "examples": [10, "$inputs.tolerance"],
            "title": "Tolerance"
          }
        },
        "required": ["type", "name", "image", "target_color"],
        "short_description": "Count the number of pixels that match a specific color within a given tolerance.",
        "title": "ColorPixelCountManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "matching_pixels_count",
          "kind": [
            {
              "name": "Batch[integer]",
              "description": "Batch of integer values",
              "docs": "\nThis kind represents batch of integer values.\n\nExamples:\n```\n[1, 2, 6]\n[9, 4]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.pixel_color_count.v1.PixelationCountBlockV1",
      "human_friendly_block_name": "Pixel Color Count",
      "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nCompare SIFT descriptors from multiple images using FLANN-based matcher.\n\nThis block is useful for determining if multiple images match based on their SIFT descriptors.\n",
        "name": "SIFT Comparison",
        "properties": {
          "type": {
            "const": "roboflow_core/sift_comparison@v1",
            "enum": ["roboflow_core/sift_comparison@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "descriptor_1": {
            "description": "Reference to SIFT descriptors from the first image to compare",
            "examples": ["$steps.sift.descriptors"],
            "kind": [
              {
                "description": "Numpy array",
                "docs": null,
                "name": "numpy_array"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Descriptor 1",
            "type": "string"
          },
          "descriptor_2": {
            "description": "Reference to SIFT descriptors from the second image to compare",
            "examples": ["$steps.sift.descriptors"],
            "kind": [
              {
                "description": "Numpy array",
                "docs": null,
                "name": "numpy_array"
              }
            ],
            "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
            "reference": true,
            "selected_element": "step_output",
            "title": "Descriptor 2",
            "type": "string"
          },
          "good_matches_threshold": {
            "anyOf": [
              {
                "exclusiveMinimum": 0,
                "type": "integer"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 50,
            "description": "Threshold for the number of good matches to consider the images as matching",
            "examples": [50, "$inputs.good_matches_threshold"],
            "title": "Good Matches Threshold"
          },
          "ratio_threshold": {
            "anyOf": [
              {
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": 0.7,
            "description": "Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.",
            "examples": [0.7, "$inputs.ratio_threshold"],
            "title": "Ratio Threshold"
          }
        },
        "required": ["type", "name", "descriptor_1", "descriptor_2"],
        "short_description": "Compare SIFT descriptors from multiple images.",
        "title": "SIFTComparisonBlockManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "good_matches_count",
          "kind": [
            {
              "name": "integer",
              "description": "Integer value",
              "docs": "\nExamples:\n```\n1\n2\n```\n"
            }
          ]
        },
        {
          "name": "images_match",
          "kind": [
            {
              "name": "boolean",
              "description": "Boolean flag",
              "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.sift_comparison.v1.SIFTComparisonBlockV1",
      "human_friendly_block_name": "SIFT Comparison",
      "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nThe Scale-Invariant Feature Transform (SIFT) algorithm is a popular method in computer vision for detecting \nand describing features (interesting parts) in images. SIFT is used to find key points in an image and \ndescribe them in a way that allows for recognizing the same objects or features in different images, \neven if the images are taken from different angles, distances, or lighting conditions.\n\nRead more: https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\n",
        "name": "SIFT",
        "properties": {
          "type": {
            "const": "roboflow_core/sift@v1",
            "enum": ["roboflow_core/sift@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Apply SIFT to an image.",
        "title": "SIFTDetectionManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "keypoints",
          "kind": [
            {
              "name": "image_keypoints",
              "description": "Image keypoints detected by classical Computer Vision method",
              "docs": "\nThe kind represents image keypoints that are detected by classical Computer Vision methods.\nUnderlying representation is serialised OpenCV KeyPoint object.\n\nExamples:\n```\n{\n    \"pt\": (2.429290294647217, 1197.7939453125),\n    \"size\": 1.9633429050445557,\n    \"angle\": 183.4322509765625,\n    \"response\": 0.03325376659631729,\n    \"octave\": 6423039,\n    \"class_id\": -1\n}\n``` \n"
            }
          ]
        },
        {
          "name": "descriptors",
          "kind": [
            {
              "name": "numpy_array",
              "description": "Numpy array",
              "docs": null
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.sift.v1.SIFTBlockV1",
      "human_friendly_block_name": "SIFT",
      "manifest_type_identifier": "roboflow_core/sift@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nApply Template Matching to an image. Block is based on OpenCV library function called `cv2.matchTemplate(...)`\nthat searches for a template image within a larger image. This is often used in computer vision tasks where \nyou need to find a specific object or pattern in a scene, like detecting logos, objects, or \nspecific regions in an image.\n\nPlease take into account the following characteristics of block:\n* it tends to produce overlapping and duplicated predictions, hence we added NMS which can be disabled\n* block may find very large number of matches in some cases due to simplicity of methods being used - \nin that cases NMS may be computationally intractable and should be disabled\n\nOutput from the block is in a form of sv.Detections objects which can be nicely paired with other blocks\naccepting this kind of input (like visualization blocks).\n",
        "name": "Template Matching",
        "properties": {
          "type": {
            "const": "roboflow_core/template_matching@v1",
            "enum": ["roboflow_core/template_matching@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "template": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The template image for this step.",
            "examples": ["$inputs.template", "$steps.cropping.template"],
            "title": "Template Image"
          },
          "matching_threshold": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Float value",
                    "docs": "\nExample:\n```\n1.3\n2.7\n```\n",
                    "name": "float"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "number"
              }
            ],
            "default": 0.8,
            "description": "The threshold value for template matching.",
            "title": "Matching Threshold"
          },
          "apply_nms": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "boolean"
              }
            ],
            "default": true,
            "description": "Flag to decide if NMS should be applied at the output detections.",
            "title": "Apply NMS"
          },
          "nms_threshold": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "`float` value in range `[0.0, 1.0]`",
                    "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n",
                    "name": "float_zero_to_one"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "maximum": 1,
                "minimum": 0,
                "type": "number"
              }
            ],
            "default": 0.5,
            "description": "The threshold value NMS procedure (if to be applied).",
            "title": "NMS threshold"
          }
        },
        "required": ["type", "name", "image", "template"],
        "short_description": "Looks for instances of template in specific image",
        "title": "TemplateMatchingManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "predictions",
          "kind": [
            {
              "name": "Batch[object_detection_prediction]",
              "description": "`'predictions'` key from Object Detection Model output",
              "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "number_of_matches",
          "kind": [
            {
              "name": "Batch[integer]",
              "description": "Batch of integer values",
              "docs": "\nThis kind represents batch of integer values.\n\nExamples:\n```\n[1, 2, 6]\n[9, 4]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.template_matching.v1.TemplateMatchingBlockV1",
      "human_friendly_block_name": "Template Matching",
      "manifest_type_identifier": "roboflow_core/template_matching@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nApply a blur to an image. \nThe blur type and kernel size can be specified.\n",
        "name": "Image Blur",
        "properties": {
          "type": {
            "const": "roboflow_core/image_blur@v1",
            "enum": ["roboflow_core/image_blur@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "blur_type": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": ["average", "gaussian", "median", "bilateral"],
                "type": "string"
              }
            ],
            "default": "gaussian",
            "description": "Type of Blur to perform on image.",
            "examples": ["average", "$inputs.blur_type"],
            "title": "Blur Type"
          },
          "kernel_size": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "integer"
              }
            ],
            "default": 5,
            "description": "Size of the average pooling kernel used for blurring.",
            "examples": [5, "$inputs.kernel_size"],
            "title": "Kernel Size"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Apply a blur to an image.",
        "title": "ImageBlurManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.image_blur.v1.ImageBlurBlockV1",
      "human_friendly_block_name": "Image Blur",
      "manifest_type_identifier": "roboflow_core/image_blur@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nBlock to convert an RGB image to grayscale. The output image will have only one channel.\n",
        "name": "Image Convert Grayscale",
        "properties": {
          "type": {
            "const": "roboflow_core/convert_grayscale@v1",
            "enum": ["roboflow_core/convert_grayscale@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Convert an RGB image to grayscale.",
        "title": "ConvertGrayscaleManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.convert_grayscale.v1.ConvertGrayscaleBlockV1",
      "human_friendly_block_name": "Image Convert Grayscale",
      "manifest_type_identifier": "roboflow_core/convert_grayscale@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nApply a threshold to an image. The image must be in grayscale.\n",
        "name": "Image Threshold",
        "properties": {
          "type": {
            "const": "roboflow_core/threshold@v1",
            "enum": ["roboflow_core/threshold@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "threshold_type": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "enum": [
                  "binary",
                  "binary_inv",
                  "trunc",
                  "tozero",
                  "tozero_inv",
                  "adaptive_mean",
                  "adaptive_gaussian",
                  "otsu"
                ],
                "type": "string"
              }
            ],
            "default": "binary",
            "description": "Type of Edge Detection to perform.",
            "examples": ["binary", "$inputs.threshold_type"],
            "title": "Threshold Type"
          },
          "thresh_value": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "integer"
              }
            ],
            "description": "Threshold value.",
            "examples": [127, "$inputs.thresh_value"],
            "title": "Thresh Value"
          },
          "max_value": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "integer"
              }
            ],
            "default": 255,
            "description": "Maximum value for thresholding",
            "examples": [255, "$inputs.max_value"],
            "title": "Max Value"
          }
        },
        "required": ["type", "name", "image", "thresh_value"],
        "short_description": "Apply a threshold to an image.",
        "title": "ImageThresholdManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.threshold.v1.ImageThresholdBlockV1",
      "human_friendly_block_name": "Image Threshold",
      "manifest_type_identifier": "roboflow_core/threshold@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nFinds the contours in an image. It returns the contours and number of contours. The input image should be thresholded before using this block.\n",
        "name": "Image Contours",
        "properties": {
          "type": {
            "const": "roboflow_core/contours_detection@v1",
            "enum": ["roboflow_core/contours_detection@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          },
          "line_thickness": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Integer value",
                    "docs": "\nExamples:\n```\n1\n2\n```\n",
                    "name": "integer"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "integer"
              }
            ],
            "default": 3,
            "description": "Line thickness for drawing contours.",
            "examples": [3, "$inputs.line_thickness"],
            "title": "Line Thickness"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Find and count the contours on an image.",
        "title": "ImageContoursDetectionManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "contours",
          "kind": [
            {
              "name": "contours",
              "description": "List of numpy arrays where each array represents contour points",
              "docs": "\nThis kind represents a value of a list of numpy arrays where each array represents contour points.\n\nExample:\n```\n[\n    np.array([[10, 10],\n              [20, 20],\n              [30, 30]], dtype=np.int32),\n    np.array([[50, 50],\n              [60, 60],\n              [70, 70]], dtype=np.int32)\n]\n```\n"
            }
          ]
        },
        {
          "name": "hierarchy",
          "kind": [
            {
              "name": "numpy_array",
              "description": "Numpy array",
              "docs": null
            }
          ]
        },
        {
          "name": "number_contours",
          "kind": [
            {
              "name": "integer",
              "description": "Integer value",
              "docs": "\nExamples:\n```\n1\n2\n```\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.contours.v1.ImageContoursDetectionBlockV1",
      "human_friendly_block_name": "Image Contours",
      "manifest_type_identifier": "roboflow_core/contours_detection@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "model",
        "license": "Apache-2.0",
        "long_description": "\nUse the OpenAI CLIP zero-shot classification model to classify images.\n\nThis block accepts an image and a list of text prompts. The block then returns the \nsimilarity of each text label to the provided image.\n\nThis block is useful for classifying images without having to train a fine-tuned \nclassification model. For example, you could use CLIP to classify the type of vehicle \nin an image, or if an image contains NSFW material.\n",
        "name": "Clip Comparison",
        "properties": {
          "type": {
            "const": "roboflow_core/clip_comparison@v2",
            "enum": ["roboflow_core/clip_comparison@v2"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "Unique name of step in workflows",
            "title": "Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "classes": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "List of values of any types",
                    "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n",
                    "name": "list_of_values"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              }
            ],
            "description": "List of classes to calculate similarity against each input image",
            "examples": [["a", "b", "c"], "$inputs.texts"],
            "minLength": 1,
            "title": "Classes"
          },
          "version": {
            "anyOf": [
              {
                "enum": [
                  "RN101",
                  "RN50",
                  "RN50x16",
                  "RN50x4",
                  "RN50x64",
                  "ViT-B-16",
                  "ViT-B-32",
                  "ViT-L-14-336px",
                  "ViT-L-14"
                ],
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "ViT-B-16",
            "description": "Variant of CLIP model",
            "examples": ["ViT-B-16", "$inputs.variant"],
            "title": "Version"
          }
        },
        "required": ["type", "name", "images", "classes"],
        "short_description": "Compare CLIP image and text embeddings.",
        "title": "BlockManifest",
        "type": "object",
        "version": "v2"
      },
      "outputs_manifest": [
        {
          "name": "similarities",
          "kind": [
            {
              "name": "list_of_values",
              "description": "List of values of any types",
              "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
            }
          ]
        },
        {
          "name": "max_similarity",
          "kind": [
            {
              "name": "float_zero_to_one",
              "description": "`float` value in range `[0.0, 1.0]`",
              "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
            }
          ]
        },
        {
          "name": "most_similar_class",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "min_similarity",
          "kind": [
            {
              "name": "float_zero_to_one",
              "description": "`float` value in range `[0.0, 1.0]`",
              "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
            }
          ]
        },
        {
          "name": "least_similar_class",
          "kind": [
            {
              "name": "string",
              "description": "String value",
              "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
            }
          ]
        },
        {
          "name": "classification_predictions",
          "kind": [
            {
              "name": "Batch[classification_prediction]",
              "description": "`'predictions'` key from Classification Model outputs",
              "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "root_parent_id",
          "kind": [
            {
              "name": "Batch[parent_id]",
              "description": "Identifier of parent for step output",
              "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.models.foundation.clip_comparison.v2.ClipComparisonBlockV2",
      "human_friendly_block_name": "Clip Comparison",
      "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "classical_computer_vision",
        "license": "Apache-2.0",
        "long_description": "\nThis block calculate the Brenner function score which is a measure of the texture in the image. \nAn in-focus image has a high Brenner function score, and contains texture at a smaller scale than\n an out-of-focus image. Conversely, an out-of-focus image has a low Brenner function score, and \n does not contain small-scale texture.\n",
        "name": "Camera Focus",
        "properties": {
          "type": {
            "const": "roboflow_core/camera_focus@v1",
            "enum": ["roboflow_core/camera_focus@v1"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "image": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The input image for this step.",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Input Image"
          }
        },
        "required": ["type", "name", "image"],
        "short_description": "Helps focus a camera by providing a focus measure.",
        "title": "CameraFocusManifest",
        "type": "object",
        "version": "v1"
      },
      "outputs_manifest": [
        {
          "name": "image",
          "kind": [
            {
              "name": "Batch[image]",
              "description": "Image in workflows",
              "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "focus_measure",
          "kind": [
            {
              "name": "float",
              "description": "Float value",
              "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.classical_cv.camera_focus.v1.CameraFocusBlockV1",
      "human_friendly_block_name": "Camera Focus",
      "manifest_type_identifier": "roboflow_core/camera_focus@v1",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    },
    {
      "block_schema": {
        "additionalProperties": true,
        "block_type": "sink",
        "license": "Apache-2.0",
        "long_description": "\nBlock let users save their images and predictions into Roboflow Dataset. Persisting data from\nproduction environments helps iteratively building more robust models. \n\nBlock provides configuration options to decide how data should be stored and what are the limits \nto be applied. We advice using this block in combination with rate limiter blocks to effectively \ncollect data that the model struggle with.\n",
        "name": "Roboflow Dataset Upload",
        "properties": {
          "type": {
            "const": "roboflow_core/roboflow_dataset_upload@v2",
            "enum": ["roboflow_core/roboflow_dataset_upload@v2"],
            "title": "Type",
            "type": "string"
          },
          "name": {
            "description": "The unique name of this step.",
            "title": "Step Name",
            "type": "string"
          },
          "images": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_image",
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "Image in workflows",
                    "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[image]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              }
            ],
            "description": "The image to infer on",
            "examples": ["$inputs.image", "$steps.cropping.crops"],
            "title": "Image"
          },
          "predictions": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "`'predictions'` key from Object Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[object_detection_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Instance Segmentation Model outputs",
                    "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[instance_segmentation_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Keypoint Detection Model output",
                    "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[keypoint_detection_prediction]"
                  },
                  {
                    "description": "`'predictions'` key from Classification Model outputs",
                    "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n",
                    "name": "Batch[classification_prediction]"
                  }
                ],
                "pattern": "^\\$steps\\.[A-Za-z_\\-0-9]+\\.[A-Za-z_*0-9\\-]+$",
                "reference": true,
                "selected_element": "step_output",
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Reference q detection-like predictions",
            "examples": ["$steps.object_detection_model.predictions"],
            "title": "Predictions"
          },
          "target_project": {
            "anyOf": [
              {
                "kind": [
                  {
                    "description": "Roboflow project name",
                    "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n",
                    "name": "roboflow_project"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              },
              {
                "type": "string"
              }
            ],
            "description": "name of Roboflow dataset / project to be used as target for collected data",
            "examples": ["my_dataset", "$inputs.target_al_dataset"],
            "title": "Target Project"
          },
          "usage_quota_name": {
            "description": "Unique name for Roboflow project pointed by `target_project` parameter, that identifies usage quota applied for this block.",
            "examples": ["quota-for-data-sampling-1"],
            "title": "Usage Quota Name",
            "type": "string"
          },
          "data_percentage": {
            "anyOf": [
              {
                "maximum": 100,
                "minimum": 0,
                "type": "number"
              },
              {
                "kind": [
                  {
                    "description": "Float value",
                    "docs": "\nExample:\n```\n1.3\n2.7\n```\n",
                    "name": "float"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "description": "Percent of data that will be saved (in range [0.0, 100.0])",
            "examples": [true, false, "$inputs.persist_predictions"],
            "title": "Data Percentage"
          },
          "persist_predictions": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Boolean flag to decide if predictions should be registered along with images",
            "examples": [true, false, "$inputs.persist_predictions"],
            "title": "Persist Predictions"
          },
          "minutely_usage_limit": {
            "default": 10,
            "description": "Maximum number of data registration requests per minute accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
            "examples": [10, 60],
            "title": "Minutely Usage Limit",
            "type": "integer"
          },
          "hourly_usage_limit": {
            "default": 100,
            "description": "Maximum number of data registration requests per hour accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
            "examples": [10, 60],
            "title": "Hourly Usage Limit",
            "type": "integer"
          },
          "daily_usage_limit": {
            "default": 1000,
            "description": "Maximum number of data registration requests per day accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
            "examples": [10, 60],
            "title": "Daily Usage Limit",
            "type": "integer"
          },
          "max_image_size": {
            "default": [1920, 1080],
            "description": "Maximum size of the image to be registered - bigger images will be downsized preserving aspect ratio. Format of data: `(width, height)`",
            "examples": [
              [1920, 1080],
              [512, 512]
            ],
            "maxItems": 2,
            "minItems": 2,
            "prefixItems": [
              {
                "type": "integer"
              },
              {
                "type": "integer"
              }
            ],
            "title": "Max Image Size",
            "type": "array"
          },
          "compression_level": {
            "default": 95,
            "description": "Compression level for images registered",
            "examples": [95, 75],
            "exclusiveMinimum": 0,
            "maximum": 100,
            "title": "Compression Level",
            "type": "integer"
          },
          "registration_tags": {
            "description": "Tags to be attached to registered datapoints",
            "examples": [
              ["location-florida", "factory-name", "$inputs.dynamic_tag"]
            ],
            "items": {
              "anyOf": [
                {
                  "kind": [
                    {
                      "description": "String value",
                      "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                      "name": "string"
                    }
                  ],
                  "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                  "reference": true,
                  "selected_element": "workflow_parameter",
                  "type": "string"
                },
                {
                  "type": "string"
                }
              ]
            },
            "title": "Registration Tags",
            "type": "array"
          },
          "disable_sink": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": false,
            "description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
            "examples": [true, "$inputs.disable_active_learning"],
            "title": "Disable Sink"
          },
          "fire_and_forget": {
            "anyOf": [
              {
                "type": "boolean"
              },
              {
                "kind": [
                  {
                    "description": "Boolean flag",
                    "docs": "\nThis kind represents single boolean value - `True` or `False`\n",
                    "name": "boolean"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": true,
            "description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
            "title": "Fire And Forget"
          },
          "labeling_batch_prefix": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "kind": [
                  {
                    "description": "String value",
                    "docs": "\nExamples:\n```\n\"my string value\"\n```\n",
                    "name": "string"
                  }
                ],
                "pattern": "^\\$inputs.[A-Za-z_0-9\\-]+$",
                "reference": true,
                "selected_element": "workflow_parameter",
                "type": "string"
              }
            ],
            "default": "workflows_data_collector",
            "description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
            "examples": ["my_labeling_batch_name"],
            "title": "Labeling Batch Prefix"
          },
          "labeling_batches_recreation_frequency": {
            "default": "never",
            "description": "Frequency in which new labeling batches are created in Roboflow app. New batches are created with name prefix provided in `labeling_batch_prefix` in given time intervals.Useful in organising labeling flow.",
            "enum": ["never", "daily", "weekly", "monthly"],
            "examples": ["never", "daily"],
            "title": "Labeling Batches Recreation Frequency",
            "type": "string"
          }
        },
        "required": [
          "type",
          "name",
          "images",
          "target_project",
          "usage_quota_name",
          "data_percentage"
        ],
        "short_description": "Save images and predictions in your Roboflow Dataset",
        "title": "BlockManifest",
        "type": "object",
        "version": "v2"
      },
      "outputs_manifest": [
        {
          "name": "error_status",
          "kind": [
            {
              "name": "Batch[boolean]",
              "description": "Boolean values batch",
              "docs": "\nThis kind represents batch of boolean values. \n\nExamples:\n```\n[True, False, False, True]\n[True, True]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        },
        {
          "name": "message",
          "kind": [
            {
              "name": "Batch[string]",
              "description": "Batch of string values",
              "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
            }
          ]
        }
      ],
      "block_source": "workflows_core",
      "fully_qualified_block_class_name": "inference.core.workflows.core_steps.sinks.roboflow.dataset_upload.v2.RoboflowDatasetUploadBlockV2",
      "human_friendly_block_name": "Roboflow Dataset Upload",
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "manifest_type_identifier_aliases": [],
      "execution_engine_compatibility": ">=1.0.0,<2.0.0",
      "input_dimensionality_offsets": {},
      "dimensionality_reference_property": null,
      "output_dimensionality_offset": 0
    }
  ],
  "declared_kinds": [
    {
      "name": "Batch[instance_segmentation_prediction]",
      "description": "`'predictions'` key from Instance Segmentation Model outputs",
      "docs": "\nThis kind represents batch of predictions from Instance Segmentation Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points \nproviding object contour,\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"points\": [{\"x\": 300, \"y\": 200}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "detection",
      "description": "Single element of detections-based prediction (like `object_detection_prediction`)",
      "docs": "\nThis kind represents single detection in prediction from a model that detects multiple elements\n(like object detection or instance segmentation model). It is represented as a tuple\nthat is created from `sv.Detections(...)` object while iterating over its content. `workflows`\nutilises `data` property of `sv.Detections(...)` to keep additional metadata which will be available\nin the tuple. Some properties may not always be present. Take a look at documentation of \n`object_detection_prediction`, `instance_segmentation_prediction`, `keypoint_detection_prediction`\nkinds to discover which additional metadata are available.\n\nMore technical details about \n[iterating over `sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/#supervision.detection.core.Detections)\n"
    },
    {
      "name": "*",
      "description": "Equivalent of any element",
      "docs": "\nThis is a special kind that represents Any value - which is to be used by default if \nkind annotation is not specified. It will not tell the compiler what is to be expected\nas a specific value in runtime, but at the same time, it makes it possible to run the \nworkflow when we do not know or do not care about types. \n\n**Important note:** Usage of this kind reduces execution engine capabilities to predict \nproblems with workflow and make those problems to be visible while running the workflow.\n"
    },
    {
      "name": "boolean",
      "description": "Boolean flag",
      "docs": "\nThis kind represents single boolean value - `True` or `False`\n"
    },
    {
      "name": "rgb_color",
      "description": "RGB color",
      "docs": "\nThis kind represents RGB color as a tuple (R, G, B).\n\nExamples:\n```\n(128, 32, 64)\n(255, 255, 255)\n```\n"
    },
    {
      "name": "instance_segmentation_prediction",
      "description": "Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object",
      "docs": "\nThis kind represents single instance segmentation prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={'class_name': array(['G'], dtype='<U1')}\n)\n```\n"
    },
    {
      "name": "dictionary",
      "description": "Dictionary",
      "docs": null
    },
    {
      "name": "Batch[image_metadata]",
      "description": "Dictionary with image metadata required by supervision",
      "docs": "\nThis kind represent batch of prediction metadata providing information about the image that prediction was made against.\n\nExamples:\n```\n[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "object_detection_prediction",
      "description": "Prediction with detected bounding boxes in form of sv.Detections(...) object",
      "docs": "\nThis kind represents single object detection prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={'class_name': array(['car', 'truck', 'car', 'car'], dtype='<U13')}\n)\n```\n"
    },
    {
      "name": "roboflow_api_key",
      "description": "Roboflow API key",
      "docs": "\nThis kind represents API key that grants access to Roboflow platform.\nTo learn more about Roboflow API keys visit [this](https://docs.roboflow.com/api-reference/authentication) \npage.\n"
    },
    {
      "name": "Batch[string]",
      "description": "Batch of string values",
      "docs": "\nThis kind represents batch of string values.\n\nExamples:\n```\n[\"a\", \"b\", \"c\"]\n[\"d\", \"e\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "Batch[top_class]",
      "description": "Batch of string values representing top class predicted by classification model",
      "docs": "\nThe kind represent top classes predicted by classification model - representing its predictions on batch of images.\n\nExample:\n```\n[\"car\", \"dog\", \"car\", \"cat\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "keypoint_detection_prediction",
      "description": "Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object",
      "docs": "\nThis kind represents single keypoints prediction in form of \n[`sv.Detections(...)`](https://supervision.roboflow.com/latest/detection/core/) object.\n\nExample:\n```\nsv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='<U1'),\n        # TODO: put details here\n    }\n)\n```\n"
    },
    {
      "name": "Batch[image]",
      "description": "Image in workflows",
      "docs": "\nThis is the representation of image batch in `workflows`. The value behind this kind \nis Python list of dictionaries. Each of this dictionary is native `inference` image with\nthe following keys defined:\n```python\n{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`\n}\n```\nThis format makes it possible to use [inference image utils](https://inference.roboflow.com/docs/reference/inference/core/utils/image_utils/)\nto operate on the images. \n\nSome blocks that output images may add additional fields - like \"parent_id\", which should\nnot be modified but may be used is specific contexts - for instance when\none needs to tag predictions with identifier of parent image.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "Batch[bar_code_detection]",
      "description": "Prediction with barcode detection",
      "docs": "\nThis kind represents batch of predictions regarding barcodes location and data their provide.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected barcodes (detections) and their metadata\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"barcode\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "float",
      "description": "Float value",
      "docs": "\nExample:\n```\n1.3\n2.7\n```\n"
    },
    {
      "name": "point",
      "description": "Single point in 2D",
      "docs": null
    },
    {
      "name": "Batch[boolean]",
      "description": "Boolean values batch",
      "docs": "\nThis kind represents batch of boolean values. \n\nExamples:\n```\n[True, False, False, True]\n[True, True]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "Batch[object_detection_prediction]",
      "description": "`'predictions'` key from Object Detection Model output",
      "docs": "\nThis kind represents batch of predictions from an Object Detection Model.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections)\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\"}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "roboflow_model_id",
      "description": "Roboflow model id",
      "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, models are\nidentified with special strings in the format: `<project_name>/<version>`. You should\nexpect this value to be provided once this kind is used. In some special cases, Roboflow \nplatform accepts model alias as model id which will not conform provided schema. List\nof aliases can be found [here](https://inference.roboflow.com/quickstart/aliases/).  \n"
    },
    {
      "name": "float_zero_to_one",
      "description": "`float` value in range `[0.0, 1.0]`",
      "docs": "\nThis kind represents float value from 0.0 to 1.0. \n\nExamples:\n```\n0.1\n0.4\n0.999\n```\n"
    },
    {
      "name": "Batch[keypoint_detection_prediction]",
      "description": "`'predictions'` key from Keypoint Detection Model output",
      "docs": "\nThis kind represents batch of predictions from Keypoint Detection Models.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected objects (detections) and list of points of \nobject skeleton. \n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}]},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}},\n        {\"x\": 600, \"y\": 900, \"width\": 100, \"height\" 50, \"confidence\": 0.3, \"class\": \"car\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"keypoints\": [{\"x\": 300, \"y\": 200, \"confidence\": 0.3, \"class_id\": 0, \"class_name\": \"tire_center\"}}\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "list_of_values",
      "description": "List of values of any types",
      "docs": "\nThis kind represents Python list of Any values.\n\nExamples:\n```\n[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n```\n"
    },
    {
      "name": "roboflow_project",
      "description": "Roboflow project name",
      "docs": "\nThis kind represents value specific for Roboflow platform. At the platform, each project has \nunique name and the value behind this kind represent this name. To learn more \non how to kick-off with Roboflow project - visit [this](https://blog.roboflow.com/getting-started-with-roboflow/) page. \n"
    },
    {
      "name": "Batch[serialised_payloads]",
      "description": "List of serialised elements that can be registered in the sink",
      "docs": "\nThis value represents list of serialised values. Each serialised value is either\nstring or bytes - if something else is provided - it will be attempted to be serialised \nto JSON.\n\nExamples:\n```\n[\"some\", b\"other\", {\"my\": \"dictionary\"}]\n```\n\nThis kind is to be used in combination with sinks blocks and serializers blocks.\nSerializer should output value of this kind which shall then be accepted by sink.\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "Batch[prediction_type]",
      "description": "String value with type of prediction",
      "docs": "\nThis kind represent batch of prediction metadata providing information about the type of prediction.\n\nExamples:\n```\n[\"object-detection\", \"object-detection\"]\n[\"instance-segmentation\", \"instance-segmentation\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "Batch[parent_id]",
      "description": "Identifier of parent for step output",
      "docs": "\nThis kind represent batch of prediction metadata providing information about the context of prediction.\nFor example - whenever there is a workflow with multiple models - such that first model detect objects \nand then other models make their predictions based on crops from first model detections - `parent_id`\nhelps to figure out which detection of the first model is associated to which downstream predictions.\n\nExamples:\n```\n[\"uuid-1\", \"uuid-1\", \"uuid-2\", \"uuid-2\"]\n[\"uuid-1\", \"uuid-1\", \"uuid-1\", \"uuid-1\"]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "Batch[classification_prediction]",
      "description": "`'predictions'` key from Classification Model outputs",
      "docs": "\nThis kind represent predictions from Classification Models.\n\nExamples:\n```\n# in case of multi-class classification\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.3}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.7},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.7}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.3},\n]\n[\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.1}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.9},\n    {\"class\": \"A\", \"class_id\": 0, \"confidence\": 0.9}, {\"class\": \"B\", \"class_id\": 1, \"confidence\": 0.1},\n]\n\n# in case of multi-label classification\n[\n    {\n        \"class_a\": 0.3,\n        \"class_b\": 0.4,\n    },\n    {\n        \"class_c\": 0.3,\n        \"class_b\": 0.4,\n    }\n]\n[\n    {\n        \"car\": 0.4,\n        \"truck\": 0.5,\n    },\n    {\n        \"truck\": 0.6,\n        \"bike\": 0.4,\n    }\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "image_keypoints",
      "description": "Image keypoints detected by classical Computer Vision method",
      "docs": "\nThe kind represents image keypoints that are detected by classical Computer Vision methods.\nUnderlying representation is serialised OpenCV KeyPoint object.\n\nExamples:\n```\n{\n    \"pt\": (2.429290294647217, 1197.7939453125),\n    \"size\": 1.9633429050445557,\n    \"angle\": 183.4322509765625,\n    \"response\": 0.03325376659631729,\n    \"octave\": 6423039,\n    \"class_id\": -1\n}\n``` \n"
    },
    {
      "name": "zone",
      "description": "Definition of polygon zone",
      "docs": null
    },
    {
      "name": "Batch[qr_code_detection]",
      "description": "Prediction with QR code detection",
      "docs": "\nThis kind represents batch of predictions regarding QR codes location and data their provide.\n\nExample:\n```\n# Each prediction in batch is list of dictionaries that contains detected QR codes (detections) and their metadata\n[\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"qr_code\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"qr_code\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},    ],\n    [\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"qr_code\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n        {\"x\": 300, \"y\": 400, \"width\": 100, \"height\" 50, \"confidence\": 1.0, \"class\": \"qr_code\", \"class_id\": 0.1, \"detection_id\": \"random-uuid\", \"data\": \"<qr-code-data>\"},\n    ]\n]\n```\n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "image",
      "description": "Image in workflows",
      "docs": "TODO"
    },
    {
      "name": "integer",
      "description": "Integer value",
      "docs": "\nExamples:\n```\n1\n2\n```\n"
    },
    {
      "name": "contours",
      "description": "List of numpy arrays where each array represents contour points",
      "docs": "\nThis kind represents a value of a list of numpy arrays where each array represents contour points.\n\nExample:\n```\n[\n    np.array([[10, 10],\n              [20, 20],\n              [30, 30]], dtype=np.int32),\n    np.array([[50, 50],\n              [60, 60],\n              [70, 70]], dtype=np.int32)\n]\n```\n"
    },
    {
      "name": "Batch[dictionary]",
      "description": "Batch of dictionaries",
      "docs": "\nThis kind represent a batch of any Python dicts.\nExamples:\n```\n[{\"my_key\", \"my_value_1\"}, {\"my_key\", \"my_value_2\"}]\n``` \n\n\n**Important note**:\n\nWhen you see `Batch[<A>]` in a name, it means that each group of data, called a batch, will contain elements \nof type `<A>`. This also implies that if there are multiple inputs or outputs for a batch-wise operation, \nthey will maintain the same order of elements within each batch. \n\n"
    },
    {
      "name": "string",
      "description": "String value",
      "docs": "\nExamples:\n```\n\"my string value\"\n```\n"
    }
  ],
  "kinds_connections": {
    "Batch[image]": [
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/convert_grayscale@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/expression@v1",
        "property_name": "data",
        "property_description": "References data to be used to construct results",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/continue_if@v1",
        "property_name": "evaluation_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ocr_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/qr_code_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/contours_detection@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/barcode_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "reference_image",
        "property_description": "Image that was origin to take crops that yielded predictions.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "reference_image",
        "property_description": "Image that was origin to take crops that yielded predictions.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "template",
        "property_description": "The template image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/contours_detection@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parameterize operations",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/qr_code_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ocr_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "template",
        "property_description": "The template image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/convert_grayscale@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/camera_focus@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/camera_focus@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/barcode_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "*": [
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "tolerance",
        "property_description": "Tolerance for color matching.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
        "property_name": "object_detection_predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will have classes replaced.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/convert_grayscale@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "height",
        "property_description": "Height of static crop (relative value 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "offset_height",
        "property_description": "Offset for boxes height",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "classes",
        "property_description": "List of classes that LMM shall classify against",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "disable_sink",
        "property_description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "kernel_size",
        "property_description": "Size of the average pooling kernel used for blurring.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "border_radius",
        "property_description": "Radius of the label in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "blur_type",
        "property_description": "Type of Blur to perform on image.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "iou_threshold",
        "property_description": "IoU threshold to consider detections from different models as matching (increasing votes for region)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes to be merged.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "base",
        "property_description": "Base width of the triangle in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "lmm_type",
        "property_description": "Type of LMM to be used",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "offset_width",
        "property_description": "Offset for boxes width",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "max_candidates",
        "property_description": "Maximum number of candidates as NMS input to be taken into account.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/expression@v1",
        "property_name": "data",
        "property_description": "References data to be used to construct results",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "nms_threshold",
        "property_description": "The threshold value NMS procedure (if to be applied).",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parameterize operations",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "y_center",
        "property_description": "Center Y of static crop (relative coordinate 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parameterize operations",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "x_center",
        "property_description": "Center X of static crop (absolute coordinate)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "required_votes",
        "property_description": "Required number of votes for single detection from different models to accept detection as output detection",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the bounding box in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "matching_threshold",
        "property_description": "The threshold value for template matching.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "max_value",
        "property_description": "Maximum value for thresholding",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "width",
        "property_description": "Width of static crop (relative value 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "openai_api_key",
        "property_description": "Your OpenAI API key",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the halo overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "perspective_polygons",
        "property_description": "Perspective polygons (for each batch at least one must be consisting of 4 vertices)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_position",
        "property_description": "The anchor position for placing the label.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "classes",
        "property_description": "List of classes to calculate similarity against each input image",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
        "property_name": "required_number_of_vertices",
        "property_description": "Keep simplifying polygon until number of vertices matches this number",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "position",
        "property_description": "The anchor position for placing the triangle.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "required_objects",
        "property_description": "If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to `InferenceParameter`, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "apply_nms",
        "property_description": "Flag to decide if NMS should be applied at the output detections.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "height",
        "property_description": "Height of static crop (absolute value)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will be used to crop the image.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "roundness",
        "property_description": "Roundness of the corners of the bounding box.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_padding",
        "property_description": "Padding around the text in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "version",
        "property_description": "Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "extend_perspective_polygon_by_detections_anchor",
        "property_description": "If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "labeling_batch_prefix",
        "property_description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for merged detections",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "class_agnostic_nms",
        "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "data_percentage",
        "property_description": "Percent of data that will be saved (in range [0.0, 100.0])",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "max_detections",
        "property_description": "Maximum number of detections to return",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "transformed_rect_height",
        "property_description": "Transformed rect height",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the Mask overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the Mask overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/first_non_empty_or_default@v1",
        "property_name": "data",
        "property_description": "Reference data to replace empty values",
        "compatible_element": "step_output",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "target_color",
        "property_description": "Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "outline_thickness",
        "property_description": "Thickness of the outline of the triangle in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "width",
        "property_description": "Width of static crop (absolute value)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "ratio_threshold",
        "property_description": "Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/contours_detection@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "class_filter",
        "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "template",
        "property_description": "The template image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "good_matches_threshold",
        "property_description": "Threshold for the number of good matches to consider the images as matching",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "overlap_filtering_strategy",
        "property_description": "Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards surplus detections, NMM merges them.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "remote_api_key",
        "property_description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "threshold",
        "property_description": "Threshold for predicted masks scores",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "classes_to_consider",
        "property_description": "Optional list of classes to consider in consensus procedure.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "height",
        "property_description": "Height of the triangle in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "registration_tags",
        "property_description": "Tags to be attached to registered datapoints",
        "compatible_element": "workflow_parameter",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ocr_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of overlap filtering strategy. If box intersection over union is above this  ratio, discard or merge the lower confidence box.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "color_clusters",
        "property_description": "Number of dominant colors to identify. Higher values increase precision but may slow processing.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "class_filter",
        "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "prompt",
        "property_description": "Holds unconstrained text prompt to LMM mode",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "class_agnostic_nms",
        "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "kernel_size",
        "property_description": "Size of the average pooling kernel used for blurring.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "outline_thickness",
        "property_description": "Thickness of the outline of the dot in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "max_detections",
        "property_description": "Maximum number of detections to return",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "version",
        "property_description": "Variant of YoloWorld model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_scale",
        "property_description": "Scale of the text.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parameterize operations",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "openai_model",
        "property_description": "Model to be used",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "transformed_rect_width",
        "property_description": "Transformed rect width",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/continue_if@v1",
        "property_name": "evaluation_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/continue_if@v1",
        "property_name": "evaluation_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "descriptor_2",
        "property_description": "Reference to SIFT descriptors from the second image to compare",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "max_iterations",
        "property_description": "Max number of iterations to perform. Higher values increase precision but may slow processing.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/contours_detection@v1",
        "property_name": "line_thickness",
        "property_description": "Line thickness for drawing contours.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "version",
        "property_description": "Variant of CLIP model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/qr_code_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_thickness",
        "property_description": "Thickness of the text characters.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "x_center",
        "property_description": "Center X of static crop (relative coordinate 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "slice_height",
        "property_description": "Height of each slice, in pixels",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "fire_and_forget",
        "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "target_size",
        "property_description": "Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "threshold_type",
        "property_description": "Type of Edge Detection to perform.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "class_agnostic_nms",
        "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "prompt",
        "property_description": "Text prompt to the OpenAI model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "start_angle",
        "property_description": "Starting angle of the ellipse in degrees.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "thresh_value",
        "property_description": "Threshold value.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "field_value",
        "property_description": "This is the name of the metadata field you are creating",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
        "property_name": "classification_predictions",
        "property_description": "The output of classification model for crops taken based on RoIs pointed as the other parameter",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "predictions",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_color",
        "property_description": "Color of the text.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "max_candidates",
        "property_description": "Maximum number of candidates as NMS input to be taken into account.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "class_names",
        "property_description": "One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "warp_image",
        "property_description": "If set to True, image will be warped into transformed rect",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/convert_grayscale@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "slice_width",
        "property_description": "Width of each slice, in pixels",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "border_thickness",
        "property_description": "Thickness of the outline in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
        "property_name": "prompt",
        "property_description": "Text prompt to the CogVLM model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "mask_decode_mode",
        "property_description": "Parameter of mask decoding in prediction post-processing.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "class_aware",
        "property_description": "Flag to decide if merging detections is class-aware or only bounding boxes aware",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "disable_sink",
        "property_description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/expression@v1",
        "property_name": "data",
        "property_description": "References data to be used to construct results",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/camera_focus@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "lmm_type",
        "property_description": "Type of LMM to be used",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "radius",
        "property_description": "Radius of the dot in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "fire_and_forget",
        "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/expression@v1",
        "property_name": "data",
        "property_description": "References data to be used to construct results",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the color overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the outline in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "remote_api_key",
        "property_description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "boxes",
        "property_description": "Boxes (from other model predictions) to ground SAM2",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
        "property_name": "predictions",
        "property_description": "",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "labeling_batch_prefix",
        "property_description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for detections",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
        "property_name": "texts",
        "property_description": "List of texts to calculate similarity against each input image",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "corner_length",
        "property_description": "Length of the corner lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/continue_if@v1",
        "property_name": "evaluation_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ocr_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/property_definition@v1",
        "property_name": "data",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "tradeoff_factor",
        "property_description": "Post-processing parameter to dictate tradeoff between fast and accurate",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "fire_and_forget",
        "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "predictions_batches",
        "property_description": "Reference to detection-like model predictions made against single image to agree on model consensus",
        "compatible_element": "step_output",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "target_project",
        "property_description": "name of Roboflow dataset / project to be used as target for collected data",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "class_filter",
        "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "keypoint_confidence",
        "property_description": "Confidence threshold to predict keypoint as visible.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/barcode_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "multimask_output",
        "property_description": "Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "position",
        "property_description": "The anchor position for placing the dot.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "descriptor_1",
        "property_description": "Reference to SIFT descriptors from the first image to compare",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "persist_predictions",
        "property_description": "Boolean flag to decide if predictions should be registered along with images",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "reference_image",
        "property_description": "Image that was origin to take crops that yielded predictions.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "reference_image",
        "property_description": "Image that was origin to take crops that yielded predictions.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/contours_detection@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "max_candidates",
        "property_description": "Maximum number of candidates as NMS input to be taken into account.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "perspective_polygons",
        "property_description": "Perspective polygons (for each batch at least one must be consisting of 4 vertices)",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "color",
        "property_description": "Color of the background.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/qr_code_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "overlap_ratio_height",
        "property_description": "Overlap ratio between consecutive slices in the height dimension",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "max_detections",
        "property_description": "Maximum number of detections to return",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "template",
        "property_description": "The template image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "end_angle",
        "property_description": "Ending angle of the ellipse in degrees.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "overlap_ratio_width",
        "property_description": "Overlap ratio between consecutive slices in the width dimension",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "field_value",
        "property_description": "This is the name of the metadata field you are creating",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "position",
        "property_description": "The anchor position for placing the crop.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "y_center",
        "property_description": "Center Y of static crop (absolute coordinate)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "pixel_size",
        "property_description": "Size of the pixelation.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/camera_focus@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "image",
        "property_description": "The input image for this step.",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "target_color",
        "property_description": "Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dimension_collapse@v1",
        "property_name": "data",
        "property_description": "Reference to step outputs at depth level n to be concatenated and moved into level n-1.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "registration_tags",
        "property_description": "Tags to be attached to registered datapoints",
        "compatible_element": "workflow_parameter",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "kernel_size",
        "property_description": "Size of the average pooling kernel used for creating the halo.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text",
        "property_description": "The type of text to display.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "target_project",
        "property_description": "name of Roboflow dataset / project to be used as target for collected data",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/barcode_detector@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "scale_factor",
        "property_description": "The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "images",
        "property_description": "The image to infer on",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "image_detail",
        "property_description": "Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "operations_parameters",
        "property_description": "References to additional parameters that may be provided in runtime to parametrise operations",
        "compatible_element": "workflow_image",
        "is_list_element": false,
        "is_dict_element": true
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "images",
        "property_description": "The input image for this step.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "Batch[instance_segmentation_prediction]": [
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "predictions_batches",
        "property_description": "Reference to detection-like model predictions made against single image to agree on model consensus",
        "compatible_element": "step_output",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
        "property_name": "object_detection_predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will have classes replaced.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes to be merged.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "boxes",
        "property_description": "Boxes (from other model predictions) to ground SAM2",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
        "property_name": "predictions",
        "property_description": "",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "predictions",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/property_definition@v1",
        "property_name": "data",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will be used to crop the image.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "Batch[keypoint_detection_prediction]": [
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "predictions_batches",
        "property_description": "Reference to detection-like model predictions made against single image to agree on model consensus",
        "compatible_element": "step_output",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
        "property_name": "object_detection_predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will have classes replaced.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "boxes",
        "property_description": "Boxes (from other model predictions) to ground SAM2",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "predictions",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/property_definition@v1",
        "property_name": "data",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will be used to crop the image.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "Batch[object_detection_prediction]": [
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "predictions_batches",
        "property_description": "Reference to detection-like model predictions made against single image to agree on model consensus",
        "compatible_element": "step_output",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
        "property_name": "object_detection_predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will have classes replaced.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes to be merged.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "boxes",
        "property_description": "Boxes (from other model predictions) to ground SAM2",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_filter@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "predictions",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/property_definition@v1",
        "property_name": "data",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
        "property_name": "predictions",
        "property_description": "Reference to detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
        "property_name": "predictions",
        "property_description": "The output of a detection model describing the bounding boxes that will be used to crop the image.",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "predictions",
        "property_description": "Predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "string": [
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "version",
        "property_description": "Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "field_value",
        "property_description": "This is the name of the metadata field you are creating",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "prompt",
        "property_description": "Text prompt to the OpenAI model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "blur_type",
        "property_description": "Type of Blur to perform on image.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "overlap_filtering_strategy",
        "property_description": "Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards surplus detections, NMM merges them.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "color",
        "property_description": "Color of the background.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "position",
        "property_description": "The anchor position for placing the crop.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "lmm_type",
        "property_description": "Type of LMM to be used",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "registration_tags",
        "property_description": "Tags to be attached to registered datapoints",
        "compatible_element": "workflow_parameter",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "target_color",
        "property_description": "Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
        "property_name": "prompt",
        "property_description": "Text prompt to the CogVLM model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "position",
        "property_description": "The anchor position for placing the triangle.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "remote_api_key",
        "property_description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "labeling_batch_prefix",
        "property_description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "mask_decode_mode",
        "property_description": "Parameter of mask decoding in prediction post-processing.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "version",
        "property_description": "Variant of YoloWorld model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "threshold_type",
        "property_description": "Type of Edge Detection to perform.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "extend_perspective_polygon_by_detections_anchor",
        "property_description": "If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "position",
        "property_description": "The anchor position for placing the dot.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "labeling_batch_prefix",
        "property_description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "registration_tags",
        "property_description": "Tags to be attached to registered datapoints",
        "compatible_element": "workflow_parameter",
        "is_list_element": true,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "openai_api_key",
        "property_description": "Your OpenAI API key",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_color",
        "property_description": "Color of the text.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "openai_model",
        "property_description": "Model to be used",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "version",
        "property_description": "Variant of CLIP model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "prompt",
        "property_description": "Holds unconstrained text prompt to LMM mode",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text",
        "property_description": "The type of text to display.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm@v1",
        "property_name": "lmm_type",
        "property_description": "Type of LMM to be used",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/open_ai@v1",
        "property_name": "image_detail",
        "property_description": "Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "color_palette",
        "property_description": "Color palette to use for annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_position",
        "property_description": "The anchor position for placing the label.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "remote_api_key",
        "property_description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "color_axis",
        "property_description": "Strategy to use for mapping colors to annotations.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "float": [
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "threshold",
        "property_description": "Threshold for predicted masks scores",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "scale_factor",
        "property_description": "The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "matching_threshold",
        "property_description": "The threshold value for template matching.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "data_percentage",
        "property_description": "Percent of data that will be saved (in range [0.0, 100.0])",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_scale",
        "property_description": "Scale of the text.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "boolean": [
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "fire_and_forget",
        "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "fire_and_forget",
        "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/segment_anything@v1",
        "property_name": "multimask_output",
        "property_description": "Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "class_aware",
        "property_description": "Flag to decide if merging detections is class-aware or only bounding boxes aware",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "disable_sink",
        "property_description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "disable_sink",
        "property_description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "persist_predictions",
        "property_description": "Boolean flag to decide if predictions should be registered along with images",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "class_agnostic_nms",
        "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "class_agnostic_nms",
        "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "apply_nms",
        "property_description": "Flag to decide if NMS should be applied at the output detections.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "class_agnostic_nms",
        "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "warp_image",
        "property_description": "If set to True, image will be warped into transformed rect",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "copy_image",
        "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "disable_active_learning",
        "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "fire_and_forget",
        "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "integer": [
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_padding",
        "property_description": "Padding around the text in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "tolerance",
        "property_description": "Tolerance for color matching.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "slice_width",
        "property_description": "Width of each slice, in pixels",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "transformed_rect_width",
        "property_description": "Transformed rect width",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "transformed_rect_height",
        "property_description": "Transformed rect height",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "border_thickness",
        "property_description": "Thickness of the outline in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "radius",
        "property_description": "Radius of the dot in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "y_center",
        "property_description": "Center Y of static crop (absolute coordinate)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "start_angle",
        "property_description": "Starting angle of the ellipse in degrees.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "slice_height",
        "property_description": "Height of each slice, in pixels",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
        "property_name": "pixel_size",
        "property_description": "Size of the pixelation.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "height",
        "property_description": "Height of the triangle in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "base",
        "property_description": "Base width of the triangle in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "max_detections",
        "property_description": "Maximum number of detections to return",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the outline in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
        "property_name": "required_number_of_vertices",
        "property_description": "Keep simplifying polygon until number of vertices matches this number",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "thresh_value",
        "property_description": "Threshold value.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "outline_thickness",
        "property_description": "Thickness of the outline of the triangle in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "width",
        "property_description": "Width of static crop (absolute value)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "max_detections",
        "property_description": "Maximum number of detections to return",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the bounding box in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "max_iterations",
        "property_description": "Max number of iterations to perform. Higher values increase precision but may slow processing.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "target_size",
        "property_description": "Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "ratio_threshold",
        "property_description": "Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/threshold@v1",
        "property_name": "max_value",
        "property_description": "Maximum value for thresholding",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "offset_width",
        "property_description": "Offset for boxes width",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "corner_length",
        "property_description": "Length of the corner lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "good_matches_threshold",
        "property_description": "Threshold for the number of good matches to consider the images as matching",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detection_offset@v1",
        "property_name": "offset_height",
        "property_description": "Offset for boxes height",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "max_candidates",
        "property_description": "Maximum number of candidates as NMS input to be taken into account.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dominant_color@v1",
        "property_name": "color_clusters",
        "property_description": "Number of dominant colors to identify. Higher values increase precision but may slow processing.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/contours_detection@v1",
        "property_name": "line_thickness",
        "property_description": "Line thickness for drawing contours.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
        "property_name": "kernel_size",
        "property_description": "Size of the average pooling kernel used for blurring.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "required_objects",
        "property_description": "If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to `InferenceParameter`, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "end_angle",
        "property_description": "Ending angle of the ellipse in degrees.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "palette_size",
        "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "max_candidates",
        "property_description": "Maximum number of candidates as NMS input to be taken into account.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "thickness",
        "property_description": "Thickness of the lines in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "kernel_size",
        "property_description": "Size of the average pooling kernel used for creating the halo.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "text_thickness",
        "property_description": "Thickness of the text characters.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_blur@v1",
        "property_name": "kernel_size",
        "property_description": "Size of the average pooling kernel used for blurring.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "outline_thickness",
        "property_description": "Thickness of the outline of the dot in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "height",
        "property_description": "Height of static crop (absolute value)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "border_radius",
        "property_description": "Radius of the label in pixels.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
        "property_name": "x_center",
        "property_description": "Center X of static crop (absolute coordinate)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "max_detections",
        "property_description": "Maximum number of detections to return",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "required_votes",
        "property_description": "Required number of votes for single detection from different models to accept detection as output detection",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "max_candidates",
        "property_description": "Maximum number of candidates as NMS input to be taken into account.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "float_zero_to_one": [
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "iou_threshold",
        "property_description": "IoU threshold to consider detections from different models as matching (increasing votes for region)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the Mask overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the color overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the Mask overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "overlap_ratio_height",
        "property_description": "Overlap ratio between consecutive slices in the height dimension",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "height",
        "property_description": "Height of static crop (relative value 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "keypoint_confidence",
        "property_description": "Confidence threshold to predict keypoint as visible.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for detections",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "width",
        "property_description": "Width of static crop (relative value 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
        "property_name": "iou_threshold",
        "property_description": "Parameter of overlap filtering strategy. If box intersection over union is above this  ratio, discard or merge the lower confidence box.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for merged detections",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/image_slicer@v1",
        "property_name": "overlap_ratio_width",
        "property_description": "Overlap ratio between consecutive slices in the width dimension",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "opacity",
        "property_description": "Transparency of the halo overlay.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/template_matching@v1",
        "property_name": "nms_threshold",
        "property_description": "The threshold value NMS procedure (if to be applied).",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "y_center",
        "property_description": "Center Y of static crop (relative coordinate 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
        "property_name": "x_center",
        "property_description": "Center X of static crop (relative coordinate 0.0-1.0)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "confidence",
        "property_description": "Confidence threshold for predictions",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "tradeoff_factor",
        "property_description": "Post-processing parameter to dictate tradeoff between fast and accurate",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "roundness",
        "property_description": "Roundness of the corners of the bounding box.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "list_of_values": [
      {
        "manifest_type_identifier": "roboflow_core/color_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "perspective_polygons",
        "property_description": "Perspective polygons (for each batch at least one must be consisting of 4 vertices)",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "class_filter",
        "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "classes_to_consider",
        "property_description": "Optional list of classes to consider in consensus procedure.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
        "property_name": "texts",
        "property_description": "List of texts to calculate similarity against each input image",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
        "property_name": "classes",
        "property_description": "List of classes that LMM shall classify against",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "class_filter",
        "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
        "property_name": "perspective_polygons",
        "property_description": "Perspective polygons (for each batch at least one must be consisting of 4 vertices)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "class_filter",
        "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/label_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
        "property_name": "class_names",
        "property_description": "One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
        "property_name": "custom_colors",
        "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
        "property_name": "classes",
        "property_description": "List of classes to calculate similarity against each input image",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "dictionary": [
      {
        "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
        "property_name": "required_objects",
        "property_description": "If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to `InferenceParameter`, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "roboflow_model_id": [
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "model_id",
        "property_description": "Roboflow model identifier",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "roboflow_project": [
      {
        "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "target_project",
        "property_description": "name of Roboflow dataset / project to be used as target for collected data",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "target_project",
        "property_description": "name of Roboflow dataset / project to be used as target for collected data",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
        "property_name": "active_learning_target_dataset",
        "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
        "compatible_element": "workflow_parameter",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "Batch[classification_prediction]": [
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/property_definition@v1",
        "property_name": "data",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
        "property_name": "classification_predictions",
        "property_description": "The output of classification model for crops taken based on RoIs pointed as the other parameter",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
        "property_name": "predictions",
        "property_description": "Reference q detection-like predictions",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "predictions",
        "property_description": "Reference data to extract property from",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "Batch[string]": [
      {
        "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
        "property_name": "field_value",
        "property_description": "This is the name of the metadata field you are creating",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "rgb_color": [
      {
        "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
        "property_name": "target_color",
        "property_description": "Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ],
    "numpy_array": [
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "descriptor_2",
        "property_description": "Reference to SIFT descriptors from the second image to compare",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      },
      {
        "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
        "property_name": "descriptor_1",
        "property_description": "Reference to SIFT descriptors from the first image to compare",
        "compatible_element": "step_output",
        "is_list_element": false,
        "is_dict_element": false
      }
    ]
  },
  "primitives_connections": [
    {
      "manifest_type_identifier": "roboflow_core/segment_anything@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/segment_anything@v1",
      "property_name": "boxes",
      "property_description": "Boxes (from other model predictions) to ground SAM2",
      "type_annotation": ""
    },
    {
      "manifest_type_identifier": "roboflow_core/segment_anything@v1",
      "property_name": "version",
      "property_description": "Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/segment_anything@v1",
      "property_name": "threshold",
      "property_description": "Threshold for predicted masks scores",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/segment_anything@v1",
      "property_name": "multimask_output",
      "property_description": "Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "required_votes",
      "property_description": "Required number of votes for single detection from different models to accept detection as output detection",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "class_aware",
      "property_description": "Flag to decide if merging detections is class-aware or only bounding boxes aware",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "iou_threshold",
      "property_description": "IoU threshold to consider detections from different models as matching (increasing votes for region)",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for merged detections",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "classes_to_consider",
      "property_description": "Optional list of classes to consider in consensus procedure.",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "required_objects",
      "property_description": "If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to `InferenceParameter`, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.",
      "type_annotation": "Optional[Dict[str, int], int]"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "presence_confidence_aggregation",
      "property_description": "Mode dictating aggregation of confidence scores and classes both in case of object presence deduction procedure.",
      "type_annotation": "AggregationMode"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "detections_merge_confidence_aggregation",
      "property_description": "Mode dictating aggregation of confidence scores and classes both in case of boxes consensus procedure. One of `average`, `max`, `min`. Default: `average`. While using for merging overlapping boxes, against classes - `average` equals to majority vote, `max` - for the class of detection with max confidence, `min` - for the class of detection with min confidence.",
      "type_annotation": "AggregationMode"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_consensus@v1",
      "property_name": "detections_merge_coordinates_aggregation",
      "property_description": "Mode dictating aggregation of bounding boxes. One of `average`, `max`, `min`. Default: `average`. `average` means taking mean from all boxes coordinates, `min` - taking smallest box, `max` - taking largest box.",
      "type_annotation": "AggregationMode"
    },
    {
      "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
      "property_name": "name",
      "property_description": "Unique name of step in workflows",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/clip_comparison@v1",
      "property_name": "texts",
      "property_description": "List of texts to calculate similarity against each input image",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "property_name": "prompt",
      "property_description": "Holds unconstrained text prompt to LMM mode",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "property_name": "lmm_type",
      "property_description": "Type of LMM to be used",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "property_name": "lmm_config",
      "property_description": "Configuration of LMM",
      "type_annotation": "LMMConfig"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "property_name": "remote_api_key",
      "property_description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm@v1",
      "property_name": "json_output",
      "property_description": "Holds dictionary that maps name of requested output field into its description",
      "type_annotation": "Dict[str, str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
      "property_name": "lmm_type",
      "property_description": "Type of LMM to be used",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
      "property_name": "classes",
      "property_description": "List of classes that LMM shall classify against",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
      "property_name": "lmm_config",
      "property_description": "Configuration of LMM",
      "type_annotation": "LMMConfig"
    },
    {
      "manifest_type_identifier": "roboflow_core/lmm_for_classification@v1",
      "property_name": "remote_api_key",
      "property_description": "Holds API key required to call LMM model - in current state of development, we require OpenAI key when `lmm_type=gpt_4v` and do not require additional API key for CogVLM calls.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "prompt",
      "property_description": "Text prompt to the OpenAI model",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "openai_api_key",
      "property_description": "Your OpenAI API key",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "openai_model",
      "property_description": "Model to be used",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "json_output_format",
      "property_description": "Holds dictionary that maps name of requested output field into its description",
      "type_annotation": "Dict[str, str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "image_detail",
      "property_description": "Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/open_ai@v1",
      "property_name": "max_tokens",
      "property_description": "Maximum number of tokens the model can generate in it's response.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
      "property_name": "prompt",
      "property_description": "Text prompt to the CogVLM model",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/cog_vlm@v1",
      "property_name": "json_output_format",
      "property_description": "Holds dictionary that maps name of requested output field into its description",
      "type_annotation": "Dict[str, str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/ocr_model@v1",
      "property_name": "name",
      "property_description": "Unique name of step in workflows",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
      "property_name": "class_names",
      "property_description": "One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects.",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
      "property_name": "version",
      "property_description": "Variant of YoloWorld model",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/yolo_world_model@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for detections",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "model_id",
      "property_description": "Roboflow model identifier",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "class_agnostic_nms",
      "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "class_filter",
      "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for predictions",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "iou_threshold",
      "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "max_detections",
      "property_description": "Maximum number of detections to return",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "max_candidates",
      "property_description": "Maximum number of candidates as NMS input to be taken into account.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "mask_decode_mode",
      "property_description": "Parameter of mask decoding in prediction post-processing.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "tradeoff_factor",
      "property_description": "Post-processing parameter to dictate tradeoff between fast and accurate",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "disable_active_learning",
      "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_instance_segmentation_model@v1",
      "property_name": "active_learning_target_dataset",
      "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "model_id",
      "property_description": "Roboflow model identifier",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "class_agnostic_nms",
      "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "class_filter",
      "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for predictions",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "iou_threshold",
      "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "max_detections",
      "property_description": "Maximum number of detections to return",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "max_candidates",
      "property_description": "Maximum number of candidates as NMS input to be taken into account.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "keypoint_confidence",
      "property_description": "Confidence threshold to predict keypoint as visible.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "disable_active_learning",
      "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_keypoint_detection_model@v1",
      "property_name": "active_learning_target_dataset",
      "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
      "property_name": "model_id",
      "property_description": "Roboflow model identifier",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for predictions",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
      "property_name": "disable_active_learning",
      "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_classification_model@v1",
      "property_name": "active_learning_target_dataset",
      "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
      "property_name": "model_id",
      "property_description": "Roboflow model identifier",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for predictions",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
      "property_name": "disable_active_learning",
      "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_multi_label_classification_model@v1",
      "property_name": "active_learning_target_dataset",
      "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "model_id",
      "property_description": "Roboflow model identifier",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "class_agnostic_nms",
      "property_description": "Value to decide if NMS is to be used in class-agnostic mode.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "class_filter",
      "property_description": "List of classes to retrieve from predictions (to define subset of those which was used while model training)",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "confidence",
      "property_description": "Confidence threshold for predictions",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "iou_threshold",
      "property_description": "Parameter of NMS, to decide on minimum box intersection over union to merge boxes",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "max_detections",
      "property_description": "Maximum number of detections to return",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "max_candidates",
      "property_description": "Maximum number of candidates as NMS input to be taken into account.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "disable_active_learning",
      "property_description": "Parameter to decide if Active Learning data sampling is disabled for the model",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_object_detection_model@v1",
      "property_name": "active_learning_target_dataset",
      "property_description": "Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/barcode_detector@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/qr_code_detector@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
      "property_name": "x_center",
      "property_description": "Center X of static crop (absolute coordinate)",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
      "property_name": "y_center",
      "property_description": "Center Y of static crop (absolute coordinate)",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
      "property_name": "width",
      "property_description": "Width of static crop (absolute value)",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/absolute_static_crop@v1",
      "property_name": "height",
      "property_description": "Height of static crop (absolute value)",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/dynamic_crop@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_filter@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_filter@v1",
      "property_name": "operations",
      "property_description": "not available",
      "type_annotation": "List[Union[ClassificationPropertyExtract, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsSelection, DetectionsShift, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]"
    },
    {
      "manifest_type_identifier": "roboflow_core/detection_offset@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detection_offset@v1",
      "property_name": "offset_width",
      "property_description": "Offset for boxes width",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/detection_offset@v1",
      "property_name": "offset_height",
      "property_description": "Offset for boxes height",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
      "property_name": "x_center",
      "property_description": "Center X of static crop (relative coordinate 0.0-1.0)",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
      "property_name": "y_center",
      "property_description": "Center Y of static crop (relative coordinate 0.0-1.0)",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
      "property_name": "width",
      "property_description": "Width of static crop (relative value 0.0-1.0)",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/relative_statoic_crop@v1",
      "property_name": "height",
      "property_description": "Height of static crop (relative value 0.0-1.0)",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_transformation@v1",
      "property_name": "operations",
      "property_description": "not available",
      "type_annotation": "List[Union[ClassificationPropertyExtract, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsSelection, DetectionsShift, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "predictions",
      "property_description": "Reference q detection-like predictions",
      "type_annotation": ""
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "target_project",
      "property_description": "name of Roboflow dataset / project to be used as target for collected data",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "usage_quota_name",
      "property_description": "Unique name for Roboflow project pointed by `target_project` parameter, that identifies usage quota applied for this block.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "persist_predictions",
      "property_description": "Boolean flag to decide if predictions should be registered along with images",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "minutely_usage_limit",
      "property_description": "Maximum number of data registration requests per minute accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "hourly_usage_limit",
      "property_description": "Maximum number of data registration requests per hour accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "daily_usage_limit",
      "property_description": "Maximum number of data registration requests per day accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "max_image_size",
      "property_description": "Maximum size of the image to be registered - bigger images will be downsized preserving aspect ratio. Format of data: `(width, height)`",
      "type_annotation": "Any"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "compression_level",
      "property_description": "Compression level for images registered",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "registration_tags",
      "property_description": "Tags to be attached to registered datapoints",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "disable_sink",
      "property_description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "fire_and_forget",
      "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "labeling_batch_prefix",
      "property_description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v1",
      "property_name": "labeling_batches_recreation_frequency",
      "property_description": "Frequency in which new labeling batches are created in Roboflow app. New batches are created with name prefix provided in `labeling_batch_prefix` in given time intervals.Useful in organising labeling flow.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/continue_if@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/continue_if@v1",
      "property_name": "condition_statement",
      "property_description": "Workflows UQL definition of conditional logic.",
      "type_annotation": "StatementGroup"
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "predictions",
      "property_description": "Predictions",
      "type_annotation": ""
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "perspective_polygons",
      "property_description": "Perspective polygons (for each batch at least one must be consisting of 4 vertices)",
      "type_annotation": "List[Any]"
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "transformed_rect_width",
      "property_description": "Transformed rect width",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "transformed_rect_height",
      "property_description": "Transformed rect height",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "extend_perspective_polygon_by_detections_anchor",
      "property_description": "If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/perspective_correction@v1",
      "property_name": "warp_image",
      "property_description": "If set to True, image will be warped into transformed rect",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/dynamic_zone@v1",
      "property_name": "required_number_of_vertices",
      "property_description": "Keep simplifying polygon until number of vertices matches this number",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_classes_replacement@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/expression@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/expression@v1",
      "property_name": "data_operations",
      "property_description": "UQL definitions of operations to be performed on defined data before switch-case instruction",
      "type_annotation": "Dict[str, List[Union[ClassificationPropertyExtract, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsSelection, DetectionsShift, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]"
    },
    {
      "manifest_type_identifier": "roboflow_core/expression@v1",
      "property_name": "switch",
      "property_description": "not available",
      "type_annotation": "CasesDefinition"
    },
    {
      "manifest_type_identifier": "roboflow_core/property_definition@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/property_definition@v1",
      "property_name": "operations",
      "property_description": "not available",
      "type_annotation": "List[Union[ClassificationPropertyExtract, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsSelection, DetectionsShift, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]"
    },
    {
      "manifest_type_identifier": "roboflow_core/dimension_collapse@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/first_non_empty_or_default@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/first_non_empty_or_default@v1",
      "property_name": "default",
      "property_description": "Default value that will be placed whenever there is no data found",
      "type_annotation": "Any"
    },
    {
      "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
      "property_name": "color",
      "property_description": "Color of the background.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/background_color_visualization@v1",
      "property_name": "opacity",
      "property_description": "Transparency of the Mask overlay.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/blur_visualization@v1",
      "property_name": "kernel_size",
      "property_description": "Size of the average pooling kernel used for blurring.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "thickness",
      "property_description": "Thickness of the bounding box in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/bounding_box_visualization@v1",
      "property_name": "roundness",
      "property_description": "Roundness of the corners of the bounding box.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/circle_visualization@v1",
      "property_name": "thickness",
      "property_description": "Thickness of the lines in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/color_visualization@v1",
      "property_name": "opacity",
      "property_description": "Transparency of the color overlay.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "thickness",
      "property_description": "Thickness of the lines in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/corner_visualization@v1",
      "property_name": "corner_length",
      "property_description": "Length of the corner lines in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "position",
      "property_description": "The anchor position for placing the crop.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "scale_factor",
      "property_description": "The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/crop_visualization@v1",
      "property_name": "border_thickness",
      "property_description": "Thickness of the outline in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "position",
      "property_description": "The anchor position for placing the dot.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "radius",
      "property_description": "Radius of the dot in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/dot_visualization@v1",
      "property_name": "outline_thickness",
      "property_description": "Thickness of the outline of the dot in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "thickness",
      "property_description": "Thickness of the lines in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "start_angle",
      "property_description": "Starting angle of the ellipse in degrees.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/ellipse_visualization@v1",
      "property_name": "end_angle",
      "property_description": "Ending angle of the ellipse in degrees.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "opacity",
      "property_description": "Transparency of the halo overlay.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/halo_visualization@v1",
      "property_name": "kernel_size",
      "property_description": "Size of the average pooling kernel used for creating the halo.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "text",
      "property_description": "The type of text to display.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "text_position",
      "property_description": "The anchor position for placing the label.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "text_color",
      "property_description": "Color of the text.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "text_scale",
      "property_description": "Scale of the text.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "text_thickness",
      "property_description": "Thickness of the text characters.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "text_padding",
      "property_description": "Padding around the text in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/label_visualization@v1",
      "property_name": "border_radius",
      "property_description": "Radius of the label in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/mask_visualization@v1",
      "property_name": "opacity",
      "property_description": "Transparency of the Mask overlay.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/pixelate_visualization@v1",
      "property_name": "pixel_size",
      "property_description": "Size of the pixelation.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/polygon_visualization@v1",
      "property_name": "thickness",
      "property_description": "Thickness of the outline in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "copy_image",
      "property_description": "Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "color_palette",
      "property_description": "Color palette to use for annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "palette_size",
      "property_description": "Number of colors in the color palette. Applies when using a matplotlib `color_palette`.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "custom_colors",
      "property_description": "List of colors to use for annotations when `color_palette` is set to \"CUSTOM\".",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "color_axis",
      "property_description": "Strategy to use for mapping colors to annotations.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "position",
      "property_description": "The anchor position for placing the triangle.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "base",
      "property_description": "Base width of the triangle in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "height",
      "property_description": "Height of the triangle in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/triangle_visualization@v1",
      "property_name": "outline_thickness",
      "property_description": "Thickness of the outline of the triangle in pixels.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
      "property_name": "field_value",
      "property_description": "This is the name of the metadata field you are creating",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
      "property_name": "field_name",
      "property_description": "Name of the field to be updated in Roboflow Customer Metadata",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_custom_metadata@v1",
      "property_name": "fire_and_forget",
      "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
      "property_name": "overlap_filtering_strategy",
      "property_description": "Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards surplus detections, NMM merges them.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/detections_stitch@v1",
      "property_name": "iou_threshold",
      "property_description": "Parameter of overlap filtering strategy. If box intersection over union is above this  ratio, discard or merge the lower confidence box.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_slicer@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_slicer@v1",
      "property_name": "slice_width",
      "property_description": "Width of each slice, in pixels",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_slicer@v1",
      "property_name": "slice_height",
      "property_description": "Height of each slice, in pixels",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_slicer@v1",
      "property_name": "overlap_ratio_width",
      "property_description": "Overlap ratio between consecutive slices in the width dimension",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_slicer@v1",
      "property_name": "overlap_ratio_height",
      "property_description": "Overlap ratio between consecutive slices in the height dimension",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/dominant_color@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/dominant_color@v1",
      "property_name": "color_clusters",
      "property_description": "Number of dominant colors to identify. Higher values increase precision but may slow processing.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/dominant_color@v1",
      "property_name": "max_iterations",
      "property_description": "Max number of iterations to perform. Higher values increase precision but may slow processing.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/dominant_color@v1",
      "property_name": "target_size",
      "property_description": "Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
      "property_name": "target_color",
      "property_description": "Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).",
      "type_annotation": "Union[Any, str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/pixel_color_count@v1",
      "property_name": "tolerance",
      "property_description": "Tolerance for color matching.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
      "property_name": "good_matches_threshold",
      "property_description": "Threshold for the number of good matches to consider the images as matching",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/sift_comparison@v1",
      "property_name": "ratio_threshold",
      "property_description": "Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/sift@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/template_matching@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/template_matching@v1",
      "property_name": "matching_threshold",
      "property_description": "The threshold value for template matching.",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/template_matching@v1",
      "property_name": "apply_nms",
      "property_description": "Flag to decide if NMS should be applied at the output detections.",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/template_matching@v1",
      "property_name": "nms_threshold",
      "property_description": "The threshold value NMS procedure (if to be applied).",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_blur@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_blur@v1",
      "property_name": "blur_type",
      "property_description": "Type of Blur to perform on image.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/image_blur@v1",
      "property_name": "kernel_size",
      "property_description": "Size of the average pooling kernel used for blurring.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/convert_grayscale@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/threshold@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/threshold@v1",
      "property_name": "threshold_type",
      "property_description": "Type of Edge Detection to perform.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/threshold@v1",
      "property_name": "thresh_value",
      "property_description": "Threshold value.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/threshold@v1",
      "property_name": "max_value",
      "property_description": "Maximum value for thresholding",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/contours_detection@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/contours_detection@v1",
      "property_name": "line_thickness",
      "property_description": "Line thickness for drawing contours.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
      "property_name": "name",
      "property_description": "Unique name of step in workflows",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
      "property_name": "classes",
      "property_description": "List of classes to calculate similarity against each input image",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/clip_comparison@v2",
      "property_name": "version",
      "property_description": "Variant of CLIP model",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/camera_focus@v1",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "name",
      "property_description": "The unique name of this step.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "predictions",
      "property_description": "Reference q detection-like predictions",
      "type_annotation": ""
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "target_project",
      "property_description": "name of Roboflow dataset / project to be used as target for collected data",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "usage_quota_name",
      "property_description": "Unique name for Roboflow project pointed by `target_project` parameter, that identifies usage quota applied for this block.",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "data_percentage",
      "property_description": "Percent of data that will be saved (in range [0.0, 100.0])",
      "type_annotation": "float"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "persist_predictions",
      "property_description": "Boolean flag to decide if predictions should be registered along with images",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "minutely_usage_limit",
      "property_description": "Maximum number of data registration requests per minute accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "hourly_usage_limit",
      "property_description": "Maximum number of data registration requests per hour accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "daily_usage_limit",
      "property_description": "Maximum number of data registration requests per day accounted in scope of single server or whole Roboflow platform, depending on context of usage.",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "max_image_size",
      "property_description": "Maximum size of the image to be registered - bigger images will be downsized preserving aspect ratio. Format of data: `(width, height)`",
      "type_annotation": "Any"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "compression_level",
      "property_description": "Compression level for images registered",
      "type_annotation": "int"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "registration_tags",
      "property_description": "Tags to be attached to registered datapoints",
      "type_annotation": "List[str]"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "disable_sink",
      "property_description": "boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "fire_and_forget",
      "property_description": "Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use `True` if best-effort registration is needed, use `False` while debugging and if error handling is needed",
      "type_annotation": "bool"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "labeling_batch_prefix",
      "property_description": "Prefix of the name for labeling batches that will be registered in Roboflow app",
      "type_annotation": "str"
    },
    {
      "manifest_type_identifier": "roboflow_core/roboflow_dataset_upload@v2",
      "property_name": "labeling_batches_recreation_frequency",
      "property_description": "Frequency in which new labeling batches are created in Roboflow app. New batches are created with name prefix provided in `labeling_batch_prefix` in given time intervals.Useful in organising labeling flow.",
      "type_annotation": "str"
    }
  ],
  "universal_query_language_description": {
    "operations_description": [
      {
        "operation_type": "StringToLowerCase",
        "compound": false,
        "input_kind": ["string"],
        "output_kind": ["string"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "StringToUpperCase",
        "compound": false,
        "input_kind": ["string"],
        "output_kind": ["string"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "LookupTable",
        "compound": false,
        "input_kind": ["*"],
        "output_kind": ["*"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "ToNumber",
        "compound": false,
        "input_kind": [
          "string",
          "boolean",
          "integer",
          "float",
          "float_zero_to_one"
        ],
        "output_kind": ["integer", "float", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "NumberRound",
        "compound": false,
        "input_kind": ["integer", "float", "float_zero_to_one"],
        "output_kind": ["integer", "float", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "SequenceMap",
        "compound": true,
        "input_kind": ["list_of_values"],
        "output_kind": ["list_of_values"],
        "nested_operation_input_kind": ["*"],
        "nested_operation_output_kind": ["*"],
        "description": null
      },
      {
        "operation_type": "SequenceApply",
        "compound": true,
        "input_kind": ["list_of_values"],
        "output_kind": ["list_of_values"],
        "nested_operation_input_kind": ["*"],
        "nested_operation_output_kind": ["*"],
        "description": null
      },
      {
        "operation_type": "NumericSequenceAggregate",
        "compound": false,
        "input_kind": ["list_of_values"],
        "output_kind": ["integer", "float", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "ToString",
        "compound": false,
        "input_kind": ["*"],
        "output_kind": ["string"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "ToBoolean",
        "compound": false,
        "input_kind": ["float", "float_zero_to_one", "integer"],
        "output_kind": ["boolean"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "StringSubSequence",
        "compound": false,
        "input_kind": ["string"],
        "output_kind": ["string"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "DetectionsPropertyExtract",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "output_kind": ["list_of_values"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "SequenceAggregate",
        "compound": false,
        "input_kind": ["list_of_values"],
        "output_kind": ["*"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "ExtractDetectionProperty",
        "compound": false,
        "input_kind": ["detection"],
        "output_kind": ["*"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "DetectionsFilter",
        "compound": true,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "output_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "nested_operation_input_kind": ["detection"],
        "nested_operation_output_kind": ["boolean"],
        "description": null
      },
      {
        "operation_type": "DetectionsOffset",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "output_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "DetectionsShift",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "output_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "RandomNumber",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction",
          "*"
        ],
        "output_kind": ["float", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "StringMatches",
        "compound": false,
        "input_kind": ["string"],
        "output_kind": ["boolean"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "ExtractImageProperty",
        "compound": false,
        "input_kind": ["image"],
        "output_kind": ["integer"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "SequenceLength",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction",
          "list_of_values",
          "dictionary"
        ],
        "output_kind": ["integer"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "Multiply",
        "compound": false,
        "input_kind": ["integer", "float", "float_zero_to_one"],
        "output_kind": ["integer", "float", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "Divide",
        "compound": false,
        "input_kind": ["integer", "float", "float_zero_to_one"],
        "output_kind": ["integer", "float", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "DetectionsSelection",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "output_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "SortDetections",
        "compound": false,
        "input_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "output_kind": [
          "object_detection_prediction",
          "instance_segmentation_prediction",
          "keypoint_detection_prediction"
        ],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      },
      {
        "operation_type": "ClassificationPropertyExtract",
        "compound": false,
        "input_kind": ["Batch[classification_prediction]"],
        "output_kind": ["string", "list_of_values", "float_zero_to_one"],
        "nested_operation_input_kind": null,
        "nested_operation_output_kind": null,
        "description": null
      }
    ],
    "operators_descriptions": [
      {
        "operator_type": "in (Sequence)",
        "operands_number": 2,
        "operands_kinds": [
          ["string", "integer", "float", "float_zero_to_one"],
          ["list_of_values", "dictionary"]
        ],
        "description": "Checks if first value is element of second value (usually list or dictionary)"
      },
      {
        "operator_type": "(String) contains",
        "operands_number": 2,
        "operands_kinds": [["string"], ["string"]],
        "description": "Checks if string given as first value contains string provided as second value"
      },
      {
        "operator_type": "(String) endsWith",
        "operands_number": 2,
        "operands_kinds": [["string"], ["string"]],
        "description": "Checks if string given as first value ends with string provided as second value"
      },
      {
        "operator_type": "(String) startsWith",
        "operands_number": 2,
        "operands_kinds": [["string"], ["string"]],
        "description": "Checks if string given as first value starts with string provided as second value"
      },
      {
        "operator_type": "(Number) <=",
        "operands_number": 2,
        "operands_kinds": [
          ["integer", "float", "float_zero_to_one"],
          ["integer", "float", "float_zero_to_one"]
        ],
        "description": "Checks if first value (number) is lower or equal than the second value (number)"
      },
      {
        "operator_type": "(Number) <",
        "operands_number": 2,
        "operands_kinds": [
          ["integer", "float", "float_zero_to_one"],
          ["integer", "float", "float_zero_to_one"]
        ],
        "description": "Checks if first value (number) is lower than the second value (number)"
      },
      {
        "operator_type": "(Number) >=",
        "operands_number": 2,
        "operands_kinds": [
          ["integer", "float", "float_zero_to_one"],
          ["integer", "float", "float_zero_to_one"]
        ],
        "description": "Checks if first value (number) is greater or equal than the second value (number)"
      },
      {
        "operator_type": "(Number) >",
        "operands_number": 2,
        "operands_kinds": [
          ["integer", "float", "float_zero_to_one"],
          ["integer", "float", "float_zero_to_one"]
        ],
        "description": "Checks if first value (number) is greater than the second value (number)"
      },
      {
        "operator_type": "(Number) !=",
        "operands_number": 2,
        "operands_kinds": [
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ],
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ]
        ],
        "description": "Checks if two values given are not equal"
      },
      {
        "operator_type": "!=",
        "operands_number": 2,
        "operands_kinds": [
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ],
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ]
        ],
        "description": "Checks if two values given are not equal"
      },
      {
        "operator_type": "(Number) ==",
        "operands_number": 2,
        "operands_kinds": [
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ],
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ]
        ],
        "description": "Checks if two values given are equal"
      },
      {
        "operator_type": "==",
        "operands_number": 2,
        "operands_kinds": [
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ],
          [
            "integer",
            "string",
            "float",
            "float_zero_to_one",
            "boolean",
            "list_of_values"
          ]
        ],
        "description": "Checks if two values given are equal"
      },
      {
        "operator_type": "Exists",
        "operands_number": 1,
        "operands_kinds": [["*"]],
        "description": "Checks if value is given (not `None`)"
      },
      {
        "operator_type": "DoesNotExist",
        "operands_number": 1,
        "operands_kinds": [["*"]],
        "description": "Checks if value is not given (`None`)"
      },
      {
        "operator_type": "(Boolean) is True",
        "operands_number": 1,
        "operands_kinds": [["boolean"]],
        "description": "Checks if value is `True`"
      },
      {
        "operator_type": "(Boolean) is False",
        "operands_number": 1,
        "operands_kinds": [["boolean"]],
        "description": "Checks if value is `False`"
      },
      {
        "operator_type": "(Sequence) is empty",
        "operands_number": 1,
        "operands_kinds": [
          [
            "list_of_values",
            "dictionary",
            "object_detection_prediction",
            "instance_segmentation_prediction",
            "keypoint_detection_prediction"
          ]
        ],
        "description": "Checks if sequence is empty"
      },
      {
        "operator_type": "(Sequence) is not empty",
        "operands_number": 1,
        "operands_kinds": [
          [
            "list_of_values",
            "dictionary",
            "object_detection_prediction",
            "instance_segmentation_prediction",
            "keypoint_detection_prediction"
          ]
        ],
        "description": "Checks if sequence is not empty"
      }
    ]
  },
  "dynamic_block_definition_schema": {
    "$defs": {
      "DynamicInputDefinition": {
        "properties": {
          "type": {
            "const": "DynamicInputDefinition",
            "enum": ["DynamicInputDefinition"],
            "title": "Type",
            "type": "string"
          },
          "has_default_value": {
            "default": false,
            "description": "Flag to decide if default value is provided for input",
            "title": "Has Default Value",
            "type": "boolean"
          },
          "default_value": {
            "default": null,
            "description": "Definition of default value for a field. Use in combination with, `has_default_value` to decide on default value if field is optional.",
            "title": "Default Value"
          },
          "is_optional": {
            "default": false,
            "description": "Flag deciding if `default_value` will be added for manifest field annotation.",
            "title": "Is Optional",
            "type": "boolean"
          },
          "is_dimensionality_reference": {
            "default": false,
            "description": "Flag deciding if declared property holds dimensionality reference - see how dimensionality works for statically defined blocks to discover meaning of the parameter.",
            "title": "Is Dimensionality Reference",
            "type": "boolean"
          },
          "dimensionality_offset": {
            "default": 0,
            "description": "Accepted dimensionality offset for parameter. Dimensionality works the same as for traditional workflows blocks.",
            "maximum": 1,
            "minimum": -1,
            "title": "Dimensionality Offset",
            "type": "integer"
          },
          "selector_types": {
            "description": "Union of selector types accepted by input. Should be empty if field does not accept selectors.",
            "items": {
              "$ref": "#/$defs/SelectorType"
            },
            "title": "Selector Types",
            "type": "array"
          },
          "selector_data_kind": {
            "additionalProperties": {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            "description": "Mapping of `selector_types` into names of kinds to be compatible. Empty dict (default value) means wildcard kind for all selectors. If name of kind given - must be valid kind, known for workflow execution engine.",
            "title": "Selector Data Kind",
            "type": "object"
          },
          "value_types": {
            "description": "List of types representing union of types for static values (non selectors) that shall be accepted for input field. Empty list represents no value types allowed.",
            "items": {
              "$ref": "#/$defs/ValueType"
            },
            "title": "Value Types",
            "type": "array"
          }
        },
        "required": ["type"],
        "title": "DynamicInputDefinition",
        "type": "object"
      },
      "DynamicOutputDefinition": {
        "properties": {
          "type": {
            "const": "DynamicOutputDefinition",
            "enum": ["DynamicOutputDefinition"],
            "title": "Type",
            "type": "string"
          },
          "kind": {
            "description": "List representing union of kinds for defined output",
            "items": {
              "type": "string"
            },
            "title": "Kind",
            "type": "array"
          }
        },
        "required": ["type"],
        "title": "DynamicOutputDefinition",
        "type": "object"
      },
      "ManifestDescription": {
        "properties": {
          "type": {
            "const": "ManifestDescription",
            "enum": ["ManifestDescription"],
            "title": "Type",
            "type": "string"
          },
          "block_type": {
            "description": "Field holds type of the bock to be dynamically created. Block can be initialised as step using the type declared in the field.",
            "title": "Block Type",
            "type": "string"
          },
          "inputs": {
            "additionalProperties": {
              "$ref": "#/$defs/DynamicInputDefinition"
            },
            "description": "Mapping name -> input definition for block inputs (parameters for run() function ofdynamic block)",
            "title": "Inputs",
            "type": "object"
          },
          "outputs": {
            "additionalProperties": {
              "$ref": "#/$defs/DynamicOutputDefinition"
            },
            "description": "Mapping name -> output kind for block outputs.",
            "title": "Outputs",
            "type": "object"
          },
          "output_dimensionality_offset": {
            "default": 0,
            "description": "Definition of output dimensionality offset",
            "maximum": 1,
            "minimum": -1,
            "title": "Output Dimensionality Offset",
            "type": "integer"
          },
          "accepts_batch_input": {
            "default": false,
            "description": "Flag to decide if function will be provided with batch data as whole or with singular batch elements while execution",
            "title": "Accepts Batch Input",
            "type": "boolean"
          },
          "accepts_empty_values": {
            "default": false,
            "description": "Flag to decide if empty (optional) values will be shipped as run() function parameters",
            "title": "Accepts Empty Values",
            "type": "boolean"
          }
        },
        "required": ["type", "block_type", "inputs"],
        "title": "ManifestDescription",
        "type": "object"
      },
      "PythonCode": {
        "properties": {
          "type": {
            "const": "PythonCode",
            "enum": ["PythonCode"],
            "title": "Type",
            "type": "string"
          },
          "run_function_code": {
            "description": "Code of python function. Content should be properly formatted including indentations. Workflows execution engine is to create dynamic module with provided function - ensuring imports of the following symbols: [Any, List, Dict, Set, sv, np, math, time, json, os, requests, cv2, shapely, Batch, WorkflowImageData, BlockResult]. Expected signature is: def run(self, ... # parameters of manifest apart from name and type). Through self, one may access self._init_results which is dict returned by `init_code` if given.",
            "title": "Run Function Code",
            "type": "string"
          },
          "run_function_name": {
            "default": "run",
            "description": "Name of the function shipped in `function_code`.",
            "title": "Run Function Name",
            "type": "string"
          },
          "init_function_code": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "default": null,
            "description": "Code of the function to perform initialisation of the block. It must be parameter-free function with signature `def init() -> Dict[str, Any]` setting self._init_results on dynamic class initialisation",
            "title": "Init Function Code"
          },
          "init_function_name": {
            "default": "init",
            "description": "Name of init_code function.",
            "title": "Init Function Name",
            "type": "string"
          },
          "imports": {
            "description": "List of additional imports required to run the code",
            "items": {
              "type": "string"
            },
            "title": "Imports",
            "type": "array"
          }
        },
        "required": ["type", "run_function_code"],
        "title": "PythonCode",
        "type": "object"
      },
      "SelectorType": {
        "enum": [
          "input_image",
          "step_output_image",
          "input_parameter",
          "step_output"
        ],
        "title": "SelectorType",
        "type": "string"
      },
      "ValueType": {
        "enum": [
          "any",
          "integer",
          "float",
          "boolean",
          "dict",
          "list",
          "string"
        ],
        "title": "ValueType",
        "type": "string"
      }
    },
    "properties": {
      "type": {
        "const": "DynamicBlockDefinition",
        "enum": ["DynamicBlockDefinition"],
        "title": "Type",
        "type": "string"
      },
      "manifest": {
        "allOf": [
          {
            "$ref": "#/$defs/ManifestDescription"
          }
        ],
        "description": "Definition of manifest for dynamic block to be created in runtime by workflows execution engine."
      },
      "code": {
        "allOf": [
          {
            "$ref": "#/$defs/PythonCode"
          }
        ],
        "description": "Code to be executed in run(...) method of block that will be dynamically created."
      }
    },
    "required": ["type", "manifest", "code"],
    "title": "DynamicBlockDefinition",
    "type": "object"
  }
}
